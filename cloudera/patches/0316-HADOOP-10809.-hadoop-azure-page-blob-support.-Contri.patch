From cc201ce47e3d9c5678775b6f5bb3230513a01b62 Mon Sep 17 00:00:00 2001
From: cnauroth <cnauroth@apache.org>
Date: Wed, 8 Oct 2014 14:20:23 -0700
Subject: [PATCH 316/596] HADOOP-10809. hadoop-azure: page blob support.
 Contributed by Dexter Bradshaw, Mostafa Elhemali,
 Eric Hanson, and Mike Liddell.

(cherry picked from commit e0cbf7a3806dedc604861eaca551e24b9891ea7a)

Conflicts:
	hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java
---
 hadoop-tools/hadoop-azure/README.txt               |   48 +
 .../hadoop-azure/dev-support/findbugs-exclude.xml  |   31 +
 .../hadoop-azure/src/config/checkstyle.xml         |    7 +-
 .../apache/hadoop/fs/azure/AzureLinkedStack.java   |  217 ++++
 .../fs/azure/AzureNativeFileSystemStore.java       |  740 +++++++----
 .../hadoop/fs/azure/NativeAzureFileSystem.java     | 1337 ++++++++++++++++----
 .../hadoop/fs/azure/NativeFileSystemStore.java     |   22 +-
 .../hadoop/fs/azure/PageBlobFormatHelpers.java     |   58 +
 .../hadoop/fs/azure/PageBlobInputStream.java       |  455 +++++++
 .../hadoop/fs/azure/PageBlobOutputStream.java      |  497 ++++++++
 .../org/apache/hadoop/fs/azure/PartialListing.java |    2 +-
 .../apache/hadoop/fs/azure/SelfRenewingLease.java  |  202 +++
 .../hadoop/fs/azure/SelfThrottlingIntercept.java   |    4 +-
 .../fs/azure/ShellDecryptionKeyProvider.java       |    3 +-
 .../apache/hadoop/fs/azure/SimpleKeyProvider.java  |    3 +-
 .../apache/hadoop/fs/azure/StorageInterface.java   |  280 ++--
 .../hadoop/fs/azure/StorageInterfaceImpl.java      |  184 ++-
 .../hadoop/fs/azure/SyncableDataOutputStream.java  |   62 +
 .../main/java/org/apache/hadoop/fs/azure/Wasb.java |    1 -
 .../metrics/AzureFileSystemInstrumentation.java    |    5 +-
 .../metrics/ResponseReceivedMetricUpdater.java     |    7 +-
 .../services/org.apache.hadoop.fs.FileSystem       |   17 +
 .../fs/azure/AzureBlobStorageTestAccount.java      |  155 ++-
 .../hadoop/fs/azure/InMemoryBlockBlobStore.java    |   70 +-
 .../hadoop/fs/azure/MockStorageInterface.java      |  233 +++-
 .../fs/azure/NativeAzureFileSystemBaseTest.java    | 1016 ++++++++++++++-
 .../hadoop/fs/azure/RunningLiveWasbTests.txt       |   22 +
 .../fs/azure/TestAzureConcurrentOutOfBandIo.java   |   75 +-
 .../azure/TestAzureFileSystemErrorConditions.java  |   57 +-
 .../hadoop/fs/azure/TestBlobDataValidation.java    |   12 +-
 .../apache/hadoop/fs/azure/TestBlobMetadata.java   |   35 +-
 .../fs/azure/TestBlobTypeSpeedDifference.java      |  160 +++
 .../fs/azure/TestNativeAzureFSPageBlobLive.java    |   43 +
 .../TestNativeAzureFileSystemConcurrency.java      |    5 +-
 .../TestNativeAzureFileSystemContractLive.java     |   26 +
 .../TestNativeAzureFileSystemContractMocked.java   |   25 +
 .../TestNativeAzureFileSystemFileNameCheck.java    |   11 +-
 .../fs/azure/TestNativeAzureFileSystemLive.java    |   75 ++
 .../fs/azure/TestNativeAzureFileSystemMocked.java  |   35 +
 .../TestNativeAzureFileSystemOperationsMocked.java |   37 +-
 .../TestNativeAzureFileSystemUploadLogic.java      |  186 +++
 .../fs/azure/TestOutOfBandAzureBlobOperations.java |   17 +-
 .../TestOutOfBandAzureBlobOperationsLive.java      |   21 +
 .../azure/TestReadAndSeekPageBlobAfterWrite.java   |  333 +++++
 .../org/apache/hadoop/fs/azure/TestWasbFsck.java   |   33 +
 .../fs/azure/TestWasbUriAndConfiguration.java      |    9 +-
 .../fs/azure/metrics/AzureMetricsTestUtil.java     |    1 +
 .../TestAzureFileSystemInstrumentation.java        |   53 +-
 .../azure/metrics/TestBandwidthGaugeUpdater.java   |   47 -
 .../hadoop-azure/src/test/resources/azure-test.xml |   12 -
 50 files changed, 6049 insertions(+), 937 deletions(-)
 create mode 100644 hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureLinkedStack.java
 create mode 100644 hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobFormatHelpers.java
 create mode 100644 hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobInputStream.java
 create mode 100644 hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobOutputStream.java
 create mode 100644 hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfRenewingLease.java
 create mode 100644 hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SyncableDataOutputStream.java
 create mode 100644 hadoop-tools/hadoop-azure/src/main/resources/META-INF/services/org.apache.hadoop.fs.FileSystem
 create mode 100644 hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/RunningLiveWasbTests.txt
 create mode 100644 hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobTypeSpeedDifference.java
 create mode 100644 hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFSPageBlobLive.java
 create mode 100644 hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemUploadLogic.java
 create mode 100644 hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestReadAndSeekPageBlobAfterWrite.java

diff --git a/hadoop-tools/hadoop-azure/README.txt b/hadoop-tools/hadoop-azure/README.txt
index 73306d3..a1d1a65 100644
--- a/hadoop-tools/hadoop-azure/README.txt
+++ b/hadoop-tools/hadoop-azure/README.txt
@@ -77,6 +77,54 @@ src\test\resources\azure-test.xml. These settings augment the hadoop configurati
 For live tests, set the following in azure-test.xml:
  1. "fs.azure.test.account.name -> {azureStorageAccountName} 
  2. "fs.azure.account.key.{AccountName} -> {fullStorageKey}"
+ 
+===================================
+Page Blob Support and Configuration
+===================================
+
+The Azure Blob Storage interface for Hadoop supports two kinds of blobs, block blobs
+and page blobs. Block blobs are the default kind of blob and are good for most 
+big-data use cases, like input data for Hive, Pig, analytical map-reduce jobs etc. 
+Page blob handling in hadoop-azure was introduced to support HBase log files. 
+Page blobs can be written any number of times, whereas block blobs can only be 
+appended to 50,000 times before you run out of blocks and your writes will fail.
+That won't work for HBase logs, so page blob support was introduced to overcome
+this limitation.
+
+Page blobs can be used for other purposes beyond just HBase log files though.
+They support the Hadoop FileSystem interface. Page blobs can be up to 1TB in
+size, larger than the maximum 200GB size for block blobs.
+
+In order to have the files you create be page blobs, you must set the configuration
+variable fs.azure.page.blob.dir to a comma-separated list of folder names.
+E.g. 
+
+    /hbase/WALs,/hbase/oldWALs,/data/mypageblobfiles
+    
+You can set this to simply / to make all files page blobs.
+
+The configuration option fs.azure.page.blob.size is the default initial 
+size for a page blob. It must be 128MB or greater, and no more than 1TB,
+specified as an integer number of bytes.
+
+====================
+Atomic Folder Rename
+====================
+
+Azure storage stores files as a flat key/value store without formal support
+for folders. The hadoop-azure file system layer simulates folders on top
+of Azure storage. By default, folder rename in the hadoop-azure file system
+layer is not atomic. That means that a failure during a folder rename 
+could, for example, leave some folders in the original directory and
+some in the new one.
+
+HBase depends on atomic folder rename. Hence, a configuration setting was
+introduced called fs.azure.atomic.rename.dir that allows you to specify a 
+comma-separated list of directories to receive special treatment so that 
+folder rename is made atomic. The default value of this setting is just /hbase.
+Redo will be applied to finish a folder rename that fails. A file  
+<folderName>-renamePending.json may appear temporarily and is the record of 
+the intention of the rename operation, to allow redo in event of a failure. 
 
 =============
 Findbugs
diff --git a/hadoop-tools/hadoop-azure/dev-support/findbugs-exclude.xml b/hadoop-tools/hadoop-azure/dev-support/findbugs-exclude.xml
index cc63141..cde1734 100644
--- a/hadoop-tools/hadoop-azure/dev-support/findbugs-exclude.xml
+++ b/hadoop-tools/hadoop-azure/dev-support/findbugs-exclude.xml
@@ -15,5 +15,36 @@
    limitations under the License.
 -->
 <FindBugsFilter>
+     <!-- It is okay to skip up to end of file. No need to check return value. -->
+     <Match>
+       <Class name="org.apache.hadoop.fs.azure.AzureNativeFileSystemStore" />
+       <Method name="retrieve" />
+       <Bug pattern="SR_NOT_CHECKED" />
+       <Priority value="2" />
+     </Match>
+
+     <!-- Returning fully loaded array to iterate through is a convenience
+	  and helps performance. -->
+     <Match>
+       <Class name="org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending" />
+       <Method name="getFiles" />
+       <Bug pattern="EI_EXPOSE_REP" />
+       <Priority value="2" />
+     </Match>
+
+     <!-- Need to start keep-alive thread for SelfRenewingLease in constructor. -->
+     <Match>
+       <Class name="org.apache.hadoop.fs.azure.SelfRenewingLease" />
+       <Bug pattern="SC_START_IN_CTOR" />
+       <Priority value="2" />
+     </Match>
      
+     <!-- Using a key set iterator is fine because this is not a performance-critical
+	  method. -->
+     <Match>
+       <Class name="org.apache.hadoop.fs.azure.PageBlobOutputStream" />
+       <Method name="logAllStackTraces" />
+       <Bug pattern="WMI_WRONG_MAP_ITERATOR" />
+       <Priority value="2" />
+     </Match>
  </FindBugsFilter>
diff --git a/hadoop-tools/hadoop-azure/src/config/checkstyle.xml b/hadoop-tools/hadoop-azure/src/config/checkstyle.xml
index 3bfc23d..9df4f78 100644
--- a/hadoop-tools/hadoop-azure/src/config/checkstyle.xml
+++ b/hadoop-tools/hadoop-azure/src/config/checkstyle.xml
@@ -108,7 +108,10 @@
           <property name="max" value="3000"/>
         </module>
         
-        <module name="ParameterNumber"/>
+        <module name="ParameterNumber">
+          <property name="max" value="8"/>
+        </module>
+
 
 
         <!-- Checks for whitespace                               -->
@@ -152,7 +155,7 @@
         <module name="IllegalInstantiation"/>
         <module name="InnerAssignment"/>
         <module name="MagicNumber">
-            <property name="ignoreNumbers" value="-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1000"/>
+            <property name="ignoreNumbers" value="-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 255, 1000, 1024"/>
         </module>
         <module name="MissingSwitchDefault"/>
         <module name="RedundantThrows"/>
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureLinkedStack.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureLinkedStack.java
new file mode 100644
index 0000000..4c52ef0
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureLinkedStack.java
@@ -0,0 +1,217 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs.azure;
+
+/**
+ * A simple generic stack implementation using linked lists. The stack
+ * implementation has five main operations:
+ * <ul>
+ * <li>push -- adds an element to the top of the stack</li>
+ * <li>pop -- removes an element from the top of the stack and returns a
+ * reference to it</li>
+ * <li>peek -- peek returns an element from the top of the stack without
+ * removing it</li>
+ * <li>isEmpty -- tests whether the stack is empty</li>
+ * <li>size -- returns the size of the stack</li>
+ * <li>toString -- returns a string representation of the stack.</li>
+ * </ul>
+ */
+
+public class AzureLinkedStack<E> {
+  /*
+   * Linked node for Azure stack collection.
+   */
+  private static class AzureLinkedNode<E> {
+    private E element; // Linked element on the list.
+    private AzureLinkedNode<E> next;// Reference to the next linked element on
+                                    // list.
+
+    /*
+     * The constructor builds the linked node with no successor
+     *
+     * @param element : The value of the element to be stored with this node.
+     */
+    private AzureLinkedNode(E anElement) {
+      element = anElement;
+      next = null;
+    }
+
+    /*
+     * Constructor builds a linked node with a specified successor. The
+     * successor may be null.
+     *
+     * @param anElement : new element to be created.
+     *
+     * @param nextElement: successor to the new element.
+     */
+    private AzureLinkedNode(E anElement, AzureLinkedNode<E> nextElement) {
+      element = anElement;
+      next = nextElement;
+    }
+
+    /*
+     * Get the element stored in the linked node.
+     *
+     * @return E : element stored in linked node.
+     */
+    private E getElement() {
+      return element;
+    }
+
+    /*
+     * Get the successor node to the element.
+     *
+     * @return E : reference to the succeeding node on the list.
+     */
+    private AzureLinkedNode<E> getNext() {
+      return next;
+    }
+  }
+
+  private int count; // The number of elements stored on the stack.
+  private AzureLinkedNode<E> top; // Top of the stack.
+
+  /*
+   * Constructor creating an empty stack.
+   */
+  public AzureLinkedStack() {
+    // Simply initialize the member variables.
+    //
+    count = 0;
+    top = null;
+  }
+
+  /*
+   * Adds an element to the top of the stack.
+   *
+   * @param element : element pushed to the top of the stack.
+   */
+  public void push(E element) {
+    // Create a new node containing a reference to be placed on the stack.
+    // Set the next reference to the new node to point to the current top
+    // of the stack. Set the top reference to point to the new node. Finally
+    // increment the count of nodes on the stack.
+    //
+    AzureLinkedNode<E> newNode = new AzureLinkedNode<E>(element, top);
+    top = newNode;
+    count++;
+  }
+
+  /*
+   * Removes the element at the top of the stack and returns a reference to it.
+   *
+   * @return E : element popped from the top of the stack.
+   *
+   * @throws Exception on pop from an empty stack.
+   */
+  public E pop() throws Exception {
+    // Make sure the stack is not empty. If it is empty, throw a StackEmpty
+    // exception.
+    //
+    if (isEmpty()) {
+      throw new Exception("AzureStackEmpty");
+    }
+
+    // Set a temporary reference equal to the element at the top of the stack,
+    // decrement the count of elements and return reference to the temporary.
+    //
+    E element = top.getElement();
+    top = top.getNext();
+    count--;
+
+    // Return the reference to the element that was at the top of the stack.
+    //
+    return element;
+  }
+
+  /*
+   * Return the top element of the stack without removing it.
+   *
+   * @return E
+   *
+   * @throws Exception on peek into an empty stack.
+   */
+  public E peek() throws Exception {
+    // Make sure the stack is not empty. If it is empty, throw a StackEmpty
+    // exception.
+    //
+    if (isEmpty()) {
+      throw new Exception("AzureStackEmpty");
+    }
+
+    // Set a temporary reference equal to the element at the top of the stack
+    // and return the temporary.
+    //
+    E element = top.getElement();
+    return element;
+  }
+
+  /*
+   * Determines whether the stack is empty
+   *
+   * @return boolean true if the stack is empty and false otherwise.
+   */
+  public boolean isEmpty() {
+    if (0 == size()) {
+      // Zero-sized stack so the stack is empty.
+      //
+      return true;
+    }
+
+    // The stack is not empty.
+    //
+    return false;
+  }
+
+  /*
+   * Determines the size of the stack
+   *
+   * @return int: Count of the number of elements in the stack.
+   */
+  public int size() {
+    return count;
+  }
+
+  /*
+   * Returns a string representation of the stack.
+   *
+   * @return String String representation of all elements in the stack.
+   */
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+
+    AzureLinkedNode<E> current = top;
+    for (int i = 0; i < size(); i++) {
+      E element = current.getElement();
+      sb.append(element.toString());
+      current = current.getNext();
+
+      // Insert commas between strings except after the last string.
+      //
+      if (size() - 1 > i) {
+        sb.append(", ");
+      }
+    }
+
+    // Return the string.
+    //
+    return sb.toString();
+  }
+}
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java
index 5afbbbe..c091767 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java
@@ -20,31 +20,42 @@
 import static org.apache.hadoop.fs.azure.NativeAzureFileSystem.PATH_DELIMITER;
 
 import java.io.BufferedInputStream;
-import java.io.ByteArrayInputStream;
 import java.io.DataInputStream;
 import java.io.DataOutputStream;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
+import java.io.UnsupportedEncodingException;
 import java.net.URI;
 import java.net.URISyntaxException;
+import java.net.URLDecoder;
+import java.net.URLEncoder;
 import java.security.InvalidKeyException;
 import java.util.ArrayList;
 import java.util.Calendar;
+import java.util.Collections;
 import java.util.Date;
 import java.util.EnumSet;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Iterator;
 import java.util.LinkedList;
+import java.util.Locale;
 import java.util.Map;
+import java.util.Set;
 
+import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.azure.StorageInterface.CloudBlobContainerWrapper;
 import org.apache.hadoop.fs.azure.StorageInterface.CloudBlobDirectoryWrapper;
+import org.apache.hadoop.fs.azure.StorageInterface.CloudBlobWrapper;
 import org.apache.hadoop.fs.azure.StorageInterface.CloudBlockBlobWrapper;
+import org.apache.hadoop.fs.azure.StorageInterface.CloudPageBlobWrapper;
+import org.apache.hadoop.fs.azure.StorageInterfaceImpl.CloudPageBlobWrapperImpl;
 import org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation;
 import org.apache.hadoop.fs.azure.metrics.BandwidthGaugeUpdater;
 import org.apache.hadoop.fs.azure.metrics.ErrorMetricUpdater;
@@ -72,7 +83,6 @@
 import com.microsoft.windowsazure.storage.blob.ListBlobItem;
 import com.microsoft.windowsazure.storage.core.Utility;
 
-
 /**
  * Core implementation of Windows Azure Filesystem for Hadoop.
  * Provides the bridging logic between Hadoop's abstract filesystem and Azure Storage 
@@ -140,6 +150,33 @@
   static final String LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY = "hdi_tmpupload";
   static final String OLD_LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY = "asv_tmpupload";
 
+  /**
+   * Configuration key to indicate the set of directories in WASB where we
+   * should store files as page blobs instead of block blobs.
+   *
+   * Entries should be plain directory names (i.e. not URIs) with no leading or
+   * trailing slashes. Delimit the entries with commas.
+   */
+  public static final String KEY_PAGE_BLOB_DIRECTORIES =
+      "fs.azure.page.blob.dir";
+  /**
+   * The set of directories where we should store files as page blobs.
+   */
+  private Set<String> pageBlobDirs;
+  
+  /**
+   * Configuration key to indicate the set of directories in WASB where
+   * we should do atomic folder rename synchronized with createNonRecursive.
+   */
+  public static final String KEY_ATOMIC_RENAME_DIRECTORIES =
+      "fs.azure.atomic.rename.dir";
+
+  /**
+   * The set of directories where we should apply atomic folder rename
+   * synchronized with createNonRecursive.
+   */
+  private Set<String> atomicRenameDirs;
+
   private static final String HTTP_SCHEME = "http";
   private static final String HTTPS_SCHEME = "https";
   private static final String WASB_AUTHORITY_DELIMITER = "@";
@@ -148,6 +185,7 @@
   private static final int DEFAULT_CONCURRENT_WRITES = 8;
 
   // Concurrent reads reads of data written out of band are disable by default.
+  //
   private static final boolean DEFAULT_READ_TOLERATE_CONCURRENT_APPEND = false;
 
   // Default block sizes
@@ -155,6 +193,8 @@
   public static final int DEFAULT_UPLOAD_BLOCK_SIZE = 4 * 1024 * 1024;
 
   // Retry parameter defaults.
+  //
+
   private static final int DEFAULT_MIN_BACKOFF_INTERVAL = 1 * 1000; // 1s
   private static final int DEFAULT_MAX_BACKOFF_INTERVAL = 30 * 1000; // 30s
   private static final int DEFAULT_BACKOFF_INTERVAL = 1 * 1000; // 1s
@@ -169,6 +209,7 @@
 
   private static final int STORAGE_CONNECTION_TIMEOUT_DEFAULT = 90;
 
+
   /**
    * MEMBER VARIABLES
    */
@@ -181,7 +222,7 @@
   private boolean connectingUsingSAS = false;
   private AzureFileSystemInstrumentation instrumentation;
   private BandwidthGaugeUpdater bandwidthGaugeUpdater;
-  private static final JSON PERMISSION_JSON_SERIALIZER = createPermissionJsonSerializer();
+  private final static JSON PERMISSION_JSON_SERIALIZER = createPermissionJsonSerializer();
 
   private boolean suppressRetryPolicy = false;
   private boolean canCreateOrModifyContainer = false;
@@ -317,7 +358,7 @@ void setAzureStorageInteractionLayer(StorageInterface storageInteractionLayer) {
   public BandwidthGaugeUpdater getBandwidthGaugeUpdater() {
     return bandwidthGaugeUpdater;
   }
-  
+
   /**
    * Check if concurrent reads and writes on the same blob are allowed.
    * 
@@ -333,19 +374,18 @@ private boolean isConcurrentOOBAppendAllowed() {
    * session with an Azure session. It parses the scheme to ensure it matches
    * the storage protocol supported by this file system.
    * 
-   * @param uri
-   *          - URI for target storage blob.
-   * @param conf
-   *          - reference to configuration object.
+   * @param uri - URI for target storage blob.
+   * @param conf - reference to configuration object.
+   * @param instrumentation - the metrics source that will keep track of operations here.
    * 
-   * @throws IllegalArgumentException
-   *           if URI or job object is null, or invalid scheme.
+   * @throws IllegalArgumentException if URI or job object is null, or invalid scheme.
    */
   @Override
-  public void initialize(URI uri, Configuration conf, AzureFileSystemInstrumentation instrumentation) throws AzureException {
-
-    if (null == this.storageInteractionLayer) {
-      this.storageInteractionLayer = new StorageInterfaceImpl();
+  public void initialize(URI uri, Configuration conf, AzureFileSystemInstrumentation instrumentation)
+      throws IllegalArgumentException, AzureException, IOException  {
+    
+    if (null == instrumentation) {
+      throw new IllegalArgumentException("Null instrumentation");
     }
 
     this.instrumentation = instrumentation;
@@ -377,6 +417,40 @@ public void initialize(URI uri, Configuration conf, AzureFileSystemInstrumentati
     // Start an Azure storage session.
     //
     createAzureStorageSession();
+
+    // Extract the directories that should contain page blobs
+    pageBlobDirs = getDirectorySet(KEY_PAGE_BLOB_DIRECTORIES);
+    LOG.debug("Page blob directories:  " + setToString(pageBlobDirs));
+
+    // Extract directories that should have atomic rename applied.
+    atomicRenameDirs = getDirectorySet(KEY_ATOMIC_RENAME_DIRECTORIES);
+    String hbaseRoot;
+    try {
+
+      // Add to this the hbase root directory, or /hbase is that is not set.
+      hbaseRoot = verifyAndConvertToStandardFormat(
+          sessionConfiguration.get("hbase.rootdir", "hbase"));
+      atomicRenameDirs.add(hbaseRoot);
+    } catch (URISyntaxException e) {
+      LOG.warn("Unable to initialize HBase root as an atomic rename directory.");
+    }
+    LOG.debug("Atomic rename directories:  " + setToString(atomicRenameDirs));
+  }
+
+  /**
+   * Helper to format a string for log output from Set<String>
+   */
+  private String setToString(Set<String> set) {
+    StringBuilder sb = new StringBuilder();
+    int i = 1;
+    for (String s : set) {
+      sb.append("/" + s);
+      if (i != set.size()) {
+        sb.append(", ");
+      }
+      i++;
+    }
+    return sb.toString();
   }
 
   /**
@@ -400,8 +474,7 @@ private String getAccountFromAuthority(URI uri) throws URISyntaxException {
           "Expected URI with a valid authority");
     }
 
-    // Check if authority container the delimiter separating the account name
-    // from the
+    // Check if authority container the delimiter separating the account name from the
     // the container.
     //
     if (!authority.contains(WASB_AUTHORITY_DELIMITER)) {
@@ -455,8 +528,7 @@ private String getContainerFromAuthority(URI uri) throws URISyntaxException {
     // The URI has a valid authority. Extract the container name. It is the
     // second component of the WASB URI authority.
     if (!authority.contains(WASB_AUTHORITY_DELIMITER)) {
-      // The authority does not have a container name. Use the default container
-      // by
+      // The authority does not have a container name. Use the default container by
       // setting the container name to the default Azure root container.
       //
       return AZURE_ROOT_CONTAINER;
@@ -491,9 +563,9 @@ private String getContainerFromAuthority(URI uri) throws URISyntaxException {
   private String getHTTPScheme() {
     String sessionScheme = sessionUri.getScheme();
     // Check if we're on a secure URI scheme: wasbs or the legacy asvs scheme.
-    if (sessionScheme != null
-        && (sessionScheme.equalsIgnoreCase("asvs") || sessionScheme
-            .equalsIgnoreCase("wasbs"))) {
+    if (sessionScheme != null &&
+        (sessionScheme.equalsIgnoreCase("asvs") ||
+         sessionScheme.equalsIgnoreCase("wasbs"))) {
       return HTTPS_SCHEME;
     } else {
       // At this point the scheme should be either null or asv or wasb.
@@ -565,20 +637,22 @@ private void configureAzureStorageSession() throws AzureException {
         Math.min(cpuCores, DEFAULT_CONCURRENT_WRITES));
 
     // Set up the exponential retry policy.
-    minBackoff = sessionConfiguration.getInt(KEY_MIN_BACKOFF_INTERVAL,
-        DEFAULT_MIN_BACKOFF_INTERVAL);
+    //
+    minBackoff = sessionConfiguration.getInt(
+        KEY_MIN_BACKOFF_INTERVAL, DEFAULT_MIN_BACKOFF_INTERVAL);
+
+    maxBackoff = sessionConfiguration.getInt(
+        KEY_MAX_BACKOFF_INTERVAL, DEFAULT_MAX_BACKOFF_INTERVAL);
 
-    maxBackoff = sessionConfiguration.getInt(KEY_MAX_BACKOFF_INTERVAL,
-        DEFAULT_MAX_BACKOFF_INTERVAL);
+    deltaBackoff = sessionConfiguration.getInt(
+        KEY_BACKOFF_INTERVAL, DEFAULT_BACKOFF_INTERVAL);
 
-    deltaBackoff = sessionConfiguration.getInt(KEY_BACKOFF_INTERVAL,
-        DEFAULT_BACKOFF_INTERVAL);
+    maxRetries = sessionConfiguration.getInt(
+        KEY_MAX_IO_RETRIES, DEFAULT_MAX_RETRY_ATTEMPTS);
 
-    maxRetries = sessionConfiguration.getInt(KEY_MAX_IO_RETRIES,
-        DEFAULT_MAX_RETRY_ATTEMPTS);
+    storageInteractionLayer.setRetryPolicyFactory(
+          new RetryExponentialRetry(minBackoff, deltaBackoff, maxBackoff, maxRetries));
 
-    storageInteractionLayer.setRetryPolicyFactory(new RetryExponentialRetry(
-        minBackoff, deltaBackoff, maxBackoff, maxRetries));
 
     // read the self-throttling config.
     selfThrottlingEnabled = sessionConfiguration.getBoolean(
@@ -659,13 +733,15 @@ private void connectUsingCredentials(String accountName,
       StorageCredentials credentials, String containerName)
       throws URISyntaxException, StorageException, AzureException {
 
+    URI blobEndPoint;
     if (isStorageEmulatorAccount(accountName)) {
       isStorageEmulator = true;
-      CloudStorageAccount account = CloudStorageAccount
-          .getDevelopmentStorageAccount();
+      CloudStorageAccount account =
+          CloudStorageAccount.getDevelopmentStorageAccount();
       storageInteractionLayer.createBlobClient(account);
     } else {
-      URI blobEndPoint = new URI(getHTTPScheme() + "://" + accountName);
+      blobEndPoint = new URI(getHTTPScheme() + "://" +
+          accountName);
       storageInteractionLayer.createBlobClient(blobEndPoint, credentials);
     }
     suppressRetryPolicyInClientIfNeeded();
@@ -753,7 +829,8 @@ static String getAccountKeyFromConfiguration(String accountName,
    * @throws AzureException
    * @throws IOException
    */
-  private void createAzureStorageSession() throws AzureException {
+  private void createAzureStorageSession ()
+      throws AzureException, IOException {
 
     // Make sure this object was properly initialized with references to
     // the sessionUri and sessionConfiguration.
@@ -886,6 +963,106 @@ private void createAzureStorageSession() throws AzureException {
   }
 
   /**
+   * Trims a suffix/prefix from the given string. For example if
+   * s is given as "/xy" and toTrim is "/", this method returns "xy"
+   */
+  private static String trim(String s, String toTrim) {
+    return StringUtils.removeEnd(StringUtils.removeStart(s, toTrim),
+        toTrim);
+  }
+
+  /**
+   * Checks if the given rawDir belongs to this account/container, and
+   * if so returns the canonicalized path for it. Otherwise return null.
+   */
+  private String verifyAndConvertToStandardFormat(String rawDir) throws URISyntaxException {
+    URI asUri = new URI(rawDir);
+    if (asUri.getAuthority() == null 
+        || asUri.getAuthority().toLowerCase(Locale.US).equalsIgnoreCase(
+        		sessionUri.getAuthority().toLowerCase(Locale.US))) {
+      // Applies to me.
+      return trim(asUri.getPath(), "/");
+    } else {
+      // Doen't apply to me.
+      return null;
+    }
+  }
+
+  /**
+   * Take a comma-separated list of directories from a configuration variable
+   * and transform it to a set of directories.
+   */
+  private Set<String> getDirectorySet(final String configVar)
+      throws AzureException {
+    String[] rawDirs = sessionConfiguration.getStrings(configVar, new String[0]);
+    Set<String> directorySet = new HashSet<String>();
+    for (String currentDir : rawDirs) {
+      String myDir;
+      try {
+        myDir = verifyAndConvertToStandardFormat(currentDir);
+      } catch (URISyntaxException ex) {
+        throw new AzureException(String.format(
+            "The directory %s specified in the configuration entry %s is not" +
+            " a valid URI.",
+            currentDir, configVar));
+      }
+      if (myDir != null) {
+        directorySet.add(myDir);
+      }
+    }
+    return directorySet;
+  }
+
+  /**
+   * Checks if the given key in Azure Storage should be stored as a page
+   * blob instead of block blob.
+   * @throws URISyntaxException
+   */
+  public boolean isPageBlobKey(String key) {
+    return isKeyForDirectorySet(key, pageBlobDirs);
+  }
+
+  /**
+   * Checks if the given key in Azure storage should have synchronized
+   * atomic folder rename createNonRecursive implemented.
+   */
+  @Override
+  public boolean isAtomicRenameKey(String key) {
+    return isKeyForDirectorySet(key, atomicRenameDirs);
+  }
+
+  public boolean isKeyForDirectorySet(String key, Set<String> dirSet) {
+    String defaultFS = FileSystem.getDefaultUri(sessionConfiguration).toString();
+    for (String dir : dirSet) {
+      if (dir.isEmpty() ||
+          key.startsWith(dir + "/")) {
+        return true;
+      }
+
+      // Allow for blob directories with paths relative to the default file
+      // system.
+      //
+      try {
+        URI uriPageBlobDir = new URI (dir);
+        if (null == uriPageBlobDir.getAuthority()) {
+          // Concatenate the default file system prefix with the relative
+          // page blob directory path.
+          //
+          if (key.startsWith(trim(defaultFS, "/") + "/" + dir + "/")){
+            return true;
+          }
+        }
+      } catch (URISyntaxException e) {
+        LOG.info(String.format(
+                   "URI syntax error creating URI for %s", dir));
+      }
+    }
+    return false;
+  }
+
+  
+  
+  /**
    * This should be called from any method that does any modifications to the
    * underlying container: it makes sure to put the WASB current version in the
    * container's metadata if it's not already there.
@@ -1032,15 +1209,15 @@ private BlobRequestOptions getUploadOptions() {
 
   private BlobRequestOptions getDownloadOptions() {
     BlobRequestOptions options = new BlobRequestOptions();
-    options.setRetryPolicyFactory(new RetryExponentialRetry(minBackoff,
-        deltaBackoff, maxBackoff, maxRetries));
+    options.setRetryPolicyFactory(
+          new RetryExponentialRetry(minBackoff, deltaBackoff, maxBackoff, maxRetries));
     options.setUseTransactionalContentMD5(getUseTransactionalContentMD5());
     return options;
   }
 
   @Override
-  public DataOutputStream storefile(String key,
-      PermissionStatus permissionStatus) throws AzureException {
+  public DataOutputStream storefile(String key, PermissionStatus permissionStatus)
+      throws AzureException {
     try {
 
       // Check if a session exists, if not create a session with the
@@ -1066,19 +1243,20 @@ public DataOutputStream storefile(String key,
       checkContainer(ContainerAccessType.PureWrite);
 
       /**
-       * Note: Windows Azure Blob Storage does not allow the creation of
-       * arbitrary directory paths under the default $root directory. This is by
-       * design to eliminate ambiguity in specifying a implicit blob address. A
-       * blob in the $root container cannot include a / in its name and must be
-       * careful not to include a trailing '/' when referencing blobs in the
-       * $root container. A '/; in the $root container permits ambiguous blob
-       * names as in the following example involving two containers $root and
-       * mycontainer: http://myaccount.blob.core.windows.net/$root
-       * http://myaccount.blob.core.windows.net/mycontainer If the URL
-       * "mycontainer/somefile.txt were allowed in $root then the URL:
-       * http://myaccount.blob.core.windows.net/mycontainer/myblob.txt could
-       * mean either: (1) container=mycontainer; blob=myblob.txt (2)
-       * container=$root; blob=mycontainer/myblob.txt
+       * Note: Windows Azure Blob Storage does not allow the creation of arbitrary directory
+       *      paths under the default $root directory.  This is by design to eliminate
+       *      ambiguity in specifying a implicit blob address. A blob in the $root conatiner
+       *      cannot include a / in its name and must be careful not to include a trailing
+       *      '/' when referencing  blobs in the $root container.
+       *      A '/; in the $root container permits ambiguous blob names as in the following
+       *      example involving two containers $root and mycontainer:
+       *                http://myaccount.blob.core.windows.net/$root
+       *                http://myaccount.blob.core.windows.net/mycontainer
+       *      If the URL "mycontainer/somefile.txt were allowed in $root then the URL:
+       *                http://myaccount.blob.core.windows.net/mycontainer/myblob.txt
+       *      could mean either:
+       *        (1) container=mycontainer; blob=myblob.txt
+       *        (2) container=$root; blob=mycontainer/myblob.txt
        * 
        * To avoid this type of ambiguity the Azure blob storage prevents
        * arbitrary path under $root. For a simple and more consistent user
@@ -1097,17 +1275,15 @@ public DataOutputStream storefile(String key,
         throw new AzureException(errMsg);
       }
 
-      // Get the block blob reference from the store's container and
+      // Get the blob reference from the store's container and
       // return it.
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
       storePermissionStatus(blob, permissionStatus);
 
       // Create the output stream for the Azure blob.
-      OutputStream outputStream = blob.openOutputStream(getUploadOptions(),
-          getInstrumentedContext());
-
-      // Return to caller with DataOutput stream.
-      DataOutputStream dataOutStream = new DataOutputStream(outputStream);
+      //
+      OutputStream outputStream = openOutputStream(blob);
+      DataOutputStream dataOutStream = new SyncableDataOutputStream(outputStream);
       return dataOutStream;
     } catch (Exception e) {
       // Caught exception while attempting to open the blob output stream.
@@ -1117,6 +1293,40 @@ public DataOutputStream storefile(String key,
   }
 
   /**
+   * Opens a new output stream to the given blob (page or block blob)
+   * to populate it from scratch with data.
+   */
+  private OutputStream openOutputStream(final CloudBlobWrapper blob)
+      throws StorageException {
+    if (blob instanceof CloudPageBlobWrapperImpl){
+      return new PageBlobOutputStream(
+          (CloudPageBlobWrapper)blob, getInstrumentedContext(), sessionConfiguration);
+    } else {
+
+      // Handle both ClouldBlockBlobWrapperImpl and (only for the test code path)
+      // MockCloudBlockBlobWrapper.
+      return ((CloudBlockBlobWrapper) blob).openOutputStream(getUploadOptions(),
+                getInstrumentedContext());
+    }
+  }
+
+  /**
+   * Opens a new input stream for the given blob (page or block blob)
+   * to read its data.
+   */
+  private InputStream openInputStream(CloudBlobWrapper blob)
+      throws StorageException, IOException {
+    if (blob instanceof CloudBlockBlobWrapper) {
+      return blob.openInputStream(getDownloadOptions(),
+          getInstrumentedContext(isConcurrentOOBAppendAllowed()));
+    } else {
+      return new PageBlobInputStream(
+          (CloudPageBlobWrapper) blob, getInstrumentedContext(
+              isConcurrentOOBAppendAllowed()));
+    }
+  }
+
+  /**
    * Default permission to use when no permission metadata is found.
    * 
    * @return The default permission to use.
@@ -1125,7 +1335,7 @@ private static PermissionStatus defaultPermissionNoBlobMetadata() {
     return new PermissionStatus("", "", FsPermission.getDefault());
   }
 
-  private static void storeMetadataAttribute(CloudBlockBlobWrapper blob,
+  private static void storeMetadataAttribute(CloudBlobWrapper blob,
       String key, String value) {
     HashMap<String, String> metadata = blob.getMetadata();
     if (null == metadata) {
@@ -1135,7 +1345,7 @@ private static void storeMetadataAttribute(CloudBlockBlobWrapper blob,
     blob.setMetadata(metadata);
   }
 
-  private static String getMetadataAttribute(CloudBlockBlobWrapper blob,
+  private static String getMetadataAttribute(CloudBlobWrapper blob,
       String... keyAlternatives) {
     HashMap<String, String> metadata = blob.getMetadata();
     if (null == metadata) {
@@ -1149,7 +1359,7 @@ private static String getMetadataAttribute(CloudBlockBlobWrapper blob,
     return null;
   }
 
-  private static void removeMetadataAttribute(CloudBlockBlobWrapper blob,
+  private static void removeMetadataAttribute(CloudBlobWrapper blob,
       String key) {
     HashMap<String, String> metadata = blob.getMetadata();
     if (metadata != null) {
@@ -1158,7 +1368,7 @@ private static void removeMetadataAttribute(CloudBlockBlobWrapper blob,
     }
   }
 
-  private void storePermissionStatus(CloudBlockBlobWrapper blob,
+  private static void storePermissionStatus(CloudBlobWrapper blob,
       PermissionStatus permissionStatus) {
     storeMetadataAttribute(blob, PERMISSION_METADATA_KEY,
         PERMISSION_JSON_SERIALIZER.toJSON(permissionStatus));
@@ -1166,39 +1376,55 @@ private void storePermissionStatus(CloudBlockBlobWrapper blob,
     removeMetadataAttribute(blob, OLD_PERMISSION_METADATA_KEY);
   }
 
-  private PermissionStatus getPermissionStatus(CloudBlockBlobWrapper blob) {
+  private PermissionStatus getPermissionStatus(CloudBlobWrapper blob) {
     String permissionMetadataValue = getMetadataAttribute(blob,
         PERMISSION_METADATA_KEY, OLD_PERMISSION_METADATA_KEY);
     if (permissionMetadataValue != null) {
-      return PermissionStatusJsonSerializer
-          .fromJSONString(permissionMetadataValue);
+      return PermissionStatusJsonSerializer.fromJSONString(
+          permissionMetadataValue);
     } else {
       return defaultPermissionNoBlobMetadata();
     }
   }
 
-  private static void storeFolderAttribute(CloudBlockBlobWrapper blob) {
+  private static void storeFolderAttribute(CloudBlobWrapper blob) {
     storeMetadataAttribute(blob, IS_FOLDER_METADATA_KEY, "true");
     // Remove the old metadata key if present
     removeMetadataAttribute(blob, OLD_IS_FOLDER_METADATA_KEY);
   }
 
-  private static void storeLinkAttribute(CloudBlockBlobWrapper blob,
-      String linkTarget) {
-    storeMetadataAttribute(blob, LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY,
-        linkTarget);
+  private static void storeLinkAttribute(CloudBlobWrapper blob,
+      String linkTarget) throws UnsupportedEncodingException {
+    // We have to URL encode the link attribute as the link URI could
+    // have URI special characters which unless encoded will result
+    // in 403 errors from the server. This is due to metadata properties
+    // being sent in the HTTP header of the request which is in turn used
+    // on the server side to authorize the request.
+    String encodedLinkTarget = null;
+    if (linkTarget != null) {
+      encodedLinkTarget = URLEncoder.encode(linkTarget, "UTF-8");
+    }
+    storeMetadataAttribute(blob,
+        LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY,
+        encodedLinkTarget);
     // Remove the old metadata key if present
     removeMetadataAttribute(blob,
         OLD_LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY);
   }
 
-  private static String getLinkAttributeValue(CloudBlockBlobWrapper blob) {
-    return getMetadataAttribute(blob,
+  private static String getLinkAttributeValue(CloudBlobWrapper blob)
+      throws UnsupportedEncodingException {
+    String encodedLinkTarget = getMetadataAttribute(blob,
         LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY,
         OLD_LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY);
+    String linkTarget = null;
+    if (encodedLinkTarget != null) {
+      linkTarget = URLDecoder.decode(encodedLinkTarget, "UTF-8");
+    }
+    return linkTarget;
   }
 
-  private static boolean retrieveFolderAttribute(CloudBlockBlobWrapper blob) {
+  private static boolean retrieveFolderAttribute(CloudBlobWrapper blob) {
     HashMap<String, String> metadata = blob.getMetadata();
     return null != metadata
         && (metadata.containsKey(IS_FOLDER_METADATA_KEY) || metadata
@@ -1255,11 +1481,10 @@ public void storeEmptyFolder(String key, PermissionStatus permissionStatus)
     try {
       checkContainer(ContainerAccessType.PureWrite);
 
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
       storePermissionStatus(blob, permissionStatus);
       storeFolderAttribute(blob);
-      blob.upload(new ByteArrayInputStream(new byte[0]),
-          getInstrumentedContext());
+      openOutputStream(blob).close();
     } catch (Exception e) {
       // Caught exception while attempting upload. Re-throw as an Azure
       // storage exception.
@@ -1293,11 +1518,10 @@ public void storeEmptyLinkFile(String key, String tempBlobKey,
     try {
       checkContainer(ContainerAccessType.PureWrite);
 
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
       storePermissionStatus(blob, permissionStatus);
       storeLinkAttribute(blob, tempBlobKey);
-      blob.upload(new ByteArrayInputStream(new byte[0]),
-          getInstrumentedContext());
+      openOutputStream(blob).close();
     } catch (Exception e) {
       // Caught exception while attempting upload. Re-throw as an Azure
       // storage exception.
@@ -1322,7 +1546,7 @@ public String getLinkInFileMetadata(String key) throws AzureException {
     try {
       checkContainer(ContainerAccessType.PureRead);
 
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
       blob.downloadAttributes(getInstrumentedContext());
       return getLinkAttributeValue(blob);
     } catch (Exception e) {
@@ -1366,10 +1590,12 @@ private boolean isAuthenticatedAccess() throws AzureException {
   private Iterable<ListBlobItem> listRootBlobs(boolean includeMetadata)
       throws StorageException, URISyntaxException {
     return rootDirectory.listBlobs(
+        null, false,
+        includeMetadata ?
+            EnumSet.of(BlobListingDetails.METADATA) :
+              EnumSet.noneOf(BlobListingDetails.class),
         null,
-        false,
-        includeMetadata ? EnumSet.of(BlobListingDetails.METADATA) : EnumSet
-            .noneOf(BlobListingDetails.class), null, getInstrumentedContext());
+              getInstrumentedContext());
   }
 
   /**
@@ -1392,11 +1618,14 @@ private boolean isAuthenticatedAccess() throws AzureException {
   private Iterable<ListBlobItem> listRootBlobs(String aPrefix,
       boolean includeMetadata) throws StorageException, URISyntaxException {
 
-    return rootDirectory.listBlobs(
-        aPrefix,
+    Iterable<ListBlobItem> list = rootDirectory.listBlobs(aPrefix,
         false,
-        includeMetadata ? EnumSet.of(BlobListingDetails.METADATA) : EnumSet
-            .noneOf(BlobListingDetails.class), null, getInstrumentedContext());
+        includeMetadata ?
+            EnumSet.of(BlobListingDetails.METADATA) :
+              EnumSet.noneOf(BlobListingDetails.class),
+              null,
+              getInstrumentedContext());
+    return list;
   }
 
   /**
@@ -1423,15 +1652,17 @@ private boolean isAuthenticatedAccess() throws AzureException {
    * @throws URISyntaxException
    * 
    */
-  private Iterable<ListBlobItem> listRootBlobs(String aPrefix,
-      boolean useFlatBlobListing, EnumSet<BlobListingDetails> listingDetails,
-      BlobRequestOptions options, OperationContext opContext)
-      throws StorageException, URISyntaxException {
+  private Iterable<ListBlobItem> listRootBlobs(String aPrefix, boolean useFlatBlobListing,
+      EnumSet<BlobListingDetails> listingDetails, BlobRequestOptions options,
+      OperationContext opContext) throws StorageException, URISyntaxException {
 
-    CloudBlobDirectoryWrapper directory = this.container
-        .getDirectoryReference(aPrefix);
-    return directory.listBlobs(null, useFlatBlobListing, listingDetails,
-        options, opContext);
+    CloudBlobDirectoryWrapper directory =  this.container.getDirectoryReference(aPrefix);
+    return directory.listBlobs(
+        null,
+        useFlatBlobListing,
+        listingDetails,
+        options,
+        opContext);
   }
 
   /**
@@ -1447,15 +1678,18 @@ private boolean isAuthenticatedAccess() throws AzureException {
    * @throws URISyntaxException
    * 
    */
-  private CloudBlockBlobWrapper getBlobReference(String aKey)
+  private CloudBlobWrapper getBlobReference(String aKey)
       throws StorageException, URISyntaxException {
 
-    CloudBlockBlobWrapper blob = this.container.getBlockBlobReference(aKey);
-
+    CloudBlobWrapper blob = null;
+    if (isPageBlobKey(aKey)) {
+      blob = this.container.getPageBlobReference(aKey);
+    } else {
+      blob = this.container.getBlockBlobReference(aKey);
     blob.setStreamMinimumReadSizeInBytes(downloadBlockSizeBytes);
     blob.setWriteBlockSizeInBytes(uploadBlockSizeBytes);
+    }
 
-    // Return with block blob.
     return blob;
   }
 
@@ -1492,7 +1726,7 @@ private String normalizeKey(URI keyUri) {
    * 
    * @returns normKey
    */
-  private String normalizeKey(CloudBlockBlobWrapper blob) {
+  private String normalizeKey(CloudBlobWrapper blob) {
     return normalizeKey(blob.getUri());
   }
 
@@ -1552,20 +1786,19 @@ private OperationContext getInstrumentedContext(boolean bindConcurrentOOBIo) {
         instrumentation,
         bandwidthGaugeUpdater);
     
-    // Bind operation context to receive send request callbacks on this
-    // operation.
-    // If reads concurrent to OOB writes are allowed, the interception will
-    // reset the conditional header on all Azure blob storage read requests.
+    // Bind operation context to receive send request callbacks on this operation.
+    // If reads concurrent to OOB writes are allowed, the interception will reset
+    // the conditional header on all Azure blob storage read requests.
     if (bindConcurrentOOBIo) {
       SendRequestIntercept.bind(storageInteractionLayer.getCredentials(),
           operationContext, true);
     }
 
     if (testHookOperationContext != null) {
-      operationContext = testHookOperationContext
-          .modifyOperationContext(operationContext);
+      operationContext =
+          testHookOperationContext.modifyOperationContext(operationContext);
     }
-    
+
     ErrorMetricUpdater.hook(operationContext, instrumentation);
 
     // Return the operation context.
@@ -1605,7 +1838,7 @@ public FileMetadata retrieveMetadata(String key) throws IOException {
             BlobMaterialization.Implicit);
       }
 
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
 
       // Download attributes and return file metadata only if the blob
       // exists.
@@ -1634,7 +1867,8 @@ public FileMetadata retrieveMetadata(String key) throws IOException {
 
           return new FileMetadata(
               key, // Always return denormalized key with metadata.
-              properties.getLength(), properties.getLastModified().getTime(),
+              getDataLength(blob, properties),
+              properties.getLastModified().getTime(),
               getPermissionStatus(blob));
         }
       }
@@ -1642,17 +1876,23 @@ public FileMetadata retrieveMetadata(String key) throws IOException {
       // There is no file with that key name, but maybe it is a folder.
       // Query the underlying folder/container to list the blobs stored
       // there under that key.
-      Iterable<ListBlobItem> objects = listRootBlobs(key, true,
-          EnumSet.of(BlobListingDetails.METADATA), null,
+      //
+      Iterable<ListBlobItem> objects =
+          listRootBlobs(
+              key,
+              true,
+              EnumSet.of(BlobListingDetails.METADATA),
+              null,
           getInstrumentedContext());
 
       // Check if the directory/container has the blob items.
       for (ListBlobItem blobItem : objects) {
-        if (blobItem instanceof CloudBlockBlobWrapper) {
+        if (blobItem instanceof CloudBlockBlobWrapper
+            || blobItem instanceof CloudPageBlobWrapper) {
           LOG.debug("Found blob as a directory-using this file under it to infer its properties "
               + blobItem.getUri());
 
-          blob = (CloudBlockBlobWrapper) blobItem;
+          blob = (CloudBlobWrapper) blobItem;
           // The key specifies a directory. Create a FileMetadata object which
           // specifies as such.
           BlobProperties properties = blob.getProperties();
@@ -1672,10 +1912,7 @@ public FileMetadata retrieveMetadata(String key) throws IOException {
   }
 
   @Override
-  public DataInputStream retrieve(String key) throws AzureException {
-    InputStream inStream = null;
-    BufferedInputStream inBufStream = null;
-    try {
+  public DataInputStream retrieve(String key) throws AzureException, IOException {
       try {
         // Check if a session exists, if not create a session with the
         // Azure storage server.
@@ -1688,27 +1925,13 @@ public DataInputStream retrieve(String key) throws AzureException {
         checkContainer(ContainerAccessType.PureRead);
 
         // Get blob reference and open the input buffer stream.
-        CloudBlockBlobWrapper blob = getBlobReference(key);
-        inStream = blob.openInputStream(getDownloadOptions(),
-            getInstrumentedContext(isConcurrentOOBAppendAllowed()));
-
-        inBufStream = new BufferedInputStream(inStream);
+        CloudBlobWrapper blob = getBlobReference(key);
+      BufferedInputStream inBufStream = new BufferedInputStream(
+          openInputStream(blob));
 
         // Return a data input stream.
         DataInputStream inDataStream = new DataInputStream(inBufStream);
         return inDataStream;
-      }
-      catch (Exception e){
-        // close the streams on error.
-        // We use nested try-catch as stream.close() can throw IOException.
-        if(inBufStream != null){
-          inBufStream.close();
-        }
-        if(inStream != null){
-          inStream.close();
-        }
-        throw e;
-      }
     } catch (Exception e) {
       // Re-throw as an Azure storage exception.
       throw new AzureException(e);
@@ -1717,11 +1940,7 @@ public DataInputStream retrieve(String key) throws AzureException {
 
   @Override
   public DataInputStream retrieve(String key, long startByteOffset)
-      throws AzureException {
-
-    InputStream in = null;
-    DataInputStream inDataStream = null;
-    try {
+      throws AzureException, IOException {
       try {
         // Check if a session exists, if not create a session with the
         // Azure storage server.
@@ -1734,31 +1953,20 @@ public DataInputStream retrieve(String key, long startByteOffset)
         checkContainer(ContainerAccessType.PureRead);
 
         // Get blob reference and open the input buffer stream.
-        CloudBlockBlobWrapper blob = getBlobReference(key);
+        CloudBlobWrapper blob = getBlobReference(key);
 
         // Open input stream and seek to the start offset.
-        in = blob.openInputStream(getDownloadOptions(),
-            getInstrumentedContext(isConcurrentOOBAppendAllowed()));
+        InputStream in = blob.openInputStream(
+          getDownloadOptions(), getInstrumentedContext(isConcurrentOOBAppendAllowed()));
 
         // Create a data input stream.
-        inDataStream = new DataInputStream(in);
-        long skippedBytes = inDataStream.skip(startByteOffset);
-        if (skippedBytes != startByteOffset) {
-          throw new IOException("Couldn't skip the requested number of bytes");
-        }
+	    DataInputStream inDataStream = new DataInputStream(in);
+	    
+	    // Skip bytes and ignore return value. This is okay
+	    // because if you try to skip too far you will be positioned
+	    // at the end and reads will not return data.
+	    inDataStream.skip(startByteOffset);
         return inDataStream;
-      }
-      catch (Exception e){
-        // close the streams on error.
-        // We use nested try-catch as stream.close() can throw IOException.
-        if(inDataStream != null){
-          inDataStream.close();
-        }
-        if(in != null){
-          in.close();
-        }
-        throw e;
-      }
     } catch (Exception e) {
       // Re-throw as an Azure storage exception.
       throw new AzureException(e);
@@ -1825,13 +2033,14 @@ private PartialListing list(String prefix, String delimiter,
       for (ListBlobItem blobItem : objects) {
         // Check that the maximum listing count is not exhausted.
         //
-        if (0 < maxListingCount && fileMetadata.size() >= maxListingCount) {
+        if (0 < maxListingCount
+            && fileMetadata.size() >= maxListingCount) {
           break;
         }
 
-        if (blobItem instanceof CloudBlockBlobWrapper) {
+        if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {
           String blobKey = null;
-          CloudBlockBlobWrapper blob = (CloudBlockBlobWrapper) blobItem;
+          CloudBlobWrapper blob = (CloudBlobWrapper) blobItem;
           BlobProperties properties = blob.getProperties();
 
           // Determine format of the blob name depending on whether an absolute
@@ -1840,11 +2049,14 @@ private PartialListing list(String prefix, String delimiter,
 
           FileMetadata metadata;
           if (retrieveFolderAttribute(blob)) {
-            metadata = new FileMetadata(blobKey, properties.getLastModified()
-                .getTime(), getPermissionStatus(blob),
+            metadata = new FileMetadata(blobKey,
+                properties.getLastModified().getTime(),
+                getPermissionStatus(blob),
                 BlobMaterialization.Explicit);
           } else {
-            metadata = new FileMetadata(blobKey, properties.getLength(),
+            metadata = new FileMetadata(
+                blobKey,
+                getDataLength(blob, properties),
                 properties.getLastModified().getTime(),
                 getPermissionStatus(blob));
           }
@@ -1890,9 +2102,11 @@ private PartialListing list(String prefix, String delimiter,
       }
       // Note: Original code indicated that this may be a hack.
       priorLastKey = null;
-      return new PartialListing(priorLastKey,
+      PartialListing listing = new PartialListing(priorLastKey,
           fileMetadata.toArray(new FileMetadata[] {}),
-          0 == fileMetadata.size() ? new String[] {} : new String[] { prefix });
+          0 == fileMetadata.size() ? new String[] {}
+      : new String[] { prefix });
+      return listing;
     } catch (Exception e) {
       // Re-throw as an Azure storage exception.
       //
@@ -1919,7 +2133,9 @@ private void buildUpList(CloudBlobDirectoryWrapper aCloudBlobDirectory,
       final int maxListingDepth) throws Exception {
 
     // Push the blob directory onto the stack.
-    LinkedList<Iterator<ListBlobItem>> dirIteratorStack = new LinkedList<Iterator<ListBlobItem>>();
+    //
+    AzureLinkedStack<Iterator<ListBlobItem>> dirIteratorStack =
+        new AzureLinkedStack<Iterator<ListBlobItem>>();
 
     Iterable<ListBlobItem> blobItems = aCloudBlobDirectory.listBlobs(null,
         false, EnumSet.of(BlobListingDetails.METADATA), null,
@@ -1958,9 +2174,10 @@ private void buildUpList(CloudBlobDirectoryWrapper aCloudBlobDirectory,
 
         // Add the file metadata to the list if this is not a blob
         // directory item.
-        if (blobItem instanceof CloudBlockBlobWrapper) {
+        //
+        if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {
           String blobKey = null;
-          CloudBlockBlobWrapper blob = (CloudBlockBlobWrapper) blobItem;
+          CloudBlobWrapper blob = (CloudBlobWrapper) blobItem;
           BlobProperties properties = blob.getProperties();
 
           // Determine format of the blob name depending on whether an absolute
@@ -1969,11 +2186,14 @@ private void buildUpList(CloudBlobDirectoryWrapper aCloudBlobDirectory,
 
           FileMetadata metadata;
           if (retrieveFolderAttribute(blob)) {
-            metadata = new FileMetadata(blobKey, properties.getLastModified()
-                .getTime(), getPermissionStatus(blob),
+            metadata = new FileMetadata(blobKey,
+                properties.getLastModified().getTime(),
+                getPermissionStatus(blob),
                 BlobMaterialization.Explicit);
           } else {
-            metadata = new FileMetadata(blobKey, properties.getLength(),
+            metadata = new FileMetadata(
+                blobKey,
+                getDataLength(blob, properties),
                 properties.getLastModified().getTime(),
                 getPermissionStatus(blob));
           }
@@ -2016,7 +2236,9 @@ private void buildUpList(CloudBlobDirectoryWrapper aCloudBlobDirectory,
               // Note: Something smarter should be done about permissions. Maybe
               // inherit the permissions of the first non-directory blob.
               // Also, getting a proper value for last-modified is tricky.
-              FileMetadata directoryMetadata = new FileMetadata(dirKey, 0,
+              //
+              FileMetadata directoryMetadata = new FileMetadata(dirKey,
+                  0,
                   defaultPermissionNoBlobMetadata(),
                   BlobMaterialization.Implicit);
 
@@ -2050,26 +2272,48 @@ private void buildUpList(CloudBlobDirectoryWrapper aCloudBlobDirectory,
   }
 
   /**
-   * Deletes the given blob, taking special care that if we get a blob-not-found
-   * exception upon retrying the operation, we just swallow the error since what
-   * most probably happened is that the first operation succeeded on the server.
-   * 
-   * @param blob
-   *          The blob to delete.
+   * Return the actual data length of the blob with the specified properties.
+   * If it is a page blob, you can't rely on the length from the properties
+   * argument and you must get it from the file. Otherwise, you can.
+   */
+  private long getDataLength(CloudBlobWrapper blob, BlobProperties properties)
+    throws AzureException {
+    if (blob instanceof CloudPageBlobWrapper) {
+      try {
+        return PageBlobInputStream.getPageBlobSize((CloudPageBlobWrapper) blob,
+            getInstrumentedContext(
+                isConcurrentOOBAppendAllowed()));
+      } catch (Exception e) {
+        throw new AzureException(
+            "Unexpected exception getting page blob actual data size.", e);
+      }
+    }
+    return properties.getLength();
+  }
+
+  /**
+   * Deletes the given blob, taking special care that if we get a
+   * blob-not-found exception upon retrying the operation, we just
+   * swallow the error since what most probably happened is that
+   * the first operation succeeded on the server.
+   * @param blob The blob to delete.
+   * @param leaseID A string identifying the lease, or null if no
+   *        lease is to be used.
    * @throws StorageException
    */
-  private void safeDelete(CloudBlockBlobWrapper blob) throws StorageException {
+  private void safeDelete(CloudBlobWrapper blob, SelfRenewingLease lease) throws StorageException {
     OperationContext operationContext = getInstrumentedContext();
     try {
-      blob.delete(operationContext);
+      blob.delete(operationContext, lease);
     } catch (StorageException e) {
       // On exception, check that if:
       // 1. It's a BlobNotFound exception AND
       // 2. It got there after one-or-more retries THEN
       // we swallow the exception.
-      if (e.getErrorCode() != null && e.getErrorCode().equals("BlobNotFound")
-          && operationContext.getRequestResults().size() > 1
-          && operationContext.getRequestResults().get(0).getException() != null) {
+      if (e.getErrorCode() != null &&
+          e.getErrorCode().equals("BlobNotFound") &&
+          operationContext.getRequestResults().size() > 1 &&
+          operationContext.getRequestResults().get(0).getException() != null) {
         if (LOG.isDebugEnabled()) {
           LOG.debug("Swallowing delete exception on retry: " + e.getMessage());
         }
@@ -2077,21 +2321,25 @@ private void safeDelete(CloudBlockBlobWrapper blob) throws StorageException {
       } else {
         throw e;
       }
+    } finally {
+      if (lease != null) {
+        lease.free();
+      }
     }
   }
 
   @Override
-  public void delete(String key) throws IOException {
+  public void delete(String key, SelfRenewingLease lease) throws IOException {
     try {
       if (checkContainer(ContainerAccessType.ReadThenWrite) == ContainerState.DoesntExist) {
         // Container doesn't exist, no need to do anything
         return;
       }
 
-      // Get the blob reference an delete it.
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      // Get the blob reference and delete it.
+      CloudBlobWrapper blob = getBlobReference(key);
       if (blob.exists(getInstrumentedContext())) {
-        safeDelete(blob);
+        safeDelete(blob, lease);
       }
     } catch (Exception e) {
       // Re-throw as an Azure storage exception.
@@ -2100,12 +2348,27 @@ public void delete(String key) throws IOException {
   }
 
   @Override
+  public void delete(String key) throws IOException {
+    delete(key, null);
+  }
+
+  @Override
   public void rename(String srcKey, String dstKey) throws IOException {
+    rename(srcKey, dstKey, false, null);
+  }
+
+  @Override
+  public void rename(String srcKey, String dstKey, boolean acquireLease,
+      SelfRenewingLease existingLease) throws IOException {
 
     if (LOG.isDebugEnabled()) {
       LOG.debug("Moving " + srcKey + " to " + dstKey);
     }
 
+    if (acquireLease && existingLease != null) {
+      throw new IOException("Cannot acquire new lease if one already exists.");
+    }
+
     try {
       // Attempts rename may occur before opening any streams so first,
       // check if a session exists, if not create a session with the Azure
@@ -2120,52 +2383,76 @@ public void rename(String srcKey, String dstKey) throws IOException {
       checkContainer(ContainerAccessType.ReadThenWrite);
       // Get the source blob and assert its existence. If the source key
       // needs to be normalized then normalize it.
-      CloudBlockBlobWrapper srcBlob = getBlobReference(srcKey);
+      //
+      CloudBlobWrapper srcBlob = getBlobReference(srcKey);
 
       if (!srcBlob.exists(getInstrumentedContext())) {
-        throw new AzureException("Source blob " + srcKey + " does not exist.");
+        throw new AzureException ("Source blob " + srcKey +
+            " does not exist.");
+      }
+
+      /**
+       * Conditionally get a lease on the source blob to prevent other writers
+       * from changing it. This is used for correctness in HBase when log files
+       * are renamed. It generally should do no harm other than take a little
+       * more time for other rename scenarios. When the HBase master renames a
+       * log file folder, the lease locks out other writers.  This
+       * prevents a region server that the master thinks is dead, but is still
+       * alive, from committing additional updates.  This is different than
+       * when HBase runs on HDFS, where the region server recovers the lease
+       * on a log file, to gain exclusive access to it, before it splits it.
+       */
+      SelfRenewingLease lease = null;
+      if (acquireLease) {
+        lease = srcBlob.acquireLease();
+      } else if (existingLease != null) {
+        lease = existingLease;
       }
 
       // Get the destination blob. The destination key always needs to be
       // normalized.
-      CloudBlockBlobWrapper dstBlob = getBlobReference(dstKey);
+      //
+      CloudBlobWrapper dstBlob = getBlobReference(dstKey);
+
+      // TODO: Remove at the time when we move to Azure Java SDK 1.2+.
+      // This is the workaround provided by Azure Java SDK team to
+      // mitigate the issue with un-encoded x-ms-copy-source HTTP
+      // request header. Azure sdk version before 1.2+ does not encode this
+      // header what causes all URIs that have special (category "other")
+      // characters in the URI not to work with startCopyFromBlob when
+      // specified as source (requests fail with HTTP 403).
+      URI srcUri = new URI(srcBlob.getUri().toASCIIString());
 
       // Rename the source blob to the destination blob by copying it to
       // the destination blob then deleting it.
       //
-      dstBlob.startCopyFromBlob(srcBlob, getInstrumentedContext());
+      dstBlob.startCopyFromBlob(srcUri, getInstrumentedContext());
       waitForCopyToComplete(dstBlob, getInstrumentedContext());
 
-      safeDelete(srcBlob);
+      safeDelete(srcBlob, lease);
     } catch (Exception e) {
       // Re-throw exception as an Azure storage exception.
       throw new AzureException(e);
     }
   }
 
-  private void waitForCopyToComplete(CloudBlockBlobWrapper blob,
-      OperationContext opContext) throws AzureException {
+  private void waitForCopyToComplete(CloudBlobWrapper blob, OperationContext opContext){
     boolean copyInProgress = true;
-    int exceptionCount = 0;
     while (copyInProgress) {
       try {
         blob.downloadAttributes(opContext);
-      } catch (StorageException se) {
-        exceptionCount++;
-        if(exceptionCount > 10){
-          throw new AzureException("Too many storage exceptions during waitForCopyToComplete", se);
         }
+      catch (StorageException se){
       }
 
-      // test for null because mocked filesystem doesn't know about copystates
-      // yet.
-      copyInProgress = (blob.getCopyState() != null && blob.getCopyState()
-          .getStatus() == CopyStatus.PENDING);
+      // test for null because mocked filesystem doesn't know about copystates yet.
+      copyInProgress = (blob.getCopyState() != null && blob.getCopyState().getStatus() == CopyStatus.PENDING);
       if (copyInProgress) {
         try {
           Thread.sleep(1000);
-        } catch (InterruptedException ie) {
-          Thread.currentThread().interrupt();
+          }
+          catch (InterruptedException ie){
+            //ignore
         }
       }
     }
@@ -2179,7 +2466,7 @@ public void changePermissionStatus(String key, PermissionStatus newPermission)
       throws AzureException {
     try {
       checkContainer(ContainerAccessType.ReadThenWrite);
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
       blob.downloadAttributes(getInstrumentedContext());
       storePermissionStatus(blob, newPermission);
       blob.uploadMetadata(getInstrumentedContext());
@@ -2220,28 +2507,51 @@ public void purge(String prefix) throws IOException {
     }
   }
 
+  /**
+   * Get a lease on the blob identified by key. This lease will be renewed
+   * indefinitely by a background thread.
+   */
+  @Override
+  public SelfRenewingLease acquireLease(String key) throws AzureException {
+    LOG.debug("acquiring lease on " + key);
+    try {
+      checkContainer(ContainerAccessType.ReadThenWrite);
+      CloudBlobWrapper blob = getBlobReference(key);
+      return blob.acquireLease();
+    }
+    catch (Exception e) {
+
+      // Caught exception while attempting to get lease. Re-throw as an
+      // Azure storage exception.
+      throw new AzureException(e);
+    }
+  }
+
   @Override
-  public void updateFolderLastModifiedTime(String key, Date lastModified)
+  public void updateFolderLastModifiedTime(String key, Date lastModified,
+      SelfRenewingLease folderLease)
       throws AzureException {
     try {
       checkContainer(ContainerAccessType.ReadThenWrite);
-      CloudBlockBlobWrapper blob = getBlobReference(key);
+      CloudBlobWrapper blob = getBlobReference(key);
       blob.getProperties().setLastModified(lastModified);
-      blob.uploadProperties(getInstrumentedContext());
+      blob.uploadProperties(getInstrumentedContext(), folderLease);
     } catch (Exception e) {
-      // Caught exception while attempting update the properties. Re-throw as an
+
+      // Caught exception while attempting to update the properties. Re-throw as an
       // Azure storage exception.
       throw new AzureException(e);
     }
   }
 
   @Override
-  public void updateFolderLastModifiedTime(String key) throws AzureException {
+  public void updateFolderLastModifiedTime(String key,
+      SelfRenewingLease folderLease) throws AzureException {
     final Calendar lastModifiedCalendar = Calendar
         .getInstance(Utility.LOCALE_US);
     lastModifiedCalendar.setTimeZone(Utility.UTC_ZONE);
     Date lastModified = lastModifiedCalendar.getTime();
-    updateFolderLastModifiedTime(key, lastModified);
+    updateFolderLastModifiedTime(key, lastModified, folderLease);
   }
 
   @Override
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java
index 577711f..076c48a 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java
@@ -25,14 +25,21 @@
 import java.io.OutputStream;
 import java.net.URI;
 import java.net.URISyntaxException;
+import java.text.SimpleDateFormat;
 import java.util.ArrayList;
-import java.util.Calendar;
 import java.util.Date;
+import java.util.EnumSet;
+import java.util.Iterator;
 import java.util.Set;
+import java.util.TimeZone;
 import java.util.TreeSet;
 import java.util.UUID;
 import java.util.concurrent.atomic.AtomicInteger;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
 
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.lang.exception.ExceptionUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
@@ -40,6 +47,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.BlockLocation;
 import org.apache.hadoop.fs.BufferedFSInputStream;
+import org.apache.hadoop.fs.CreateFlag;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FSInputStream;
@@ -50,12 +58,26 @@
 import org.apache.hadoop.fs.azure.metrics.AzureFileSystemMetricsSystem;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.fs.permission.PermissionStatus;
+import org.apache.hadoop.fs.azure.AzureException;
+import org.apache.hadoop.fs.azure.StorageInterface.CloudBlobWrapper;
+import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.metrics2.lib.DefaultMetricsSystem;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.util.Progressable;
 
+
+import org.codehaus.jackson.JsonNode;
+import org.codehaus.jackson.JsonParseException;
+import org.codehaus.jackson.JsonParser;
+import org.codehaus.jackson.map.JsonMappingException;
+import org.codehaus.jackson.map.ObjectMapper;
+
 import com.google.common.annotations.VisibleForTesting;
-import com.microsoft.windowsazure.storage.core.Utility;
+import com.microsoft.windowsazure.storage.AccessCondition;
+import com.microsoft.windowsazure.storage.OperationContext;
+import com.microsoft.windowsazure.storage.StorageException;
+import com.microsoft.windowsazure.storage.blob.CloudBlob;
+import com.microsoft.windowsazure.storage.core.*;
 
 /**
  * <p>
@@ -68,6 +90,495 @@
 @InterfaceAudience.Public
 @InterfaceStability.Stable
 public class NativeAzureFileSystem extends FileSystem {
+  private static final int USER_WX_PERMISION = 0300;
+
+  /**
+   * A description of a folder rename operation, including the source and
+   * destination keys, and descriptions of the files in the source folder.
+   */
+  public static class FolderRenamePending {
+    private SelfRenewingLease folderLease;
+    private String srcKey;
+    private String dstKey;
+    private FileMetadata[] fileMetadata = null;    // descriptions of source files
+    private ArrayList<String> fileStrings = null;
+    private NativeAzureFileSystem fs;
+    private static final int MAX_RENAME_PENDING_FILE_SIZE = 10000000;
+    private static final int FORMATTING_BUFFER = 10000;
+    private boolean committed;
+    public static final String SUFFIX = "-RenamePending.json";
+
+    // Prepare in-memory information needed to do or redo a folder rename.
+    public FolderRenamePending(String srcKey, String dstKey, SelfRenewingLease lease,
+        NativeAzureFileSystem fs) throws IOException {
+      this.srcKey = srcKey;
+      this.dstKey = dstKey;
+      this.folderLease = lease;
+      this.fs = fs;
+      ArrayList<FileMetadata> fileMetadataList = new ArrayList<FileMetadata>();
+
+      // List all the files in the folder.
+      String priorLastKey = null;
+      do {
+        PartialListing listing = fs.getStoreInterface().listAll(srcKey, AZURE_LIST_ALL,
+          AZURE_UNBOUNDED_DEPTH, priorLastKey);
+        for(FileMetadata file : listing.getFiles()) {
+          fileMetadataList.add(file);
+        }
+        priorLastKey = listing.getPriorLastKey();
+      } while (priorLastKey != null);
+      fileMetadata = fileMetadataList.toArray(new FileMetadata[fileMetadataList.size()]);
+      this.committed = true;
+    }
+
+    // Prepare in-memory information needed to do or redo folder rename from
+    // a -RenamePending.json file read from storage. This constructor is to use during
+    // redo processing.
+    public FolderRenamePending(Path redoFile, NativeAzureFileSystem fs)
+        throws IllegalArgumentException, IOException {
+
+      this.fs = fs;
+
+      // open redo file
+      Path f = redoFile;
+      FSDataInputStream input = fs.open(f);
+      byte[] bytes = new byte[MAX_RENAME_PENDING_FILE_SIZE];
+      int l = input.read(bytes);
+      if (l < 0) {
+        throw new IOException(
+            "Error reading pending rename file contents -- no data available");
+      }
+      if (l == MAX_RENAME_PENDING_FILE_SIZE) {
+        throw new IOException(
+            "Error reading pending rename file contents -- "
+                + "maximum file size exceeded");
+      }
+      String contents = new String(bytes, 0, l);
+
+      // parse the JSON
+      ObjectMapper objMapper = new ObjectMapper();
+      objMapper.configure(JsonParser.Feature.ALLOW_UNQUOTED_FIELD_NAMES, true);
+      JsonNode json = null;
+      try {
+        json = objMapper.readValue(contents, JsonNode.class);
+        this.committed = true;
+      } catch (JsonMappingException e) {
+
+        // The -RedoPending.json file is corrupted, so we assume it was
+        // not completely written
+        // and the redo operation did not commit.
+        this.committed = false;
+      } catch (JsonParseException e) {
+        this.committed = false;
+      } catch (IOException e) {
+        this.committed = false;  
+      }
+      
+      if (!this.committed) {
+        LOG.error("Deleting corruped rename pending file "
+            + redoFile + "\n" + contents);
+
+        // delete the -RenamePending.json file
+        fs.delete(redoFile, false);
+        return;
+      }
+
+      // initialize this object's fields
+      ArrayList<String> fileStrList = new ArrayList<String>();
+      JsonNode oldFolderName = json.get("OldFolderName");
+      JsonNode newFolderName = json.get("NewFolderName");
+      if (oldFolderName == null || newFolderName == null) {
+    	  this.committed = false;
+      } else {
+        this.srcKey = oldFolderName.getTextValue();
+        this.dstKey = newFolderName.getTextValue();
+        if (this.srcKey == null || this.dstKey == null) {
+          this.committed = false;    	  
+        } else {
+          JsonNode fileList = json.get("FileList");
+          if (fileList == null) {
+            this.committed = false;	
+          } else {
+            for (int i = 0; i < fileList.size(); i++) {
+              fileStrList.add(fileList.get(i).getTextValue());
+            }
+          }
+        }
+      }
+      this.fileStrings = fileStrList;
+    }
+
+    public FileMetadata[] getFiles() {
+      return fileMetadata;
+    }
+
+    public SelfRenewingLease getFolderLease() {
+      return folderLease;
+    }
+
+    /**
+     * Write to disk the information needed to redo folder rename, in JSON format.
+     * The file name will be wasb://<sourceFolderPrefix>/folderName-RenamePending.json
+     * The file format will be:
+     * {
+     *   FormatVersion: "1.0",
+     *   OperationTime: "<YYYY-MM-DD HH:MM:SS.MMM>",
+     *   OldFolderName: "<key>",
+     *   NewFolderName: "<key>",
+     *   FileList: [ <string> , <string> , ... ]
+     * }
+     *
+     * Here's a sample:
+     * {
+     *  FormatVersion: "1.0",
+     *  OperationUTCTime: "2014-07-01 23:50:35.572",
+     *  OldFolderName: "user/ehans/folderToRename",
+     *  NewFolderName: "user/ehans/renamedFolder",
+     *  FileList: [
+     *    "innerFile",
+     *    "innerFile2"
+     *  ]
+     * }
+     * @throws IOException
+     */
+    public void writeFile(FileSystem fs) throws IOException {
+      Path path = getRenamePendingFilePath();
+      if (LOG.isDebugEnabled()){
+        LOG.debug("Preparing to write atomic rename state to " + path.toString());
+      }
+      OutputStream output = null;
+
+      String contents = makeRenamePendingFileContents();
+
+      // Write file.
+      try {
+        output = fs.create(path);
+        output.write(contents.getBytes());
+      } catch (IOException e) {
+        throw new IOException("Unable to write RenamePending file for folder rename from "
+            + srcKey + " to " + dstKey, e);
+      } finally {
+        IOUtils.cleanup(LOG, output);
+      }
+    }
+
+    /**
+     * Return the contents of the JSON file to represent the operations
+     * to be performed for a folder rename.
+     */
+    public String makeRenamePendingFileContents() {
+      SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS");
+      sdf.setTimeZone(TimeZone.getTimeZone("UTC"));
+      String time = sdf.format(new Date());
+
+      // Make file list string
+      StringBuilder builder = new StringBuilder();
+      builder.append("[\n");
+      for (int i = 0; i != fileMetadata.length; i++) {
+        if (i > 0) {
+          builder.append(",\n");
+        }
+        builder.append("    ");
+        String noPrefix = StringUtils.removeStart(fileMetadata[i].getKey(), srcKey + "/");
+
+        // Quote string file names, escaping any possible " characters or other
+        // necessary characters in the name.
+        builder.append(quote(noPrefix));
+        if (builder.length() >=
+            MAX_RENAME_PENDING_FILE_SIZE - FORMATTING_BUFFER) {
+
+          // Give up now to avoid using too much memory.
+          LOG.error("Internal error: Exceeded maximum rename pending file size of "
+              + MAX_RENAME_PENDING_FILE_SIZE + " bytes.");
+
+          // return some bad JSON with an error message to make it human readable
+          return "exceeded maximum rename pending file size";
+        }
+      }
+      builder.append("\n  ]");
+      String fileList = builder.toString();
+
+      // Make file contents as a string. Again, quote file names, escaping
+      // characters as appropriate.
+      String contents = "{\n"
+          + "  FormatVersion: \"1.0\",\n"
+          + "  OperationUTCTime: \"" + time + "\",\n"
+          + "  OldFolderName: " + quote(srcKey) + ",\n"
+          + "  NewFolderName: " + quote(dstKey) + ",\n"
+          + "  FileList: " + fileList + "\n"
+          + "}\n";
+
+      return contents;
+    }
+    
+    /**
+     * This is an exact copy of org.codehaus.jettison.json.JSONObject.quote 
+     * method.
+     * 
+     * Produce a string in double quotes with backslash sequences in all the
+     * right places. A backslash will be inserted within </, allowing JSON
+     * text to be delivered in HTML. In JSON text, a string cannot contain a
+     * control character or an unescaped quote or backslash.
+     * @param string A String
+     * @return  A String correctly formatted for insertion in a JSON text.
+     */
+    private String quote(String string) {
+        if (string == null || string.length() == 0) {
+            return "\"\"";
+        }
+
+        char c = 0;
+        int  i;
+        int  len = string.length();
+        StringBuilder sb = new StringBuilder(len + 4);
+        String t;
+
+        sb.append('"');
+        for (i = 0; i < len; i += 1) {
+            c = string.charAt(i);
+            switch (c) {
+            case '\\':
+            case '"':
+                sb.append('\\');
+                sb.append(c);
+                break;
+            case '/':
+                sb.append('\\');
+                sb.append(c);
+                break;
+            case '\b':
+                sb.append("\\b");
+                break;
+            case '\t':
+                sb.append("\\t");
+                break;
+            case '\n':
+                sb.append("\\n");
+                break;
+            case '\f':
+                sb.append("\\f");
+                break;
+            case '\r':
+                sb.append("\\r");
+                break;
+            default:
+                if (c < ' ') {
+                    t = "000" + Integer.toHexString(c);
+                    sb.append("\\u" + t.substring(t.length() - 4));
+                } else {
+                    sb.append(c);
+                }
+            }
+        }
+        sb.append('"');
+        return sb.toString();
+    }
+
+    public String getSrcKey() {
+      return srcKey;
+    }
+
+    public String getDstKey() {
+      return dstKey;
+    }
+
+    public FileMetadata getSourceMetadata() throws IOException {
+      return fs.getStoreInterface().retrieveMetadata(srcKey);
+    }
+
+    /**
+     * Execute a folder rename. This is the execution path followed
+     * when everything is working normally. See redo() for the alternate
+     * execution path for the case where we're recovering from a folder rename
+     * failure.
+     * @throws IOException
+     */
+    public void execute() throws IOException {
+
+      for (FileMetadata file : this.getFiles()) {
+
+        // Rename all materialized entries under the folder to point to the
+        // final destination.
+        if (file.getBlobMaterialization() == BlobMaterialization.Explicit) {
+          String srcName = file.getKey();
+          String suffix  = srcName.substring((this.getSrcKey()).length());
+          String dstName = this.getDstKey() + suffix;
+
+          // Rename gets exclusive access (via a lease) for files
+          // designated for atomic rename.
+          // The main use case is for HBase write-ahead log (WAL) and data
+          // folder processing correctness.  See the rename code for details.
+          boolean acquireLease = fs.getStoreInterface().isAtomicRenameKey(srcName);
+          fs.getStoreInterface().rename(srcName, dstName, acquireLease, null);
+        }
+      }
+
+      // Rename the source folder 0-byte root file itself.
+      FileMetadata srcMetadata2 = this.getSourceMetadata();
+      if (srcMetadata2.getBlobMaterialization() ==
+          BlobMaterialization.Explicit) {
+
+        // It already has a lease on it from the "prepare" phase so there's no
+        // need to get one now. Pass in existing lease to allow file delete.
+        fs.getStoreInterface().rename(this.getSrcKey(), this.getDstKey(),
+            false, folderLease);
+      }
+
+      // Update the last-modified time of the parent folders of both source and
+      // destination.
+      fs.updateParentFolderLastModifiedTime(srcKey);
+      fs.updateParentFolderLastModifiedTime(dstKey);
+    }
+
+    /** Clean up after execution of rename.
+     * @throws IOException */
+    public void cleanup() throws IOException {
+
+      if (fs.getStoreInterface().isAtomicRenameKey(srcKey)) {
+
+        // Remove RenamePending file
+        fs.delete(getRenamePendingFilePath(), false);
+
+        // Freeing source folder lease is not necessary since the source
+        // folder file was deleted.
+      }
+    }
+
+    private Path getRenamePendingFilePath() {
+      String fileName = srcKey + SUFFIX;
+      Path fileNamePath = keyToPath(fileName);
+      Path path = fs.makeAbsolute(fileNamePath);
+      return path;
+    }
+
+    /**
+     * Recover from a folder rename failure by redoing the intended work,
+     * as recorded in the -RenamePending.json file.
+     * 
+     * @throws IOException
+     */
+    public void redo() throws IOException {
+
+      if (!committed) {
+
+        // Nothing to do. The -RedoPending.json file should have already been
+        // deleted.
+        return;
+      }
+
+      // Try to get a lease on source folder to block concurrent access to it.
+      // It may fail if the folder is already gone. We don't check if the
+      // source exists explicitly because that could recursively trigger redo
+      // and give an infinite recursion.
+      SelfRenewingLease lease = null;
+      boolean sourceFolderGone = false;
+      try {
+        lease = fs.leaseSourceFolder(srcKey);
+      } catch (AzureException e) {
+
+        // If the source folder was not found then somebody probably
+        // raced with us and finished the rename first, or the
+        // first rename failed right before deleting the rename pending
+        // file.
+        String errorCode = "";
+        try {
+          StorageException se = (StorageException) e.getCause();
+          errorCode = se.getErrorCode();
+        } catch (Exception e2) {
+          ; // do nothing -- could not get errorCode
+        }
+        if (errorCode.equals("BlobNotFound")) {
+          sourceFolderGone = true;
+        } else {
+          throw new IOException(
+              "Unexpected error when trying to lease source folder name during "
+              + "folder rename redo",
+              e);
+        }
+      }
+
+      if (!sourceFolderGone) {
+        // Make sure the target folder exists.
+        Path dst = fullPath(dstKey);
+        if (!fs.exists(dst)) {
+          fs.mkdirs(dst);
+        }
+
+        // For each file inside the folder to be renamed,
+        // make sure it has been renamed.
+        for(String fileName : fileStrings) {
+          finishSingleFileRename(fileName);
+        }
+
+        // Remove the source folder. Don't check explicitly if it exists,
+        // to avoid triggering redo recursively.
+        try {
+          fs.getStoreInterface().delete(srcKey, lease);
+        } catch (Exception e) {
+          LOG.info("Unable to delete source folder during folder rename redo. "
+              + "If the source folder is already gone, this is not an error "
+              + "condition. Continuing with redo.", e);
+        }
+
+        // Update the last-modified time of the parent folders of both source
+        // and destination.
+        fs.updateParentFolderLastModifiedTime(srcKey);
+        fs.updateParentFolderLastModifiedTime(dstKey);
+      }
+
+      // Remove the -RenamePending.json file.
+      fs.delete(getRenamePendingFilePath(), false);
+    }
+
+    // See if the source file is still there, and if it is, rename it.
+    private void finishSingleFileRename(String fileName)
+        throws IOException {
+      Path srcFile = fullPath(srcKey, fileName);
+      Path dstFile = fullPath(dstKey, fileName);
+      boolean srcExists = fs.exists(srcFile);
+      boolean dstExists = fs.exists(dstFile);
+      if (srcExists && !dstExists) {
+
+        // Rename gets exclusive access (via a lease) for HBase write-ahead log
+        // (WAL) file processing correctness.  See the rename code for details.
+        String srcName = fs.pathToKey(srcFile);
+        String dstName = fs.pathToKey(dstFile);
+        fs.getStoreInterface().rename(srcName, dstName, true, null);
+      } else if (srcExists && dstExists) {
+
+        // Get a lease on source to block write access.
+        String srcName = fs.pathToKey(srcFile);
+        SelfRenewingLease lease = fs.acquireLease(srcFile);
+
+        // Delete the file. This will free the lease too.
+        fs.getStoreInterface().delete(srcName, lease);
+      } else if (!srcExists && dstExists) {
+
+        // The rename already finished, so do nothing.
+        ;
+      } else {
+        throw new IOException(
+            "Attempting to complete rename of file " + srcKey + "/" + fileName
+            + " during folder rename redo, and file was not found in source "
+            + "or destination.");
+      }
+    }
+
+    // Return an absolute path for the specific fileName within the folder
+    // specified by folderKey.
+    private Path fullPath(String folderKey, String fileName) {
+      return new Path(new Path(fs.getUri()), "/" + folderKey + "/" + fileName);
+    }
+
+    private Path fullPath(String fileKey) {
+      return new Path(new Path(fs.getUri()), "/" + fileKey);
+    }
+  }
+
+  private static final String TRAILING_PERIOD_PLACEHOLDER = "[[.]]";
+  private static final Pattern TRAILING_PERIOD_PLACEHOLDER_PATTERN =
+      Pattern.compile("\\[\\[\\.\\]\\](?=$|/)");
+  private static final Pattern TRAILING_PERIOD_PATTERN = Pattern.compile("\\.(?=$|/)");
 
   @Override
   public String getScheme() {
@@ -121,17 +632,53 @@ public String getScheme() {
    */
   static final String AZURE_DEFAULT_GROUP_DEFAULT = "supergroup";
 
-  static final String AZURE_BLOCK_LOCATION_HOST_PROPERTY_NAME = "fs.azure.block.location.impersonatedhost";
-  private static final String AZURE_BLOCK_LOCATION_HOST_DEFAULT = "localhost";
+  static final String AZURE_BLOCK_LOCATION_HOST_PROPERTY_NAME =
+      "fs.azure.block.location.impersonatedhost";
+  private static final String AZURE_BLOCK_LOCATION_HOST_DEFAULT =
+      "localhost";
+  static final String AZURE_RINGBUFFER_CAPACITY_PROPERTY_NAME =
+      "fs.azure.ring.buffer.capacity";
+  static final String AZURE_OUTPUT_STREAM_BUFFER_SIZE_PROPERTY_NAME =
+      "fs.azure.output.stream.buffer.size";
 
   private class NativeAzureFsInputStream extends FSInputStream {
     private InputStream in;
     private final String key;
     private long pos = 0;
+    private boolean closed = false;
+    private boolean isPageBlob;
 
-    public NativeAzureFsInputStream(DataInputStream in, String key) {
+    // File length, valid only for streams over block blobs.
+    private long fileLength;
+
+    public NativeAzureFsInputStream(DataInputStream in, String key, long fileLength) {
       this.in = in;
       this.key = key;
+      this.isPageBlob = store.isPageBlobKey(key);
+      this.fileLength = fileLength;
+    }
+
+    /**
+     * Return the size of the remaining available bytes
+     * if the size is less than or equal to {@link Integer#MAX_VALUE},
+     * otherwise, return {@link Integer#MAX_VALUE}.
+     *
+     * This is to match the behavior of DFSInputStream.available(),
+     * which some clients may rely on (HBase write-ahead log reading in
+     * particular).
+     */
+    @Override
+    public synchronized int available() throws IOException {
+      if (isPageBlob) {
+        return in.available();
+      } else {
+        if (closed) {
+          throw new IOException("Stream closed");
+        }
+        final long remaining = this.fileLength - pos;
+        return remaining <= Integer.MAX_VALUE ?
+            (int) remaining : Integer.MAX_VALUE;
+      }
     }
 
     /*
@@ -140,7 +687,7 @@ public NativeAzureFsInputStream(DataInputStream in, String key) {
      * because the end of the stream has been reached, the value -1 is returned.
      * This method blocks until input data is available, the end of the stream
      * is detected, or an exception is thrown.
-     * 
+     *
      * @returns int An integer corresponding to the byte read.
      */
     @Override
@@ -169,13 +716,13 @@ public synchronized int read() throws IOException {
      * one byte. If no byte is available because the stream is at end of file,
      * the value -1 is returned; otherwise, at least one byte is read and stored
      * into b.
-     * 
+     *
      * @param b -- the buffer into which data is read
-     * 
+     *
      * @param off -- the start offset in the array b at which data is written
-     * 
+     *
      * @param len -- the maximum number of bytes read
-     * 
+     *
      * @ returns int The total number of byes read into the buffer, or -1 if
      * there is no more data because the end of stream is reached.
      */
@@ -196,15 +743,20 @@ public synchronized int read(byte[] b, int off, int len) throws IOException {
     }
 
     @Override
-    public synchronized void close() throws IOException {
+    public void close() throws IOException {
       in.close();
+      closed = true;
     }
 
     @Override
     public synchronized void seek(long pos) throws IOException {
-      in.close();
-      in = store.retrieve(key, pos);
-      this.pos = pos;
+     in.close();
+     in = store.retrieve(key);
+     this.pos = in.skip(pos);
+     if (LOG.isDebugEnabled()) {
+       LOG.debug(String.format("Seek to position %d. Bytes skipped %d", pos,
+         this.pos));
+     }
     }
 
     @Override
@@ -468,7 +1020,8 @@ protected void checkPath(Path path) {
   }
 
   @Override
-  public void initialize(URI uri, Configuration conf) throws IOException {
+  public void initialize(URI uri, Configuration conf)
+      throws IOException, IllegalArgumentException {
     // Check authority for the URI to guarantee that it is non-null.
     uri = reconstructAuthorityIfNeeded(uri, conf);
     if (null == uri.getAuthority()) {
@@ -514,10 +1067,29 @@ private NativeFileSystemStore createDefaultStore(Configuration conf) {
     return actualStore;
   }
 
-  // Note: The logic for this method is confusing as to whether it strips the
-  // last slash or not (it adds it in the beginning, then strips it at the end).
-  // We should revisit that.
-  private String pathToKey(Path path) {
+  /**
+   * Azure Storage doesn't allow the blob names to end in a period,
+   * so encode this here to work around that limitation.
+   */
+  private static String encodeTrailingPeriod(String toEncode) {
+    Matcher matcher = TRAILING_PERIOD_PATTERN.matcher(toEncode);
+    return matcher.replaceAll(TRAILING_PERIOD_PLACEHOLDER);
+  }
+
+  /**
+   * Reverse the encoding done by encodeTrailingPeriod().
+   */
+  private static String decodeTrailingPeriod(String toDecode) {
+    Matcher matcher = TRAILING_PERIOD_PLACEHOLDER_PATTERN.matcher(toDecode);
+    return matcher.replaceAll(".");
+  }
+
+  /**
+   * Convert the path to a key. By convention, any leading or trailing slash is
+   * removed, except for the special case of a single slash.
+   */
+  @VisibleForTesting
+  public String pathToKey(Path path) {
     // Convert the path to a URI to parse the scheme, the authority, and the
     // path from the path object.
     URI tmpUri = path.toUri();
@@ -537,6 +1109,8 @@ private String pathToKey(Path path) {
 
     String key = null;
     key = newPath.toUri().getPath();
+    key = removeTrailingSlash(key);
+    key = encodeTrailingPeriod(key);
     if (key.length() == 1) {
       return key;
     } else {
@@ -544,14 +1118,34 @@ private String pathToKey(Path path) {
     }
   }
 
+  // Remove any trailing slash except for the case of a single slash.
+  private static String removeTrailingSlash(String key) {
+    if (key.length() == 0 || key.length() == 1) {
+      return key;
+    }
+    if (key.charAt(key.length() - 1) == '/') {
+      return key.substring(0, key.length() - 1);
+    } else {
+      return key;
+    }
+  }
+
   private static Path keyToPath(String key) {
     if (key.equals("/")) {
       return new Path("/"); // container
     }
-    return new Path("/" + key);
+    return new Path("/" + decodeTrailingPeriod(key));
   }
 
-  private Path makeAbsolute(Path path) {
+  /**
+   * Get the absolute version of the path (fully qualified).
+   * This is public for testing purposes.
+   *
+   * @param path
+   * @return fully qualified path
+   */
+  @VisibleForTesting
+  public Path makeAbsolute(Path path) {
     if (path.isAbsolute()) {
       return path;
     }
@@ -569,6 +1163,10 @@ public AzureNativeFileSystemStore getStore() {
     return actualStore;
   }
   
+  NativeFileSystemStore getStoreInterface() {
+    return store;
+  }
+
   /**
    * Gets the metrics source for this file system.
    * This is mainly here for unit testing purposes.
@@ -590,6 +1188,145 @@ public FSDataOutputStream append(Path f, int bufferSize, Progressable progress)
   public FSDataOutputStream create(Path f, FsPermission permission,
       boolean overwrite, int bufferSize, short replication, long blockSize,
       Progressable progress) throws IOException {
+    return create(f, permission, overwrite, true,
+        bufferSize, replication, blockSize, progress,
+        (SelfRenewingLease) null);
+  }
+
+  /**
+   * Get a self-renewing lease on the specified file.
+   */
+  public SelfRenewingLease acquireLease(Path path) throws AzureException {
+    String fullKey = pathToKey(makeAbsolute(path));
+    return getStore().acquireLease(fullKey);
+  }
+
+  @Override
+  @SuppressWarnings("deprecation")
+  public FSDataOutputStream createNonRecursive(Path f, FsPermission permission,
+      boolean overwrite, int bufferSize, short replication, long blockSize,
+      Progressable progress) throws IOException {
+
+    Path parent = f.getParent();
+
+    // Get exclusive access to folder if this is a directory designated
+    // for atomic rename. The primary use case of for HBase write-ahead
+    // log file management.
+    SelfRenewingLease lease = null;
+    if (store.isAtomicRenameKey(pathToKey(f))) {
+      try {
+        lease = acquireLease(parent);
+      } catch (AzureException e) {
+
+        String errorCode = "";
+        try {
+          StorageException e2 = (StorageException) e.getCause();
+          errorCode = e2.getErrorCode();
+        } catch (Exception e3) {
+          // do nothing if cast fails
+        }
+        if (errorCode.equals("BlobNotFound")) {
+          throw new FileNotFoundException("Cannot create file " +
+              f.getName() + " because parent folder does not exist.");
+        }
+
+        LOG.warn("Got unexpected exception trying to get lease on "
+          + pathToKey(parent) + ". " + e.getMessage());
+        throw e;
+      }
+    }
+
+    // See if the parent folder exists. If not, throw error.
+    // The exists() check will push any pending rename operation forward,
+    // if there is one, and return false.
+    //
+    // At this point, we have exclusive access to the source folder
+    // via the lease, so we will not conflict with an active folder
+    // rename operation.
+    if (!exists(parent)) {
+      try {
+
+        // This'll let the keep-alive thread exit as soon as it wakes up.
+        lease.free();
+      } catch (Exception e) {
+        LOG.warn("Unable to free lease because: " + e.getMessage());
+      }
+      throw new FileNotFoundException("Cannot create file " +
+          f.getName() + " because parent folder does not exist.");
+    }
+
+    // Create file inside folder.
+    FSDataOutputStream out = null;
+    try {
+      out = create(f, permission, overwrite, false,
+          bufferSize, replication, blockSize, progress, lease);
+    } finally {
+      // Release exclusive access to folder.
+      try {
+        if (lease != null) {
+          lease.free();
+        }
+      } catch (Exception e) {
+        IOUtils.cleanup(LOG, out);
+        String msg = "Unable to free lease on " + parent.toUri();
+        LOG.error(msg);
+        throw new IOException(msg, e);
+      }
+    }
+    return out;
+  }
+
+  @Override
+  @SuppressWarnings("deprecation")
+  public FSDataOutputStream createNonRecursive(Path f, FsPermission permission,
+      EnumSet<CreateFlag> flags, int bufferSize, short replication, long blockSize,
+      Progressable progress) throws IOException {
+
+    // Check if file should be appended or overwritten. Assume that the file
+    // is overwritten on if the CREATE and OVERWRITE create flags are set. Note
+    // that any other combinations of create flags will result in an open new or
+    // open with append.
+    final EnumSet<CreateFlag> createflags =
+        EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE);
+    boolean overwrite = flags.containsAll(createflags);
+
+    // Delegate the create non-recursive call.
+    return this.createNonRecursive(f, permission, overwrite,
+        bufferSize, replication, blockSize, progress);
+  }
+
+  @Override
+  @SuppressWarnings("deprecation")
+  public FSDataOutputStream createNonRecursive(Path f,
+      boolean overwrite, int bufferSize, short replication, long blockSize,
+      Progressable progress) throws IOException {
+    return this.createNonRecursive(f, FsPermission.getFileDefault(),
+        overwrite, bufferSize, replication, blockSize, progress);
+  }
+
+
+  /**
+   * Create an Azure blob and return an output stream to use
+   * to write data to it.
+   *
+   * @param f
+   * @param permission
+   * @param overwrite
+   * @param createParent
+   * @param bufferSize
+   * @param replication
+   * @param blockSize
+   * @param progress
+   * @param parentFolderLease Lease on parent folder (or null if
+   * no lease).
+   * @return
+   * @throws IOException
+   */
+  private FSDataOutputStream create(Path f, FsPermission permission,
+      boolean overwrite, boolean createParent, int bufferSize,
+      short replication, long blockSize, Progressable progress,
+      SelfRenewingLease parentFolderLease)
+          throws IOException {
 
     if (LOG.isDebugEnabled()) {
       LOG.debug("Creating file: " + f.toString());
@@ -620,45 +1357,60 @@ public FSDataOutputStream create(Path f, FsPermission permission,
       // already exists.
       String parentKey = pathToKey(parentFolder);
       FileMetadata parentMetadata = store.retrieveMetadata(parentKey);
-      if (parentMetadata != null
-          && parentMetadata.isDir()
-          && parentMetadata.getBlobMaterialization() == BlobMaterialization.Explicit) {
-        store.updateFolderLastModifiedTime(parentKey);
+      if (parentMetadata != null && parentMetadata.isDir() &&
+          parentMetadata.getBlobMaterialization() == BlobMaterialization.Explicit) {
+        store.updateFolderLastModifiedTime(parentKey, parentFolderLease);
       } else {
         // Make sure that the parent folder exists.
-        mkdirs(parentFolder, permission);
+        // Create it using inherited permissions from the first existing directory going up the path
+        Path firstExisting = parentFolder.getParent();
+        FileMetadata metadata = store.retrieveMetadata(pathToKey(firstExisting));
+        while(metadata == null) {
+          // Guaranteed to terminate properly because we will eventually hit root, which will return non-null metadata
+          firstExisting = firstExisting.getParent();
+          metadata = store.retrieveMetadata(pathToKey(firstExisting));
+        }
+        mkdirs(parentFolder, metadata.getPermissionStatus().getPermission(), true);
       }
     }
 
-    // Open the output blob stream based on the encoded key.
-    String keyEncoded = encodeKey(key);
-
     // Mask the permission first (with the default permission mask as well).
     FsPermission masked = applyUMask(permission, UMaskApplyMode.NewFile);
     PermissionStatus permissionStatus = createPermissionStatus(masked);
 
-    // First create a blob at the real key, pointing back to the temporary file
-    // This accomplishes a few things:
-    // 1. Makes sure we can create a file there.
-    // 2. Makes it visible to other concurrent threads/processes/nodes what
-    // we're
-    // doing.
-    // 3. Makes it easier to restore/cleanup data in the event of us crashing.
-    store.storeEmptyLinkFile(key, keyEncoded, permissionStatus);
-
-    // The key is encoded to point to a common container at the storage server.
-    // This reduces the number of splits on the server side when load balancing.
-    // Ingress to Azure storage can take advantage of earlier splits. We remove
-    // the root path to the key and prefix a random GUID to the tail (or leaf
-    // filename) of the key. Keys are thus broadly and randomly distributed over
-    // a single container to ease load balancing on the storage server. When the
-    // blob is committed it is renamed to its earlier key. Uncommitted blocks
-    // are not cleaned up and we leave it to Azure storage to garbage collect
-    // these
-    // blocks.
-    OutputStream bufOutStream = new NativeAzureFsOutputStream(store.storefile(
-        keyEncoded, permissionStatus), key, keyEncoded);
-
+    OutputStream bufOutStream;
+    if (store.isPageBlobKey(key)) {
+      // Store page blobs directly in-place without renames.
+      bufOutStream = store.storefile(key, permissionStatus);
+    } else {
+      // This is a block blob, so open the output blob stream based on the
+      // encoded key.
+      //
+      String keyEncoded = encodeKey(key);
+
+
+      // First create a blob at the real key, pointing back to the temporary file
+      // This accomplishes a few things:
+      // 1. Makes sure we can create a file there.
+      // 2. Makes it visible to other concurrent threads/processes/nodes what
+      // we're
+      // doing.
+      // 3. Makes it easier to restore/cleanup data in the event of us crashing.
+      store.storeEmptyLinkFile(key, keyEncoded, permissionStatus);
+
+      // The key is encoded to point to a common container at the storage server.
+      // This reduces the number of splits on the server side when load balancing.
+      // Ingress to Azure storage can take advantage of earlier splits. We remove
+      // the root path to the key and prefix a random GUID to the tail (or leaf
+      // filename) of the key. Keys are thus broadly and randomly distributed over
+      // a single container to ease load balancing on the storage server. When the
+      // blob is committed it is renamed to its earlier key. Uncommitted blocks
+      // are not cleaned up and we leave it to Azure storage to garbage collect
+      // these
+      // blocks.
+      bufOutStream = new NativeAzureFsOutputStream(store.storefile(
+          keyEncoded, permissionStatus), key, keyEncoded);
+    }
     // Construct the data output stream from the buffered output stream.
     FSDataOutputStream fsOut = new FSDataOutputStream(bufOutStream, statistics);
 
@@ -678,6 +1430,28 @@ public boolean delete(Path path) throws IOException {
 
   @Override
   public boolean delete(Path f, boolean recursive) throws IOException {
+    return delete(f, recursive, false);
+  }
+
+  /**
+   * Delete the specified file or folder. The parameter
+   * skipParentFolderLastModifidedTimeUpdate
+   * is used in the case of atomic folder rename redo. In that case, there is
+   * a lease on the parent folder, so (without reworking the code) modifying
+   * the parent folder update time will fail because of a conflict with the
+   * lease. Since we are going to delete the folder soon anyway so accurate
+   * modified time is not necessary, it's easier to just skip
+   * the modified time update.
+   *
+   * @param f
+   * @param recursive
+   * @param skipParentFolderLastModifidedTimeUpdate If true, don't update the folder last
+   * modified time.
+   * @return true if and only if the file is deleted
+   * @throws IOException
+   */
+  public boolean delete(Path f, boolean recursive,
+      boolean skipParentFolderLastModifidedTimeUpdate) throws IOException {
 
     if (LOG.isDebugEnabled()) {
       LOG.debug("Deleting file: " + f.toString());
@@ -723,11 +1497,13 @@ public boolean delete(Path f, boolean recursive) throws IOException {
           store.storeEmptyFolder(parentKey,
               createPermissionStatus(FsPermission.getDefault()));
         } else {
-          store.updateFolderLastModifiedTime(parentKey);
+          if (!skipParentFolderLastModifidedTimeUpdate) {
+            store.updateFolderLastModifiedTime(parentKey, null);
+          }
         }
       }
-      instrumentation.fileDeleted();
       store.delete(key);
+      instrumentation.fileDeleted();
     } else {
       // The path specifies a folder. Recursively delete all entries under the
       // folder.
@@ -784,7 +1560,9 @@ public boolean delete(Path f, boolean recursive) throws IOException {
       Path parent = absolutePath.getParent();
       if (parent != null && parent.getParent() != null) { // not root
         String parentKey = pathToKey(parent);
-        store.updateFolderLastModifiedTime(parentKey);
+        if (!skipParentFolderLastModifidedTimeUpdate) {
+          store.updateFolderLastModifiedTime(parentKey, null);
+        }
       }
       instrumentation.directoryDeleted();
     }
@@ -818,6 +1596,13 @@ public FileStatus getFileStatus(Path f) throws IOException {
           LOG.debug("Path " + f.toString() + "is a folder.");
         }
 
+        // If a rename operation for the folder was pending, redo it.
+        // Then the file does not exist, so signal that.
+        if (conditionalRedoFolderRename(f)) {
+          throw new FileNotFoundException(
+              absolutePath + ": No such file or directory.");
+        }
+
         // Return reference to the directory object.
         return newDirectory(meta, absolutePath);
       }
@@ -832,9 +1617,38 @@ public FileStatus getFileStatus(Path f) throws IOException {
     }
 
     // File not found. Throw exception no such file or directory.
-    // Note: Should never get to this point since the root always exists.
-    throw new FileNotFoundException(absolutePath
-        + ": No such file or directory.");
+    //
+    throw new FileNotFoundException(
+        absolutePath + ": No such file or directory.");
+  }
+
+  // Return true if there is a rename pending and we redo it, otherwise false.
+  private boolean conditionalRedoFolderRename(Path f) throws IOException {
+
+    // Can't rename /, so return immediately in that case.
+    if (f.getName().equals("")) {
+      return false;
+    }
+
+    // Check if there is a -RenamePending.json file for this folder, and if so,
+    // redo the rename.
+    Path absoluteRenamePendingFile = renamePendingFilePath(f);
+    if (exists(absoluteRenamePendingFile)) {
+      FolderRenamePending pending =
+          new FolderRenamePending(absoluteRenamePendingFile, this);
+      pending.redo();
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  // Return the path name that would be used for rename of folder with path f.
+  private Path renamePendingFilePath(Path f) {
+    Path absPath = makeAbsolute(f);
+    String key = pathToKey(absPath);
+    key += "-RenamePending.json";
+    return keyToPath(key);
   }
 
   @Override
@@ -867,6 +1681,17 @@ public URI getUri() {
       }
       String partialKey = null;
       PartialListing listing = store.list(key, AZURE_LIST_ALL, 1, partialKey);
+
+      // For any -RenamePending.json files in the listing,
+      // push the rename forward.
+      boolean renamed = conditionalRedoFolderRenames(listing);
+
+      // If any renames were redone, get another listing,
+      // since the current one may have changed due to the redo.
+      if (renamed) {
+        listing = store.list(key, AZURE_LIST_ALL, 1, partialKey);
+      }
+
       for (FileMetadata fileMetadata : listing.getFiles()) {
         Path subpath = keyToPath(fileMetadata.getKey());
 
@@ -903,25 +1728,62 @@ public URI getUri() {
     return status.toArray(new FileStatus[0]);
   }
 
+  // Redo any folder renames needed if there are rename pending files in the
+  // directory listing. Return true if one or more redo operations were done.
+  private boolean conditionalRedoFolderRenames(PartialListing listing)
+      throws IllegalArgumentException, IOException {
+    boolean renamed = false;
+    for (FileMetadata fileMetadata : listing.getFiles()) {
+      Path subpath = keyToPath(fileMetadata.getKey());
+      if (isRenamePendingFile(subpath)) {
+        FolderRenamePending pending =
+            new FolderRenamePending(subpath, this);
+        pending.redo();
+        renamed = true;
+      }
+    }
+    return renamed;
+  }
+
+  // True if this is a folder rename pending file, else false.
+  private boolean isRenamePendingFile(Path path) {
+    return path.toString().endsWith(FolderRenamePending.SUFFIX);
+  }
+
   private FileStatus newFile(FileMetadata meta, Path path) {
-    return new FileStatus(meta.getLength(), false, 1, blockSize,
-        meta.getLastModified(), 0, meta.getPermissionStatus().getPermission(),
-        meta.getPermissionStatus().getUserName(), meta.getPermissionStatus()
-            .getGroupName(),
+    return new FileStatus (
+        meta.getLength(),
+        false,
+        1,
+        blockSize,
+        meta.getLastModified(),
+        0,
+        meta.getPermissionStatus().getPermission(),
+        meta.getPermissionStatus().getUserName(),
+        meta.getPermissionStatus().getGroupName(),
         path.makeQualified(getUri(), getWorkingDirectory()));
   }
 
   private FileStatus newDirectory(FileMetadata meta, Path path) {
-    return new FileStatus(0, true, 1, blockSize, meta == null ? 0
-        : meta.getLastModified(), 0, meta == null ? FsPermission.getDefault()
-        : meta.getPermissionStatus().getPermission(), meta == null ? "" : meta
-        .getPermissionStatus().getUserName(), meta == null ? "" : meta
-        .getPermissionStatus().getGroupName(), path.makeQualified(getUri(),
-        getWorkingDirectory()));
+    return new FileStatus (
+        0,
+        true,
+        1,
+        blockSize,
+        meta == null ? 0 : meta.getLastModified(),
+        0,
+        meta == null ? FsPermission.getDefault() : meta.getPermissionStatus().getPermission(),
+        meta == null ? "" : meta.getPermissionStatus().getUserName(),
+        meta == null ? "" : meta.getPermissionStatus().getGroupName(),
+        path.makeQualified(getUri(), getWorkingDirectory()));
   }
 
   private static enum UMaskApplyMode {
-    NewFile, NewDirectory, ChangeExistingFile, ChangeExistingDirectory,
+    NewFile,
+    NewDirectory,
+    NewDirectoryNoUmask,
+    ChangeExistingFile,
+    ChangeExistingDirectory,
   }
 
   /**
@@ -958,13 +1820,19 @@ private FsPermission applyUMask(final FsPermission permission,
   private PermissionStatus createPermissionStatus(FsPermission permission)
       throws IOException {
     // Create the permission status for this file based on current user
-    return new PermissionStatus(UserGroupInformation.getCurrentUser()
-        .getShortUserName(), getConf().get(AZURE_DEFAULT_GROUP_PROPERTY_NAME,
-        AZURE_DEFAULT_GROUP_DEFAULT), permission);
+    return new PermissionStatus(
+        UserGroupInformation.getCurrentUser().getShortUserName(),
+        getConf().get(AZURE_DEFAULT_GROUP_PROPERTY_NAME,
+            AZURE_DEFAULT_GROUP_DEFAULT),
+        permission);
   }
 
   @Override
   public boolean mkdirs(Path f, FsPermission permission) throws IOException {
+      return mkdirs(f, permission, false);
+  }
+
+  public boolean mkdirs(Path f, FsPermission permission, boolean noUmask) throws IOException {
     if (LOG.isDebugEnabled()) {
       LOG.debug("Creating directory: " + f.toString());
     }
@@ -975,24 +1843,31 @@ public boolean mkdirs(Path f, FsPermission permission) throws IOException {
     }
 
     Path absolutePath = makeAbsolute(f);
-    PermissionStatus permissionStatus = createPermissionStatus(applyUMask(
-        permission, UMaskApplyMode.NewDirectory));
+    PermissionStatus permissionStatus = null;
+    if(noUmask) {
+      // ensure owner still has wx permissions at the minimum
+      permissionStatus = createPermissionStatus(
+          applyUMask(FsPermission.createImmutable((short) (permission.toShort() | USER_WX_PERMISION)),
+              UMaskApplyMode.NewDirectoryNoUmask));
+    } else {
+      permissionStatus = createPermissionStatus(
+          applyUMask(permission, UMaskApplyMode.NewDirectory));
+    }
+
 
     ArrayList<String> keysToCreateAsFolder = new ArrayList<String>();
     ArrayList<String> keysToUpdateAsFolder = new ArrayList<String>();
     boolean childCreated = false;
     // Check that there is no file in the parent chain of the given path.
-    // Stop when you get to the root
-    for (Path current = absolutePath, parent = current.getParent(); parent != null; current = parent, parent = current
-        .getParent()) {
+    for (Path current = absolutePath, parent = current.getParent();
+        parent != null; // Stop when you get to the root
+        current = parent, parent = current.getParent()) {
       String currentKey = pathToKey(current);
       FileMetadata currentMetadata = store.retrieveMetadata(currentKey);
       if (currentMetadata != null && !currentMetadata.isDir()) {
-        throw new IOException("Cannot create directory " + f + " because "
-            + current + " is an existing file.");
-      } else if (currentMetadata == null
-          || (currentMetadata.isDir() && currentMetadata
-              .getBlobMaterialization() == BlobMaterialization.Implicit)) {
+        throw new IOException("Cannot create directory " + f + " because " +
+            current + " is an existing file.");
+      } else if (currentMetadata == null) {
         keysToCreateAsFolder.add(currentKey);
         childCreated = true;
       } else {
@@ -1009,18 +1884,8 @@ public boolean mkdirs(Path f, FsPermission permission) throws IOException {
       store.storeEmptyFolder(currentKey, permissionStatus);
     }
 
-    // Take the time after finishing mkdirs as the modified time, and update all
-    // the existing directories' modified time to it uniformly.
-    final Calendar lastModifiedCalendar = Calendar
-        .getInstance(Utility.LOCALE_US);
-    lastModifiedCalendar.setTimeZone(Utility.UTC_ZONE);
-    Date lastModified = lastModifiedCalendar.getTime();
-    for (String key : keysToUpdateAsFolder) {
-      store.updateFolderLastModifiedTime(key, lastModified);
-    }
-
     instrumentation.directoryCreated();
-    
+
     // otherwise throws exception
     return true;
   }
@@ -1043,12 +1908,14 @@ public FSDataInputStream open(Path f, int bufferSize) throws IOException {
     }
 
     return new FSDataInputStream(new BufferedFSInputStream(
-        new NativeAzureFsInputStream(store.retrieve(key), key), bufferSize));
+        new NativeAzureFsInputStream(store.retrieve(key), key, meta.getLength()), bufferSize));
   }
 
   @Override
   public boolean rename(Path src, Path dst) throws IOException {
 
+    FolderRenamePending renamePending = null;
+
     if (LOG.isDebugEnabled()) {
       LOG.debug("Moving " + src + " to " + dst);
     }
@@ -1065,91 +1932,28 @@ public boolean rename(Path src, Path dst) throws IOException {
       return false;
     }
 
-    FileMetadata srcMetadata = store.retrieveMetadata(srcKey);
-    if (srcMetadata == null) {
-      // Source doesn't exist
-      if (LOG.isDebugEnabled()) {
-        LOG.debug("Source " + src + " doesn't exist, failing the rename.");
-      }
-      return false;
-    }
-
     // Figure out the final destination
     Path absoluteDst = makeAbsolute(dst);
     String dstKey = pathToKey(absoluteDst);
     FileMetadata dstMetadata = store.retrieveMetadata(dstKey);
-
-    // directory rename validations
-    if (srcMetadata.isDir()) {
-
-      // rename dir to self is an error
-      if (srcKey.equals(dstKey)) {
-        if (LOG.isDebugEnabled()) {
-          LOG.debug("Renaming directory to itself is disallowed. path=" + src);
-        }
-        return false;
-      }
-
-      // rename dir to (sub-)child of self is an error. see
-      // FileSystemContractBaseTest.testRenameChildDirForbidden
-      if (dstKey.startsWith(srcKey + PATH_DELIMITER)) {
-
-        if (LOG.isDebugEnabled()) {
-          LOG.debug("Renaming directory to a itself is disallowed. src=" + src
-              + " dest=" + dst);
-        }
-        return false;
-      }
-    }
-
-    // file rename early checks
-    if (!srcMetadata.isDir()) {
-      if (srcKey.equals(dstKey)) {
-        // rename file to self is OK
-        if (LOG.isDebugEnabled()) {
-          LOG.debug("Renaming file to itself. This is allowed and is treated as no-op. path="
-              + src);
-        }
-        return true;
-      }
-    }
-
-    // More validations..
-    // If target is dir but target already exists, alter the dst to be a
-    // subfolder.
-    // eg move("/a/file.txt", "/b") where "/b" already exists causes the target
-    // to be "/c/file.txt
     if (dstMetadata != null && dstMetadata.isDir()) {
+      // It's an existing directory.
       dstKey = pathToKey(makeAbsolute(new Path(dst, src.getName())));
-      // Best would be to update dstMetadata, but it is not used further, so set
-      // it to null and skip the additional cost
-      dstMetadata = null;
-      // dstMetadata = store.retrieveMetadata(dstKey);
       if (LOG.isDebugEnabled()) {
         LOG.debug("Destination " + dst
             + " is a directory, adjusted the destination to be " + dstKey);
       }
-
-      // rename dir to self is an error
-      if (srcKey.equals(dstKey)) {
-        if (LOG.isDebugEnabled()) {
-          LOG.debug("Renaming directory to itself is disallowed. path=" + src);
-        }
-        return false;
-      }
-
     } else if (dstMetadata != null) {
-      // Otherwise, attempting to overwrite a file is error
+      // Attempting to overwrite a file using rename()
       if (LOG.isDebugEnabled()) {
         LOG.debug("Destination " + dst
             + " is an already existing file, failing the rename.");
       }
       return false;
     } else {
-      // Either dir or file and target doesn't exist.. Check that the parent
-      // directory exists.
-      FileMetadata parentOfDestMetadata = store
-          .retrieveMetadata(pathToKey(absoluteDst.getParent()));
+      // Check that the parent directory exists.
+      FileMetadata parentOfDestMetadata =
+          store.retrieveMetadata(pathToKey(absoluteDst.getParent()));
       if (parentOfDestMetadata == null) {
         if (LOG.isDebugEnabled()) {
           LOG.debug("Parent of the destination " + dst
@@ -1164,88 +1968,136 @@ public boolean rename(Path src, Path dst) throws IOException {
         return false;
       }
     }
-
-    // Validations complete, do the move.
-    if (!srcMetadata.isDir()) {
+    FileMetadata srcMetadata = store.retrieveMetadata(srcKey);
+    if (srcMetadata == null) {
+      // Source doesn't exist
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Source " + src + " doesn't exist, failing the rename.");
+      }
+      return false;
+    } else if (!srcMetadata.isDir()) {
       if (LOG.isDebugEnabled()) {
         LOG.debug("Source " + src + " found as a file, renaming.");
       }
       store.rename(srcKey, dstKey);
     } else {
-      // Move everything inside the folder.
-      String priorLastKey = null;
 
-      // Calculate the index of the part of the string to be moved. That
-      // is everything on the path up to the folder name.
-      do {
-        // List all blobs rooted at the source folder.
-        PartialListing listing = store.listAll(srcKey, AZURE_LIST_ALL,
-            AZURE_UNBOUNDED_DEPTH, priorLastKey);
-
-        // Rename all the files in the folder.
-        for (FileMetadata file : listing.getFiles()) {
-          // Rename all materialized entries under the folder to point to the
-          // final destination.
-          if (file.getBlobMaterialization() == BlobMaterialization.Explicit) {
-            String srcName = file.getKey();
-            String suffix = srcName.substring(srcKey.length());
-            String dstName = dstKey + suffix;
-            store.rename(srcName, dstName);
-          }
-        }
-        priorLastKey = listing.getPriorLastKey();
-      } while (priorLastKey != null);
-      // Rename the top level empty blob for the folder.
-      if (srcMetadata.getBlobMaterialization() == BlobMaterialization.Explicit) {
-        store.rename(srcKey, dstKey);
+      // Prepare for, execute and clean up after of all files in folder, and
+      // the root file, and update the last modified time of the source and
+      // target parent folders. The operation can be redone if it fails part
+      // way through, by applying the "Rename Pending" file.
+
+      // The following code (internally) only does atomic rename preparation
+      // and lease management for page blob folders, limiting the scope of the
+      // operation to HBase log file folders, where atomic rename is required.
+      // In the future, we could generalize it easily to all folders.
+      renamePending = prepareAtomicFolderRename(srcKey, dstKey);
+      renamePending.execute();
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Renamed " + src + " to " + dst + " successfully.");
       }
+      renamePending.cleanup();
+      return true;
     }
 
-    // Update both source and destination parent folder last modified time.
-    Path srcParent = makeAbsolute(keyToPath(srcKey)).getParent();
-    if (srcParent != null && srcParent.getParent() != null) { // not root
-      String srcParentKey = pathToKey(srcParent);
+    // Update the last-modified time of the parent folders of both source
+    // and destination.
+    updateParentFolderLastModifiedTime(srcKey);
+    updateParentFolderLastModifiedTime(dstKey);
 
-      // ensure the srcParent is a materialized folder
-      FileMetadata srcParentMetadata = store.retrieveMetadata(srcParentKey);
-      if (srcParentMetadata.isDir()
-          && srcParentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {
-        store.storeEmptyFolder(srcParentKey,
-            createPermissionStatus(FsPermission.getDefault()));
-      }
-
-      store.updateFolderLastModifiedTime(srcParentKey);
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Renamed " + src + " to " + dst + " successfully.");
     }
+    return true;
+  }
 
-    Path destParent = makeAbsolute(keyToPath(dstKey)).getParent();
-    if (destParent != null && destParent.getParent() != null) { // not root
-      String dstParentKey = pathToKey(destParent);
+  /**
+   * Update the last-modified time of the parent folder of the file
+   * identified by key.
+   * @param key
+   * @throws IOException
+   */
+  private void updateParentFolderLastModifiedTime(String key)
+      throws IOException {
+    Path parent = makeAbsolute(keyToPath(key)).getParent();
+    if (parent != null && parent.getParent() != null) { // not root
+      String parentKey = pathToKey(parent);
 
-      // ensure the dstParent is a materialized folder
-      FileMetadata dstParentMetadata = store.retrieveMetadata(dstParentKey);
-      if (dstParentMetadata.isDir()
-          && dstParentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {
-        store.storeEmptyFolder(dstParentKey,
-            createPermissionStatus(FsPermission.getDefault()));
-      }
+      // ensure the parent is a materialized folder
+      FileMetadata parentMetadata = store.retrieveMetadata(parentKey);
+      // The metadata could be null if the implicit folder only contains a
+      // single file. In this case, the parent folder no longer exists if the
+      // file is renamed; so we can safely ignore the null pointer case.
+      if (parentMetadata != null) {
+        if (parentMetadata.isDir()
+            && parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {
+          store.storeEmptyFolder(parentKey,
+              createPermissionStatus(FsPermission.getDefault()));
+        }
 
-      store.updateFolderLastModifiedTime(dstParentKey);
+        store.updateFolderLastModifiedTime(parentKey, null);
+      }
     }
+  }
 
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Renamed " + src + " to " + dst + " successfully.");
+  /**
+   * If the source is a page blob folder,
+   * prepare to rename this folder atomically. This means to get exclusive
+   * access to the source folder, and record the actions to be performed for
+   * this rename in a "Rename Pending" file. This code was designed to
+   * meet the needs of HBase, which requires atomic rename of write-ahead log
+   * (WAL) folders for correctness.
+   *
+   * Before calling this method, the caller must ensure that the source is a
+   * folder.
+   *
+   * For non-page-blob directories, prepare the in-memory information needed,
+   * but don't take the lease or write the redo file. This is done to limit the
+   * scope of atomic folder rename to HBase, at least at the time of writing
+   * this code.
+   *
+   * @param srcKey Source folder name.
+   * @param dstKey Destination folder name.
+   * @throws IOException
+   */
+  private FolderRenamePending prepareAtomicFolderRename(
+      String srcKey, String dstKey) throws IOException {
+
+    if (store.isAtomicRenameKey(srcKey)) {
+
+      // Block unwanted concurrent access to source folder.
+      SelfRenewingLease lease = leaseSourceFolder(srcKey);
+
+      // Prepare in-memory information needed to do or redo a folder rename.
+      FolderRenamePending renamePending =
+          new FolderRenamePending(srcKey, dstKey, lease, this);
+
+      // Save it to persistent storage to help recover if the operation fails.
+      renamePending.writeFile(this);
+      return renamePending;
+    } else {
+      FolderRenamePending renamePending =
+          new FolderRenamePending(srcKey, dstKey, null, this);
+      return renamePending;
     }
-    return true;
   }
 
   /**
-   * Return an array containing hostnames, offset and size of portions of the
-   * given file. For WASB we'll just lie and give fake hosts to make sure we get
-   * many splits in MR jobs.
+   * Get a self-renewing Azure blob lease on the source folder zero-byte file.
+   */
+  private SelfRenewingLease leaseSourceFolder(String srcKey)
+      throws AzureException {
+    return store.acquireLease(srcKey);
+  }
+
+  /**
+   * Return an array containing hostnames, offset and size of
+   * portions of the given file. For WASB we'll just lie and give
+   * fake hosts to make sure we get many splits in MR jobs.
    */
   @Override
-  public BlockLocation[] getFileBlockLocations(FileStatus file, long start,
-      long len) throws IOException {
+  public BlockLocation[] getFileBlockLocations(FileStatus file,
+      long start, long len) throws IOException {
     if (file == null) {
       return null;
     }
@@ -1306,11 +2158,12 @@ public void setPermission(Path p, FsPermission permission) throws IOException {
     if (metadata.getBlobMaterialization() == BlobMaterialization.Implicit) {
       // It's an implicit folder, need to materialize it.
       store.storeEmptyFolder(key, createPermissionStatus(permission));
-    } else if (!metadata.getPermissionStatus().getPermission()
-        .equals(permission)) {
-      store.changePermissionStatus(key, new PermissionStatus(metadata
-          .getPermissionStatus().getUserName(), metadata.getPermissionStatus()
-          .getGroupName(), permission));
+    } else if (!metadata.getPermissionStatus().getPermission().
+        equals(permission)) {
+      store.changePermissionStatus(key, new PermissionStatus(
+          metadata.getPermissionStatus().getUserName(),
+          metadata.getPermissionStatus().getGroupName(),
+          permission));
     }
   }
 
@@ -1324,10 +2177,11 @@ public void setOwner(Path p, String username, String groupname)
       throw new FileNotFoundException("File doesn't exist: " + p);
     }
     PermissionStatus newPermissionStatus = new PermissionStatus(
-        username == null ? metadata.getPermissionStatus().getUserName()
-            : username, groupname == null ? metadata.getPermissionStatus()
-            .getGroupName() : groupname, metadata.getPermissionStatus()
-            .getPermission());
+        username == null ?
+            metadata.getPermissionStatus().getUserName() : username,
+        groupname == null ?
+            metadata.getPermissionStatus().getGroupName() : groupname,
+        metadata.getPermissionStatus().getPermission());
     if (metadata.getBlobMaterialization() == BlobMaterialization.Implicit) {
       // It's an implicit folder, need to materialize it.
       store.storeEmptyFolder(key, newPermissionStatus);
@@ -1341,12 +2195,12 @@ public synchronized void close() throws IOException {
     if (isClosed) {
       return;
     }
-    
+
     // Call the base close() to close any resources there.
     super.close();
-    // Close the store
+    // Close the store to close any resources there - e.g. the bandwidth
+    // updater thread would be stopped at this time.
     store.close();
-    
     // Notify the metrics system that this file system is closed, which may
     // trigger one final metrics push to get the accurate final file system
     // metrics out.
@@ -1364,16 +2218,17 @@ public synchronized void close() throws IOException {
   }
 
   /**
-   * A handler that defines what to do with blobs whose upload was interrupted.
+   * A handler that defines what to do with blobs whose upload was
+   * interrupted.
    */
   private abstract class DanglingFileHandler {
     abstract void handleFile(FileMetadata file, FileMetadata tempFile)
-        throws IOException;
+      throws IOException;
   }
 
   /**
-   * Handler implementation for just deleting dangling files and cleaning them
-   * up.
+   * Handler implementation for just deleting dangling files and cleaning
+   * them up.
    */
   private class DanglingFileDeleter extends DanglingFileHandler {
     @Override
@@ -1388,8 +2243,8 @@ void handleFile(FileMetadata file, FileMetadata tempFile)
   }
 
   /**
-   * Handler implementation for just moving dangling files to recovery location
-   * (/lost+found).
+   * Handler implementation for just moving dangling files to recovery
+   * location (/lost+found).
    */
   private class DanglingFileRecoverer extends DanglingFileHandler {
     private final Path destination;
@@ -1405,8 +2260,8 @@ void handleFile(FileMetadata file, FileMetadata tempFile)
         LOG.debug("Recovering " + file.getKey());
       }
       // Move to the final destination
-      String finalDestinationKey = pathToKey(new Path(destination,
-          file.getKey()));
+      String finalDestinationKey =
+          pathToKey(new Path(destination, file.getKey()));
       store.rename(tempFile.getKey(), finalDestinationKey);
       if (!finalDestinationKey.equals(file.getKey())) {
         // Delete the empty link file now that we've restored it.
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeFileSystemStore.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeFileSystemStore.java
index 4e1d0b6..0229cb7 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeFileSystemStore.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/NativeFileSystemStore.java
@@ -53,6 +53,10 @@ void storeEmptyFolder(String key, PermissionStatus permissionStatus)
   DataOutputStream storefile(String key, PermissionStatus permissionStatus)
       throws AzureException;
 
+  boolean isPageBlobKey(String key);
+
+  boolean isAtomicRenameKey(String key);
+
   void storeEmptyLinkFile(String key, String tempBlobKey,
       PermissionStatus permissionStatus) throws AzureException;
 
@@ -74,9 +78,12 @@ void changePermissionStatus(String key, PermissionStatus newPermission)
 
   void rename(String srcKey, String dstKey) throws IOException;
 
+  void rename(String srcKey, String dstKey, boolean acquireLease, SelfRenewingLease existingLease)
+      throws IOException;
+
   /**
    * Delete all keys with the given prefix. Used for testing.
-   * 
+   *
    * @throws IOException
    */
   @VisibleForTesting
@@ -84,15 +91,20 @@ void changePermissionStatus(String key, PermissionStatus newPermission)
 
   /**
    * Diagnostic method to dump state to the console.
-   * 
+   *
    * @throws IOException
    */
   void dump() throws IOException;
 
   void close();
 
-  void updateFolderLastModifiedTime(String key) throws AzureException;
-
-  void updateFolderLastModifiedTime(String key, Date lastModified)
+  void updateFolderLastModifiedTime(String key, SelfRenewingLease folderLease)
       throws AzureException;
+
+  void updateFolderLastModifiedTime(String key, Date lastModified,
+      SelfRenewingLease folderLease) throws AzureException;
+
+  void delete(String key, SelfRenewingLease lease) throws IOException;
+      
+  SelfRenewingLease acquireLease(String key) throws AzureException;
 }
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobFormatHelpers.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobFormatHelpers.java
new file mode 100644
index 0000000..ad11aac
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobFormatHelpers.java
@@ -0,0 +1,58 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs.azure;
+
+import java.nio.ByteBuffer;
+
+import com.microsoft.windowsazure.storage.blob.BlobRequestOptions;
+
+/**
+ * Constants and helper methods for ASV's custom data format in page blobs.
+ */
+final class PageBlobFormatHelpers {
+  public static final short PAGE_SIZE = 512;
+  public static final short PAGE_HEADER_SIZE = 2;
+  public static final short PAGE_DATA_SIZE = PAGE_SIZE - PAGE_HEADER_SIZE;
+
+  // Hide constructor for utility class.
+  private PageBlobFormatHelpers() {
+    
+  }
+  
+  /**
+   * Stores the given short as a two-byte array.
+   */
+  public static byte[] fromShort(short s) {
+    return ByteBuffer.allocate(2).putShort(s).array();
+  }
+
+  /**
+   * Retrieves a short from the given two bytes.
+   */
+  public static short toShort(byte firstByte, byte secondByte) {
+    return ByteBuffer.wrap(new byte[] { firstByte, secondByte })
+        .getShort();
+  }
+
+  public static BlobRequestOptions withMD5Checking() {
+    BlobRequestOptions options = new BlobRequestOptions();
+    options.setUseTransactionalContentMD5(true);
+    return options;
+  }
+}
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobInputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobInputStream.java
new file mode 100644
index 0000000..62b47ee
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobInputStream.java
@@ -0,0 +1,455 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs.azure;
+
+import static org.apache.hadoop.fs.azure.PageBlobFormatHelpers.PAGE_DATA_SIZE;
+import static org.apache.hadoop.fs.azure.PageBlobFormatHelpers.PAGE_HEADER_SIZE;
+import static org.apache.hadoop.fs.azure.PageBlobFormatHelpers.PAGE_SIZE;
+import static org.apache.hadoop.fs.azure.PageBlobFormatHelpers.toShort;
+import static org.apache.hadoop.fs.azure.PageBlobFormatHelpers.withMD5Checking;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.ArrayList;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.azure.StorageInterface.CloudPageBlobWrapper;
+
+import com.microsoft.windowsazure.storage.OperationContext;
+import com.microsoft.windowsazure.storage.StorageException;
+import com.microsoft.windowsazure.storage.blob.BlobRequestOptions;
+import com.microsoft.windowsazure.storage.blob.PageRange;
+
+/**
+ * An input stream that reads file data from a page blob stored
+ * using ASV's custom format.
+ */
+
+final class PageBlobInputStream extends InputStream {
+  private static final Log LOG = LogFactory.getLog(PageBlobInputStream.class);
+
+  // The blob we're reading from.
+  private final CloudPageBlobWrapper blob;
+  // The operation context to use for storage requests.
+  private final OperationContext opContext;
+  // The number of pages remaining to be read from the server.
+  private long numberOfPagesRemaining;
+  // The current byte offset to start reading from the server next,
+  // equivalent to (total number of pages we've read) * (page size).
+  private long currentOffsetInBlob;
+  // The buffer holding the current data we last read from the server.
+  private byte[] currentBuffer;
+  // The current byte offset we're at in the buffer.
+  private int currentOffsetInBuffer;
+  // Maximum number of pages to get per any one request.
+  private static final int MAX_PAGES_PER_DOWNLOAD =
+      4 * 1024 * 1024 / PAGE_SIZE;
+  // Whether the stream has been closed.
+  private boolean closed = false;
+  // Total stream size, or -1 if not initialized.
+  long pageBlobSize = -1;
+  // Current position in stream of valid data.
+  long filePosition = 0;
+
+  /**
+   * Helper method to extract the actual data size of a page blob.
+   * This typically involves 2 service requests (one for page ranges, another
+   * for the last page's data).
+   *
+   * @param blob The blob to get the size from.
+   * @param opContext The operation context to use for the requests.
+   * @return The total data size of the blob in bytes.
+   * @throws IOException If the format is corrupt.
+   * @throws StorageException If anything goes wrong in the requests.
+   */
+  public static long getPageBlobSize(CloudPageBlobWrapper blob,
+      OperationContext opContext) throws IOException, StorageException {
+    // Get the page ranges for the blob. There should be one range starting
+    // at byte 0, but we tolerate (and ignore) ranges after the first one.
+    ArrayList<PageRange> pageRanges =
+        blob.downloadPageRanges(new BlobRequestOptions(), opContext);
+    if (pageRanges.size() == 0) {
+      return 0;
+    }
+    if (pageRanges.get(0).getStartOffset() != 0) {
+      // Not expected: we always upload our page blobs as a contiguous range
+      // starting at byte 0.
+      throw badStartRangeException(blob, pageRanges.get(0));
+    }
+    long totalRawBlobSize = pageRanges.get(0).getEndOffset() + 1;
+
+    // Get the last page.
+    long lastPageStart = totalRawBlobSize - PAGE_SIZE;
+    ByteArrayOutputStream baos = 
+        new ByteArrayOutputStream(PageBlobFormatHelpers.PAGE_SIZE);
+    blob.downloadRange(lastPageStart, PAGE_SIZE, baos,
+        new BlobRequestOptions(), opContext);
+
+    byte[] lastPage = baos.toByteArray();
+    short lastPageSize = getPageSize(blob, lastPage, 0);
+    long totalNumberOfPages = totalRawBlobSize / PAGE_SIZE;
+    return (totalNumberOfPages - 1) * PAGE_DATA_SIZE + lastPageSize;
+  }
+
+  /**
+   * Constructs a stream over the given page blob.
+   */
+  public PageBlobInputStream(CloudPageBlobWrapper blob,
+      OperationContext opContext)
+      throws IOException {
+    this.blob = blob;
+    this.opContext = opContext;
+    ArrayList<PageRange> allRanges;
+    try {
+      allRanges =
+          blob.downloadPageRanges(new BlobRequestOptions(), opContext);
+    } catch (StorageException e) {
+      throw new IOException(e);
+    }
+    if (allRanges.size() > 0) {
+      if (allRanges.get(0).getStartOffset() != 0) {
+        throw badStartRangeException(blob, allRanges.get(0));
+      }
+      if (allRanges.size() > 1) {
+        LOG.warn(String.format(
+            "Blob %s has %d page ranges beyond the first range. " 
+            + "Only reading the first range.",
+            blob.getUri(), allRanges.size() - 1));
+      }
+      numberOfPagesRemaining =
+          (allRanges.get(0).getEndOffset() + 1) / PAGE_SIZE;
+    } else {
+      numberOfPagesRemaining = 0;
+    }
+  }
+
+  /** Return the size of the remaining available bytes
+   * if the size is less than or equal to {@link Integer#MAX_VALUE},
+   * otherwise, return {@link Integer#MAX_VALUE}.
+   *
+   * This is to match the behavior of DFSInputStream.available(),
+   * which some clients may rely on (HBase write-ahead log reading in
+   * particular).
+   */
+  @Override
+  public synchronized int available() throws IOException {
+    if (closed) {
+      throw new IOException("Stream closed");
+    }
+    if (pageBlobSize == -1) {
+      try {
+        pageBlobSize = getPageBlobSize(blob, opContext);
+      } catch (StorageException e) {
+        throw new IOException("Unable to get page blob size.", e);
+      }
+    }
+
+    final long remaining = pageBlobSize - filePosition;
+    return remaining <= Integer.MAX_VALUE ?
+        (int) remaining : Integer.MAX_VALUE;
+  }
+
+  @Override
+  public synchronized void close() throws IOException {
+    closed = true;
+  }
+
+  private boolean dataAvailableInBuffer() {
+    return currentBuffer != null 
+        && currentOffsetInBuffer < currentBuffer.length;
+  }
+
+  /**
+   * Check our buffer and download more from the server if needed.
+   * @return true if there's more data in the buffer, false if we're done.
+   * @throws IOException
+   */
+  private synchronized boolean ensureDataInBuffer() throws IOException {
+    if (dataAvailableInBuffer()) {
+      // We still have some data in our buffer.
+      return true;
+    }
+    currentBuffer = null;
+    if (numberOfPagesRemaining == 0) {
+      // No more data to read.
+      return false;
+    }
+    final long pagesToRead = Math.min(MAX_PAGES_PER_DOWNLOAD,
+        numberOfPagesRemaining);
+    final int bufferSize = (int) (pagesToRead * PAGE_SIZE);
+ 
+    // Download page to current buffer.
+    try {
+      // Create a byte array output stream to capture the results of the
+      // download.
+      ByteArrayOutputStream baos = new ByteArrayOutputStream(bufferSize);
+      blob.downloadRange(currentOffsetInBlob, bufferSize, baos,
+          withMD5Checking(), opContext);
+      currentBuffer = baos.toByteArray();
+    } catch (StorageException e) {
+      throw new IOException(e);
+    }
+    numberOfPagesRemaining -= pagesToRead;
+    currentOffsetInBlob += bufferSize;
+    currentOffsetInBuffer = PAGE_HEADER_SIZE;
+
+    // Since we just downloaded a new buffer, validate its consistency.
+    validateCurrentBufferConsistency();
+
+    return true;
+  }
+
+  private void validateCurrentBufferConsistency()
+      throws IOException {
+    if (currentBuffer.length % PAGE_SIZE != 0) {
+      throw new AssertionError("Unexpected buffer size: " 
+      + currentBuffer.length);
+    }
+    int numberOfPages = currentBuffer.length / PAGE_SIZE;
+    for (int page = 0; page < numberOfPages; page++) {
+      short currentPageSize = getPageSize(blob, currentBuffer,
+          page * PAGE_SIZE);
+      // Calculate the number of pages that exist after this one
+      // in the blob.
+      long totalPagesAfterCurrent =
+          (numberOfPages - page - 1) + numberOfPagesRemaining;
+      // Only the last page is allowed to be not filled completely.
+      if (currentPageSize < PAGE_DATA_SIZE 
+          && totalPagesAfterCurrent > 0) {
+        throw fileCorruptException(blob, String.format(
+            "Page with partial data found in the middle (%d pages from the" 
+            + " end) that only has %d bytes of data.",
+            totalPagesAfterCurrent, currentPageSize));
+      }
+    }
+  }
+
+  // Reads the page size from the page header at the given offset.
+  private static short getPageSize(CloudPageBlobWrapper blob,
+      byte[] data, int offset) throws IOException {
+    short pageSize = toShort(data[offset], data[offset + 1]);
+    if (pageSize < 0 || pageSize > PAGE_DATA_SIZE) {
+      throw fileCorruptException(blob, String.format(
+          "Unexpected page size in the header: %d.",
+          pageSize));
+    }
+    return pageSize;
+  }
+
+  @Override
+  public synchronized int read(byte[] outputBuffer, int offset, int len)
+      throws IOException {
+    int numberOfBytesRead = 0;
+    while (len > 0) {
+      if (!ensureDataInBuffer()) {
+        filePosition += numberOfBytesRead;
+        return numberOfBytesRead;
+      }
+      int bytesRemainingInCurrentPage = getBytesRemainingInCurrentPage();
+      int numBytesToRead = Math.min(len, bytesRemainingInCurrentPage);
+      System.arraycopy(currentBuffer, currentOffsetInBuffer, outputBuffer,
+          offset, numBytesToRead);
+      numberOfBytesRead += numBytesToRead;
+      offset += numBytesToRead;
+      len -= numBytesToRead;
+      if (numBytesToRead == bytesRemainingInCurrentPage) {
+        // We've finished this page, move on to the next.
+        advancePagesInBuffer(1);
+      } else {
+        currentOffsetInBuffer += numBytesToRead;
+      }
+    }
+    filePosition += numberOfBytesRead;
+    return numberOfBytesRead;
+  }
+
+  @Override
+  public int read() throws IOException {
+    byte[] oneByte = new byte[1];
+    if (read(oneByte) == 0) {
+      return -1;
+    }
+    return oneByte[0];
+  }
+
+  /**
+   * Skips over and discards n bytes of data from this input stream.
+   * @param n the number of bytes to be skipped.
+   * @return the actual number of bytes skipped.
+   */
+  @Override
+  public synchronized long skip(long n) throws IOException {
+    long skipped = skipImpl(n);
+    filePosition += skipped; // track the position in the stream
+    return skipped;
+  }
+
+  private long skipImpl(long n) throws IOException {
+
+    if (n == 0) {
+      return 0;
+    }
+
+    // First skip within the current buffer as much as possible.
+    long skippedWithinBuffer = skipWithinBuffer(n);
+    if (skippedWithinBuffer > n) {
+      // TO CONSIDER: Using a contracts framework such as Google's cofoja for
+      // these post-conditions.
+      throw new AssertionError(String.format(
+          "Bug in skipWithinBuffer: it skipped over %d bytes when asked to "
+          + "skip %d bytes.", skippedWithinBuffer, n));
+    }
+    n -= skippedWithinBuffer;
+    long skipped = skippedWithinBuffer;
+
+    // Empty the current buffer, we're going beyond it.
+    currentBuffer = null;
+
+    // Skip over whole pages as necessary without retrieving them from the
+    // server.
+    long pagesToSkipOver = Math.min(
+        n / PAGE_DATA_SIZE,
+        numberOfPagesRemaining - 1);
+    numberOfPagesRemaining -= pagesToSkipOver;
+    currentOffsetInBlob += pagesToSkipOver * PAGE_SIZE;
+    skipped += pagesToSkipOver * PAGE_DATA_SIZE;
+    n -= pagesToSkipOver * PAGE_DATA_SIZE;
+    if (n == 0) {
+      return skipped;
+    }
+
+    // Now read in at the current position, and skip within current buffer.
+    if (!ensureDataInBuffer()) {
+      return skipped;
+    }
+    return skipped + skipWithinBuffer(n);
+  }
+
+  /**
+   * Skip over n bytes within the current buffer or just over skip the whole
+   * buffer if n is greater than the bytes remaining in the buffer.
+   * @param n The number of data bytes to skip.
+   * @return The number of bytes actually skipped.
+   * @throws IOException if data corruption found in the buffer.
+   */
+  private long skipWithinBuffer(long n) throws IOException {
+    if (!dataAvailableInBuffer()) {
+      return 0;
+    }
+    long skipped = 0;
+    // First skip within the current page.
+    skipped = skipWithinCurrentPage(n);
+    if (skipped > n) {
+      throw new AssertionError(String.format(
+          "Bug in skipWithinCurrentPage: it skipped over %d bytes when asked" 
+          + " to skip %d bytes.", skipped, n));
+    }
+    n -= skipped;
+    if (n == 0 || !dataAvailableInBuffer()) {
+      return skipped;
+    }
+
+    // Calculate how many whole pages (pages before the possibly partially
+    // filled last page) remain.
+    int currentPageIndex = currentOffsetInBuffer / PAGE_SIZE;
+    int numberOfPagesInBuffer = currentBuffer.length / PAGE_SIZE;
+    int wholePagesRemaining = numberOfPagesInBuffer - currentPageIndex - 1;
+
+    if (n < (PAGE_DATA_SIZE * wholePagesRemaining)) {
+      // I'm within one of the whole pages remaining, skip in there.
+      advancePagesInBuffer((int) (n / PAGE_DATA_SIZE));
+      currentOffsetInBuffer += n % PAGE_DATA_SIZE;
+      return n + skipped;
+    }
+
+    // Skip over the whole pages.
+    advancePagesInBuffer(wholePagesRemaining);
+    skipped += wholePagesRemaining * PAGE_DATA_SIZE;
+    n -= wholePagesRemaining * PAGE_DATA_SIZE;
+
+    // At this point we know we need to skip to somewhere in the last page,
+    // or just go to the end.
+    return skipWithinCurrentPage(n) + skipped;
+  }
+
+  /**
+   * Skip over n bytes within the current page or just over skip the whole
+   * page if n is greater than the bytes remaining in the page.
+   * @param n The number of data bytes to skip.
+   * @return The number of bytes actually skipped.
+   * @throws IOException if data corruption found in the buffer.
+   */
+  private long skipWithinCurrentPage(long n) throws IOException {
+    int remainingBytesInCurrentPage = getBytesRemainingInCurrentPage();
+    if (n < remainingBytesInCurrentPage) {
+      currentOffsetInBuffer += n;
+      return n;
+    } else {
+      advancePagesInBuffer(1);
+      return remainingBytesInCurrentPage;
+    }
+  }
+
+  /**
+   * Gets the number of bytes remaining within the current page in the buffer.
+   * @return The number of bytes remaining.
+   * @throws IOException if data corruption found in the buffer.
+   */
+  private int getBytesRemainingInCurrentPage() throws IOException {
+    if (!dataAvailableInBuffer()) {
+      return 0;
+    }
+    // Calculate our current position relative to the start of the current
+    // page.
+    int currentDataOffsetInPage =
+        (currentOffsetInBuffer % PAGE_SIZE) - PAGE_HEADER_SIZE;
+    int pageBoundary = getCurrentPageStartInBuffer();
+    // Get the data size of the current page from the header.
+    short sizeOfCurrentPage = getPageSize(blob, currentBuffer, pageBoundary);
+    return sizeOfCurrentPage - currentDataOffsetInPage;
+  }
+
+  private static IOException badStartRangeException(CloudPageBlobWrapper blob,
+      PageRange startRange) {
+    return fileCorruptException(blob, String.format(
+        "Page blobs for ASV should always use a page range starting at byte 0. " 
+        + "This starts at byte %d.",
+        startRange.getStartOffset()));
+  }
+
+  private void advancePagesInBuffer(int numberOfPages) {
+    currentOffsetInBuffer =
+        getCurrentPageStartInBuffer() 
+        + (numberOfPages * PAGE_SIZE) 
+        + PAGE_HEADER_SIZE;
+  }
+
+  private int getCurrentPageStartInBuffer() {
+    return PAGE_SIZE * (currentOffsetInBuffer / PAGE_SIZE);
+  }
+
+  private static IOException fileCorruptException(CloudPageBlobWrapper blob,
+      String reason) {
+    return new IOException(String.format(
+        "The page blob: '%s' is corrupt or has an unexpected format: %s.",
+        blob.getUri(), reason));
+  }
+}
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobOutputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobOutputStream.java
new file mode 100644
index 0000000..95f0c22
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobOutputStream.java
@@ -0,0 +1,497 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs.azure;
+
+import static org.apache.hadoop.fs.azure.PageBlobFormatHelpers.PAGE_DATA_SIZE;
+import static org.apache.hadoop.fs.azure.PageBlobFormatHelpers.PAGE_HEADER_SIZE;
+import static org.apache.hadoop.fs.azure.PageBlobFormatHelpers.PAGE_SIZE;
+import static org.apache.hadoop.fs.azure.PageBlobFormatHelpers.fromShort;
+import static org.apache.hadoop.fs.azure.PageBlobFormatHelpers.withMD5Checking;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.util.Arrays;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.hadoop.fs.Syncable;
+import org.apache.hadoop.fs.azure.StorageInterface.CloudPageBlobWrapper;
+import org.apache.commons.lang.exception.ExceptionUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.microsoft.windowsazure.storage.OperationContext;
+import com.microsoft.windowsazure.storage.StorageException;
+import com.microsoft.windowsazure.storage.blob.BlobRequestOptions;
+
+
+/**
+ * An output stream that write file data to a page blob stored using ASV's
+ * custom format.
+ */
+final class PageBlobOutputStream extends OutputStream implements Syncable {
+  /**
+   * The maximum number of raw bytes Azure Storage allows us to upload in a
+   * single request (4 MB).
+   */
+  private static final int MAX_RAW_BYTES_PER_REQUEST = 4 * 1024 * 1024;
+  /**
+   * The maximum number of pages Azure Storage allows us to upload in a
+   * single request.
+   */
+  private static final int MAX_PAGES_IN_REQUEST =
+      MAX_RAW_BYTES_PER_REQUEST / PAGE_SIZE;
+  /**
+   * The maximum number of data bytes (header not included) we can upload
+   * in a single request. I'm limiting it to (N - 1) pages to account for
+   * the possibility that we may have to rewrite the previous request's
+   * last page.
+   */
+  private static final int MAX_DATA_BYTES_PER_REQUEST =
+      PAGE_DATA_SIZE * (MAX_PAGES_IN_REQUEST - 1);
+
+  private final CloudPageBlobWrapper blob;
+  private final OperationContext opContext;
+
+  /**
+   * If the IO thread encounters an error, it'll store it here.
+   */
+  private volatile IOException lastError;
+
+  /**
+   * The current byte offset we're at in the blob (how many bytes we've
+   * uploaded to the server).
+   */
+  private long currentBlobOffset;
+  /**
+   * The data in the last page that we wrote to the server, in case we have to
+   * overwrite it in the new request.
+   */
+  private byte[] previousLastPageDataWritten = new byte[0];
+  /**
+   * The current buffer we're writing to before sending to the server.
+   */
+  private ByteArrayOutputStream outBuffer;
+  /**
+   * The task queue for writing to the server.
+   */
+  private final LinkedBlockingQueue<Runnable> ioQueue;
+  /**
+   * The thread pool we're using for writing to the server. Note that the IO
+   * write is NOT designed for parallelism, so there can only be one thread
+   * in that pool (I'm using the thread pool mainly for the lifetime management
+   * capabilities, otherwise I'd have just used a simple Thread).
+   */
+  private final ThreadPoolExecutor ioThreadPool;
+
+  // The last task given to the ioThreadPool to execute, to allow
+  // waiting until it's done.
+  private WriteRequest lastQueuedTask;
+
+  public static final Log LOG = LogFactory.getLog(AzureNativeFileSystemStore.class);
+
+  // Set the minimum page blob file size to 128MB, which is >> the default block size of 32MB.
+  // This default block size is often used as the hbase.regionserver.hlog.blocksize.
+  // The goal is to have a safe minimum size for HBase log files to allow them
+  // to be filled and rolled without exceeding the minimum size. A larger size can be
+  // used by setting the fs.azure.page.blob.size configuration variable.
+  public static final long PAGE_BLOB_MIN_SIZE = 128L * 1024L * 1024L;
+
+  /**
+   * Constructs an output stream over the given page blob.
+   *
+   * @param blob the blob that this stream is associated with.
+   * @param opContext an object used to track the execution of the operation
+   * @throws StorageException if anything goes wrong creating the blob.
+   */
+  public PageBlobOutputStream(final CloudPageBlobWrapper blob,
+      final OperationContext opContext,
+      final Configuration conf) throws StorageException {
+    this.blob = blob;
+    this.outBuffer = new ByteArrayOutputStream();
+    this.opContext = opContext;
+    this.lastQueuedTask = null;
+    this.ioQueue = new LinkedBlockingQueue<Runnable>();
+
+    // As explained above: the IO writes are not designed for parallelism,
+    // so we only have one thread in this thread pool.
+    this.ioThreadPool = new ThreadPoolExecutor(1, 1, 2, TimeUnit.SECONDS,
+        ioQueue);
+
+
+
+    // Make page blob files have a size that is the greater of a
+    // minimum size, or the value of fs.azure.page.blob.size from configuration.
+    long pageBlobConfigSize = conf.getLong("fs.azure.page.blob.size", 0);
+    LOG.debug("Read value of fs.azure.page.blob.size as " + pageBlobConfigSize
+        + " from configuration (0 if not present).");
+    long pageBlobSize = Math.max(PAGE_BLOB_MIN_SIZE, pageBlobConfigSize);
+
+    // Ensure that the pageBlobSize is a multiple of page size.
+    if (pageBlobSize % PAGE_SIZE != 0) {
+      pageBlobSize += PAGE_SIZE - pageBlobSize % PAGE_SIZE;
+    }
+    blob.create(pageBlobSize, new BlobRequestOptions(), opContext);
+  }
+
+  private void checkStreamState() throws IOException {
+    if (lastError != null) {
+      throw lastError;
+    }
+  }
+
+  /**
+   * Closes this output stream and releases any system resources associated with
+   * this stream. If any data remains in the buffer it is committed to the
+   * service.
+   */
+  @Override
+  public void close() throws IOException {
+    LOG.debug("Closing page blob output stream.");
+    flush();
+    checkStreamState();
+    ioThreadPool.shutdown();
+    try {
+      LOG.debug(ioThreadPool.toString());
+      if (!ioThreadPool.awaitTermination(10, TimeUnit.MINUTES)) {
+        LOG.debug("Timed out after 10 minutes waiting for IO requests to finish");
+        logAllStackTraces();
+        LOG.debug(ioThreadPool.toString());
+        throw new IOException("Timed out waiting for IO requests to finish");
+      }
+    } catch (InterruptedException e) {
+      LOG.debug("Caught InterruptedException");
+
+      // Restore the interrupted status
+      Thread.currentThread().interrupt();
+    }
+
+    this.lastError = new IOException("Stream is already closed.");
+  }
+
+  // Log the stacks of all threads.
+  private void logAllStackTraces() {
+    Map liveThreads = Thread.getAllStackTraces();
+    for (Iterator i = liveThreads.keySet().iterator(); i.hasNext(); ) {
+      Thread key = (Thread) i.next();
+      LOG.debug("Thread " + key.getName());
+      StackTraceElement[] trace = (StackTraceElement[]) liveThreads.get(key);
+      for (int j = 0; j < trace.length; j++) {
+        LOG.debug("\tat " + trace[j]);
+      }
+    }
+  }
+
+  /**
+   * A single write request for data to write to Azure storage.
+   */
+  private class WriteRequest implements Runnable {
+    private final byte[] dataPayload;
+    private final CountDownLatch doneSignal = new CountDownLatch(1);
+
+    public WriteRequest(byte[] dataPayload) {
+      this.dataPayload = dataPayload;
+    }
+
+    public void waitTillDone() throws InterruptedException {
+      doneSignal.await();
+    }
+
+    @Override
+    public void run() {
+      try {
+        LOG.debug("before runInternal()");
+        runInternal();
+        LOG.debug("after runInternal()");
+      } finally {
+        doneSignal.countDown();
+      }
+    }
+
+    private void runInternal() {
+      if (lastError != null) {
+        // We're already in an error state, no point doing anything.
+        return;
+      }
+      if (dataPayload.length == 0) {
+        // Nothing to do.
+        return;
+      }
+
+      // Since we have to rewrite the last request's last page's data
+      // (may be empty), total data size is our data plus whatever was
+      // left from there.
+      final int totalDataBytes = dataPayload.length 
+          + previousLastPageDataWritten.length;
+      // Calculate the total number of pages we're writing to the server.
+      final int numberOfPages = (totalDataBytes / PAGE_DATA_SIZE) 
+          + (totalDataBytes % PAGE_DATA_SIZE == 0 ? 0 : 1);
+      // Fill up the raw bytes we're writing.
+      byte[] rawPayload = new byte[numberOfPages * PAGE_SIZE];
+      // Keep track of the size of the last page we uploaded.
+      int currentLastPageDataSize = -1;
+      for (int page = 0; page < numberOfPages; page++) {
+        // Our current byte offset in the data.
+        int dataOffset = page * PAGE_DATA_SIZE;
+        // Our current byte offset in the raw buffer.
+        int rawOffset = page * PAGE_SIZE;
+        // The size of the data in the current page.
+        final short currentPageDataSize = (short) Math.min(PAGE_DATA_SIZE,
+            totalDataBytes - dataOffset);
+        // Save off this page's size as the potential last page's size.
+        currentLastPageDataSize = currentPageDataSize;
+
+        // Write out the page size in the header.
+        final byte[] header = fromShort(currentPageDataSize);
+        System.arraycopy(header, 0, rawPayload, rawOffset, header.length);
+        rawOffset += header.length;
+
+        int bytesToCopyFromDataPayload = currentPageDataSize;
+        if (dataOffset < previousLastPageDataWritten.length) {
+          // First write out the last page's data.
+          final int bytesToCopyFromLastPage = Math.min(currentPageDataSize,
+              previousLastPageDataWritten.length - dataOffset);
+          System.arraycopy(previousLastPageDataWritten, dataOffset,
+              rawPayload, rawOffset, bytesToCopyFromLastPage);
+          bytesToCopyFromDataPayload -= bytesToCopyFromLastPage;
+          rawOffset += bytesToCopyFromLastPage;
+          dataOffset += bytesToCopyFromLastPage;
+        }
+
+        if (dataOffset >= previousLastPageDataWritten.length) {
+          // Then write the current payload's data.
+          System.arraycopy(dataPayload, 
+        	  dataOffset - previousLastPageDataWritten.length,
+              rawPayload, rawOffset, bytesToCopyFromDataPayload);
+        }
+      }
+
+      // Raw payload constructed, ship it off to the server.
+      writePayloadToServer(rawPayload);
+
+      // Post-send bookkeeping.
+      currentBlobOffset += rawPayload.length;
+      if (currentLastPageDataSize < PAGE_DATA_SIZE) {
+        // Partial page, save it off so it's overwritten in the next request.
+        final int startOffset = (numberOfPages - 1) * PAGE_SIZE + PAGE_HEADER_SIZE;
+        previousLastPageDataWritten = Arrays.copyOfRange(rawPayload,
+            startOffset,
+            startOffset + currentLastPageDataSize);
+        // Since we're rewriting this page, set our current offset in the server
+        // to that page's beginning.
+        currentBlobOffset -= PAGE_SIZE;
+      } else {
+        // It wasn't a partial page, we won't need to rewrite it.
+        previousLastPageDataWritten = new byte[0];
+      }
+    }
+
+    /**
+     * Writes the given raw payload to Azure Storage at the current blob
+     * offset.
+     */
+    private void writePayloadToServer(byte[] rawPayload) {
+      final ByteArrayInputStream wrapperStream =
+                  new ByteArrayInputStream(rawPayload);
+      LOG.debug("writing payload of " + rawPayload.length + " bytes to Azure page blob");
+      try {
+        long start = System.currentTimeMillis();
+        blob.uploadPages(wrapperStream, currentBlobOffset, rawPayload.length,
+            withMD5Checking(), PageBlobOutputStream.this.opContext);
+        long end = System.currentTimeMillis();
+        LOG.trace("Azure uploadPages time for " + rawPayload.length + " bytes = " + (end - start));
+      } catch (IOException ex) {
+        LOG.debug(ExceptionUtils.getStackTrace(ex));
+        lastError = ex;
+      } catch (StorageException ex) {
+        LOG.debug(ExceptionUtils.getStackTrace(ex));
+        lastError = new IOException(ex);
+      }
+      if (lastError != null) {
+        LOG.debug("Caught error in PageBlobOutputStream#writePayloadToServer()");
+      }
+    }
+  }
+
+  private synchronized void flushIOBuffers()  {
+    if (outBuffer.size() == 0) {
+      return;
+    }
+    lastQueuedTask = new WriteRequest(outBuffer.toByteArray());
+    ioThreadPool.execute(lastQueuedTask);
+    outBuffer = new ByteArrayOutputStream();
+  }
+
+  /**
+   * Flushes this output stream and forces any buffered output bytes to be
+   * written out. If any data remains in the buffer it is committed to the
+   * service. Data is queued for writing but not forced out to the service
+   * before the call returns.
+   */
+  @Override
+  public void flush() throws IOException {
+    checkStreamState();
+    flushIOBuffers();
+  }
+
+  /**
+   * Writes b.length bytes from the specified byte array to this output stream.
+   *
+   * @param data
+   *          the byte array to write.
+   *
+   * @throws IOException
+   *           if an I/O error occurs. In particular, an IOException may be
+   *           thrown if the output stream has been closed.
+   */
+  @Override
+  public void write(final byte[] data) throws IOException {
+    write(data, 0, data.length);
+  }
+
+  /**
+   * Writes length bytes from the specified byte array starting at offset to
+   * this output stream.
+   *
+   * @param data
+   *          the byte array to write.
+   * @param offset
+   *          the start offset in the data.
+   * @param length
+   *          the number of bytes to write.
+   * @throws IOException
+   *           if an I/O error occurs. In particular, an IOException may be
+   *           thrown if the output stream has been closed.
+   */
+  @Override
+  public void write(final byte[] data, final int offset, final int length)
+      throws IOException {
+    if (offset < 0 || length < 0 || length > data.length - offset) {
+      throw new IndexOutOfBoundsException();
+    }
+
+    writeInternal(data, offset, length);
+  }
+
+  /**
+   * Writes the specified byte to this output stream. The general contract for
+   * write is that one byte is written to the output stream. The byte to be
+   * written is the eight low-order bits of the argument b. The 24 high-order
+   * bits of b are ignored.
+   *
+   * @param byteVal
+   *          the byteValue to write.
+   * @throws IOException
+   *           if an I/O error occurs. In particular, an IOException may be
+   *           thrown if the output stream has been closed.
+   */
+  @Override
+  public void write(final int byteVal) throws IOException {
+    write(new byte[] { (byte) (byteVal & 0xFF) });
+  }
+
+  /**
+   * Writes the data to the buffer and triggers writes to the service as needed.
+   *
+   * @param data
+   *          the byte array to write.
+   * @param offset
+   *          the start offset in the data.
+   * @param length
+   *          the number of bytes to write.
+   * @throws IOException
+   *           if an I/O error occurs. In particular, an IOException may be
+   *           thrown if the output stream has been closed.
+   */
+  private synchronized void writeInternal(final byte[] data, int offset,
+      int length) throws IOException {
+    while (length > 0) {
+      checkStreamState();
+      final int availableBufferBytes = MAX_DATA_BYTES_PER_REQUEST
+          - this.outBuffer.size();
+      final int nextWrite = Math.min(availableBufferBytes, length);
+
+      outBuffer.write(data, offset, nextWrite);
+      offset += nextWrite;
+      length -= nextWrite;
+
+      if (outBuffer.size() > MAX_DATA_BYTES_PER_REQUEST) {
+        throw new RuntimeException("Internal error: maximum write size " +
+            Integer.toString(MAX_DATA_BYTES_PER_REQUEST) + "exceeded.");
+      }
+
+      if (outBuffer.size() == MAX_DATA_BYTES_PER_REQUEST) {
+        flushIOBuffers();
+      }
+    }
+  }
+
+  /**
+   * Force all data in the output stream to be written to Azure storage.
+   * Wait to return until this is complete.
+   */
+  @Override
+  public synchronized void hsync() throws IOException {
+    LOG.debug("Entering PageBlobOutputStream#hsync().");
+    long start = System.currentTimeMillis();
+  	flush();
+    LOG.debug(ioThreadPool.toString());
+    try {
+      if (lastQueuedTask != null) {
+        lastQueuedTask.waitTillDone();
+      }
+    } catch (InterruptedException e1) {
+
+      // Restore the interrupted status
+      Thread.currentThread().interrupt();
+    }
+    LOG.debug("Leaving PageBlobOutputStream#hsync(). Total hsync duration = "
+  	  + (System.currentTimeMillis() - start) + " msec.");
+  }
+
+  @Override
+
+  public void hflush() throws IOException {
+
+    // hflush is required to force data to storage, so call hsync,
+    // which does that.
+    hsync();
+  }
+
+  @Deprecated
+  public void sync() throws IOException {
+
+    // Sync has been deprecated in favor of hflush.
+    hflush();
+  }
+
+  // For unit testing purposes: kill the IO threads.
+  @VisibleForTesting
+  void killIoThreads() {
+    ioThreadPool.shutdownNow();
+  }
+}
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PartialListing.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PartialListing.java
index 9e49de8..4a80d2e 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PartialListing.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PartialListing.java
@@ -30,7 +30,7 @@
  * This listing may be returned in chunks, so a <code>priorLastKey</code> is
  * provided so that the next chunk may be requested.
  * </p>
- * 
+ *
  * @see NativeFileSystemStore#list(String, int, String)
  */
 @InterfaceAudience.Private
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfRenewingLease.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfRenewingLease.java
new file mode 100644
index 0000000..2d5c0c8
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfRenewingLease.java
@@ -0,0 +1,202 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs.azure;
+
+import org.apache.commons.lang.exception.ExceptionUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.azure.StorageInterface.CloudBlobWrapper;
+
+import com.microsoft.windowsazure.storage.AccessCondition;
+import com.microsoft.windowsazure.storage.StorageException;
+import com.microsoft.windowsazure.storage.blob.CloudBlob;
+
+/**
+ * An Azure blob lease that automatically renews itself indefinitely
+ * using a background thread. Use it to synchronize distributed processes,
+ * or to prevent writes to the blob by other processes that don't
+ * have the lease.
+ *
+ * Creating a new Lease object blocks the caller until the Azure blob lease is
+ * acquired.
+ *
+ * Attempting to get a lease on a non-existent blob throws StorageException.
+ *
+ * Call free() to release the Lease.
+ *
+ * You can use this Lease like a distributed lock. If the holder process
+ * dies, the lease will time out since it won't be renewed.
+ */
+public class SelfRenewingLease {
+
+  private CloudBlobWrapper blobWrapper;
+  private Thread renewer;
+  private volatile boolean leaseFreed;
+  private String leaseID = null;
+  private static final int LEASE_TIMEOUT = 60;  // Lease timeout in seconds
+
+  // Time to wait to renew lease in milliseconds
+  public static final int LEASE_RENEWAL_PERIOD = 40000;
+  private static final Log LOG = LogFactory.getLog(SelfRenewingLease.class);
+
+  // Used to allocate thread serial numbers in thread name
+  private static volatile int threadNumber = 0;
+
+
+  // Time to wait to retry getting the lease in milliseconds
+  private static final int LEASE_ACQUIRE_RETRY_INTERVAL = 2000;
+
+  public SelfRenewingLease(CloudBlobWrapper blobWrapper)
+      throws StorageException {
+
+    this.leaseFreed = false;
+    this.blobWrapper = blobWrapper;
+
+    // Keep trying to get the lease until you get it.
+    CloudBlob blob = blobWrapper.getBlob();
+    while(leaseID == null) {
+      try {
+        leaseID = blob.acquireLease(LEASE_TIMEOUT, null);
+      } catch (StorageException e) {
+
+        // Throw again if we don't want to keep waiting.
+        // We expect it to be that the lease is already present,
+        // or in some cases that the blob does not exist.
+        if (!e.getErrorCode().equals("LeaseAlreadyPresent")) {
+          LOG.info(
+            "Caught exception when trying to get lease on blob "
+            + blobWrapper.getUri().toString() + ". " + e.getMessage());
+          throw e;
+        }
+      }
+      if (leaseID == null) {
+        try {
+          Thread.sleep(LEASE_ACQUIRE_RETRY_INTERVAL);
+        } catch (InterruptedException e) {
+
+          // Restore the interrupted status
+          Thread.currentThread().interrupt();
+        }
+      }
+    }
+    renewer = new Thread(new Renewer());
+
+    // A Renewer running should not keep JVM from exiting, so make it a daemon.
+    renewer.setDaemon(true);
+    renewer.setName("AzureLeaseRenewer-" + threadNumber++);
+    renewer.start();
+    LOG.debug("Acquired lease " + leaseID + " on " + blob.getUri()
+        + " managed by thread " + renewer.getName());
+  }
+
+  /**
+   * Free the lease and stop the keep-alive thread.
+   * @throws StorageException
+   */
+  public void free() throws StorageException {
+    AccessCondition accessCondition = AccessCondition.generateEmptyCondition();
+    accessCondition.setLeaseID(leaseID);
+    try {
+      blobWrapper.getBlob().releaseLease(accessCondition);
+    } catch (StorageException e) {
+      if (e.getErrorCode().equals("BlobNotFound")) {
+
+        // Don't do anything -- it's okay to free a lease
+        // on a deleted file. The delete freed the lease
+        // implicitly.
+      } else {
+
+        // This error is not anticipated, so re-throw it.
+        LOG.warn("Unanticipated exception when trying to free lease " + leaseID
+            + " on " +  blobWrapper.getStorageUri());
+        throw(e);
+      }
+    } finally {
+
+      // Even if releasing the lease fails (e.g. because the file was deleted),
+      // make sure to record that we freed the lease, to terminate the
+      // keep-alive thread.
+      leaseFreed = true;
+      LOG.debug("Freed lease " + leaseID + " on " + blobWrapper.getUri()
+          + " managed by thread " + renewer.getName());
+    }
+  }
+
+  public boolean isFreed() {
+    return leaseFreed;
+  }
+
+  public String getLeaseID() {
+    return leaseID;
+  }
+
+  public CloudBlob getCloudBlob() {
+    return blobWrapper.getBlob();
+  }
+
+  private class Renewer implements Runnable {
+
+    /**
+     * Start a keep-alive thread that will continue to renew
+     * the lease until it is freed or the process dies.
+     */
+    @Override
+    public void run() {
+      LOG.debug("Starting lease keep-alive thread.");
+      AccessCondition accessCondition =
+          AccessCondition.generateEmptyCondition();
+      accessCondition.setLeaseID(leaseID);
+
+      while(!leaseFreed) {
+        try {
+          Thread.sleep(LEASE_RENEWAL_PERIOD);
+        } catch (InterruptedException e) {
+          LOG.debug("Keep-alive thread for lease " + leaseID +
+              " interrupted.");
+
+          // Restore the interrupted status
+          Thread.currentThread().interrupt();
+        }
+        try {
+          if (!leaseFreed) {
+            blobWrapper.getBlob().renewLease(accessCondition);
+
+            // It'll be very rare to renew the lease (most will be short)
+            // so log that we did it, to help with system debugging.
+            LOG.info("Renewed lease " + leaseID + " on "
+                + getCloudBlob().getUri());
+          }
+        } catch (StorageException e) {
+          if (!leaseFreed) {
+
+            // Free the lease so we don't leave this thread running forever.
+            leaseFreed = true;
+
+            // Normally leases should be freed and there should be no
+            // exceptions, so log a warning.
+            LOG.warn("Attempt to renew lease " + leaseID + " on "
+                + getCloudBlob().getUri()
+                + " failed, but lease not yet freed. Reason: " +
+                e.getMessage());
+          }
+        }
+      }
+    }
+  }
+}
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfThrottlingIntercept.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfThrottlingIntercept.java
index 25f2883..d18a144 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfThrottlingIntercept.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfThrottlingIntercept.java
@@ -68,12 +68,14 @@
 
   private final float readFactor;
   private final float writeFactor;
+  private final OperationContext operationContext;
 
   // Concurrency: access to non-final members must be thread-safe
   private long lastE2Elatency;
 
-  public SelfThrottlingIntercept(OperationContext operationContext,
+  public SelfThrottlingIntercept(OperationContext operationContext, 
       float readFactor, float writeFactor) {
+    this.operationContext = operationContext;
     this.readFactor = readFactor;
     this.writeFactor = writeFactor;
   }
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ShellDecryptionKeyProvider.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ShellDecryptionKeyProvider.java
index 2ce8ebd..d9d6fc3 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ShellDecryptionKeyProvider.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ShellDecryptionKeyProvider.java
@@ -31,7 +31,8 @@
  */
 @InterfaceAudience.Private
 public class ShellDecryptionKeyProvider extends SimpleKeyProvider {
-  static final String KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT = "fs.azure.shellkeyprovider.script";
+  static final String KEY_ACCOUNT_SHELLKEYPROVIDER_SCRIPT =
+      "fs.azure.shellkeyprovider.script";
 
   @Override
   public String getStorageAccountKey(String accountName, Configuration conf)
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SimpleKeyProvider.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SimpleKeyProvider.java
index ef44a85..3cd3eda 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SimpleKeyProvider.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SimpleKeyProvider.java
@@ -28,7 +28,8 @@
 @InterfaceAudience.Private
 public class SimpleKeyProvider implements KeyProvider {
 
-  protected static final String KEY_ACCOUNT_KEY_PREFIX = "fs.azure.account.key.";
+  protected static final String KEY_ACCOUNT_KEY_PREFIX =
+      "fs.azure.account.key.";
 
   @Override
   public String getStorageAccountKey(String accountName, Configuration conf)
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java
index 87cef86..8d0229d 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java
@@ -23,6 +23,7 @@
 import java.io.OutputStream;
 import java.net.URI;
 import java.net.URISyntaxException;
+import java.util.ArrayList;
 import java.util.EnumSet;
 import java.util.HashMap;
 
@@ -36,15 +37,17 @@
 import com.microsoft.windowsazure.storage.blob.BlobListingDetails;
 import com.microsoft.windowsazure.storage.blob.BlobProperties;
 import com.microsoft.windowsazure.storage.blob.BlobRequestOptions;
+import com.microsoft.windowsazure.storage.blob.CloudBlob;
 import com.microsoft.windowsazure.storage.blob.CopyState;
 import com.microsoft.windowsazure.storage.blob.ListBlobItem;
+import com.microsoft.windowsazure.storage.blob.PageRange;
 
 /**
  * This is a very thin layer over the methods exposed by the Windows Azure
  * Storage SDK that we need for WASB implementation. This base class has a real
  * implementation that just simply redirects to the SDK, and a memory-backed one
  * that's used for unit tests.
- * 
+ *
  * IMPORTANT: all the methods here must remain very simple redirects since code
  * written here can't be properly unit tested.
  */
@@ -323,23 +326,39 @@ public abstract CloudBlobDirectoryWrapper getDirectoryReference(
      * @throws URISyntaxException
      *           If URI syntax exception occurred.
      */
-    public abstract CloudBlockBlobWrapper getBlockBlobReference(
+    public abstract CloudBlobWrapper getBlockBlobReference(
         String relativePath) throws URISyntaxException, StorageException;
+  
+    /**
+     * Returns a wrapper for a CloudPageBlob.
+     *
+     * @param relativePath
+     *            A <code>String</code> that represents the name of the blob, relative to the container 
+     *
+     * @throws StorageException
+     *             If a storage service error occurred.
+     * 
+     * @throws URISyntaxException
+     *             If URI syntax exception occurred.            
+     */
+    public abstract CloudBlobWrapper getPageBlobReference(String relativePath)
+        throws URISyntaxException, StorageException;
   }
-
+  
+  
   /**
-   * A thin wrapper over the {@link CloudBlockBlob} class that simply redirects
-   * calls to the real object except in unit tests.
+   * A thin wrapper over the {@link CloudBlob} class that simply redirects calls
+   * to the real object except in unit tests.
    */
   @InterfaceAudience.Private
-  public abstract static class CloudBlockBlobWrapper implements ListBlobItem {
+  public interface CloudBlobWrapper extends ListBlobItem {
     /**
      * Returns the URI for this blob.
      * 
      * @return A <code>java.net.URI</code> object that represents the URI for
      *         the blob.
      */
-    public abstract URI getUri();
+    URI getUri();
 
     /**
      * Returns the metadata for the blob.
@@ -347,7 +366,7 @@ public abstract CloudBlockBlobWrapper getBlockBlobReference(
      * @return A <code>java.util.HashMap</code> object that represents the
      *         metadata for the blob.
      */
-    public abstract HashMap<String, String> getMetadata();
+    HashMap<String, String> getMetadata();
 
     /**
      * Sets the metadata for the blob.
@@ -356,37 +375,64 @@ public abstract CloudBlockBlobWrapper getBlockBlobReference(
      *          A <code>java.util.HashMap</code> object that contains the
      *          metadata being assigned to the blob.
      */
-    public abstract void setMetadata(HashMap<String, String> metadata);
+    void setMetadata(HashMap<String, String> metadata);
 
     /**
-     * Copies an existing blob's contents, properties, and metadata to this
-     * instance of the <code>CloudBlob</code> class, using the specified
-     * operation context.
-     * 
-     * @param sourceBlob
-     *          A <code>CloudBlob</code> object that represents the source blob
-     *          to copy.
+     * Copies an existing blob's contents, properties, and metadata to this instance of the <code>CloudBlob</code>
+     * class, using the specified operation context.
+     *
+     * @param source
+     *            A <code>java.net.URI</code> The URI of a source blob.
      * @param opContext
-     *          An {@link OperationContext} object that represents the context
-     *          for the current operation. This object is used to track requests
-     *          to the storage service, and to provide additional runtime
-     *          information about the operation.
-     * 
+     *            An {@link OperationContext} object that represents the context for the current operation. This object
+     *            is used to track requests to the storage service, and to provide additional runtime information about
+     *            the operation.
+     *
      * @throws StorageException
-     *           If a storage service error occurred.
+     *             If a storage service error occurred.
      * @throws URISyntaxException
-     * 
+     *
      */
-    public abstract void startCopyFromBlob(CloudBlockBlobWrapper sourceBlob,
-        OperationContext opContext) throws StorageException, URISyntaxException;
-
+    public abstract void startCopyFromBlob(URI source,
+        OperationContext opContext)
+        throws StorageException, URISyntaxException;
+    
     /**
      * Returns the blob's copy state.
      * 
      * @return A {@link CopyState} object that represents the copy state of the
      *         blob.
      */
-    public abstract CopyState getCopyState();
+    CopyState getCopyState();
+
+    /**
+     * Downloads a range of bytes from the blob to the given byte buffer, using the specified request options and
+     * operation context.
+     *
+     * @param offset
+     *            The byte offset to use as the starting point for the source.
+     * @param length
+     *            The number of bytes to read.
+     * @param buffer
+     *            The byte buffer, as an array of bytes, to which the blob bytes are downloaded.
+     * @param bufferOffset
+     *            The byte offset to use as the starting point for the target.
+     * @param options
+     *            A {@link BlobRequestOptions} object that specifies any additional options for the request. Specifying
+     *            <code>null</code> will use the default request options from the associated service client (
+     *            {@link CloudBlobClient}).
+     * @param opContext
+     *            An {@link OperationContext} object that represents the context for the current operation. This object
+     *            is used to track requests to the storage service, and to provide additional runtime information about
+     *            the operation.
+     *
+     * @throws StorageException
+     *             If a storage service error occurred.
+     */
+    void downloadRange(final long offset, final long length,
+        final OutputStream outStream, final BlobRequestOptions options,
+        final OperationContext opContext)
+            throws StorageException, IOException;
 
     /**
      * Deletes the blob using the specified operation context.
@@ -407,7 +453,7 @@ public abstract void startCopyFromBlob(CloudBlockBlobWrapper sourceBlob,
      * @throws StorageException
      *           If a storage service error occurred.
      */
-    public abstract void delete(OperationContext opContext)
+    void delete(OperationContext opContext, SelfRenewingLease lease)
         throws StorageException;
 
     /**
@@ -419,13 +465,13 @@ public abstract void delete(OperationContext opContext)
      *          to the storage service, and to provide additional runtime
      *          information about the operation.
      * 
-     * @return <code>true</code> if the blob exists, other wise
+     * @return <code>true</code> if the blob exists, otherwise
      *         <code>false</code>.
      * 
      * @throws StorageException
-     *           f a storage service error occurred.
+     *           If a storage service error occurred.
      */
-    public abstract boolean exists(OperationContext opContext)
+    boolean exists(OperationContext opContext)
         throws StorageException;
 
     /**
@@ -446,7 +492,7 @@ public abstract boolean exists(OperationContext opContext)
      * @throws StorageException
      *           If a storage service error occurred.
      */
-    public abstract void downloadAttributes(OperationContext opContext)
+    void downloadAttributes(OperationContext opContext)
         throws StorageException;
 
     /**
@@ -455,7 +501,7 @@ public abstract void downloadAttributes(OperationContext opContext)
      * @return A {@link BlobProperties} object that represents the properties of
      *         the blob.
      */
-    public abstract BlobProperties getProperties();
+    BlobProperties getProperties();
 
     /**
      * Opens a blob input stream to download the blob using the specified
@@ -476,49 +522,10 @@ public abstract void downloadAttributes(OperationContext opContext)
      * @throws StorageException
      *           If a storage service error occurred.
      */
-    public abstract InputStream openInputStream(BlobRequestOptions options,
+    InputStream openInputStream(BlobRequestOptions options,
         OperationContext opContext) throws StorageException;
 
     /**
-     * Creates and opens an output stream to write data to the block blob using
-     * the specified operation context.
-     * 
-     * @param opContext
-     *          An {@link OperationContext} object that represents the context
-     *          for the current operation. This object is used to track requests
-     *          to the storage service, and to provide additional runtime
-     *          information about the operation.
-     * 
-     * @return A {@link BlobOutputStream} object used to write data to the blob.
-     * 
-     * @throws StorageException
-     *           If a storage service error occurred.
-     */
-    public abstract OutputStream openOutputStream(BlobRequestOptions options,
-        OperationContext opContext) throws StorageException;
-
-    /**
-     * Uploads the source stream data to the blob, using the specified operation
-     * context.
-     * 
-     * @param sourceStream
-     *          An <code>InputStream</code> object that represents the input
-     *          stream to write to the block blob.
-     * @param opContext
-     *          An {@link OperationContext} object that represents the context
-     *          for the current operation. This object is used to track requests
-     *          to the storage service, and to provide additional runtime
-     *          information about the operation.
-     * 
-     * @throws IOException
-     *           If an I/O error occurred.
-     * @throws StorageException
-     *           If a storage service error occurred.
-     */
-    public abstract void upload(InputStream sourceStream,
-        OperationContext opContext) throws StorageException, IOException;
-
-    /**
      * Uploads the blob's metadata to the storage service using the specified
      * lease ID, request options, and operation context.
      * 
@@ -531,12 +538,15 @@ public abstract void upload(InputStream sourceStream,
      * @throws StorageException
      *           If a storage service error occurred.
      */
-    public abstract void uploadMetadata(OperationContext opContext)
+    void uploadMetadata(OperationContext opContext)
         throws StorageException;
 
-    public abstract void uploadProperties(OperationContext opContext)
+    void uploadProperties(OperationContext opContext,
+        SelfRenewingLease lease)
         throws StorageException;
 
+    SelfRenewingLease acquireLease() throws StorageException;
+    
     /**
      * Sets the minimum read block size to use with this Blob.
      * 
@@ -545,7 +555,7 @@ public abstract void uploadProperties(OperationContext opContext)
      *          while using a {@link BlobInputStream} object, ranging from 512
      *          bytes to 64 MB, inclusive.
      */
-    public abstract void setStreamMinimumReadSizeInBytes(
+    void setStreamMinimumReadSizeInBytes(
         int minimumReadSizeBytes);
 
     /**
@@ -560,7 +570,121 @@ public abstract void setStreamMinimumReadSizeInBytes(
      *           If <code>writeBlockSizeInBytes</code> is less than 1 MB or
      *           greater than 4 MB.
      */
-    public abstract void setWriteBlockSizeInBytes(int writeBlockSizeBytes);
+    void setWriteBlockSizeInBytes(int writeBlockSizeBytes);
+
+    CloudBlob getBlob();
+  }
+
+  /**
+   * A thin wrapper over the {@link CloudBlockBlob} class that simply redirects calls
+   * to the real object except in unit tests.
+   */
+  public abstract interface CloudBlockBlobWrapper
+      extends CloudBlobWrapper {
+    /**
+     * Creates and opens an output stream to write data to the block blob using the specified 
+     * operation context.
+     * 
+     * @param opContext
+     *            An {@link OperationContext} object that represents the context for the current operation. This object
+     *            is used to track requests to the storage service, and to provide additional runtime information about
+     *            the operation.
+     * 
+     * @return A {@link BlobOutputStream} object used to write data to the blob.
+     * 
+     * @throws StorageException
+     *             If a storage service error occurred.
+     */
+    OutputStream openOutputStream(
+        BlobRequestOptions options,
+        OperationContext opContext) throws StorageException;
+  }
+
+  /**
+   * A thin wrapper over the {@link CloudPageBlob} class that simply redirects calls
+   * to the real object except in unit tests.
+   */
+  public abstract interface CloudPageBlobWrapper
+      extends CloudBlobWrapper {
+    /**
+     * Creates a page blob using the specified request options and operation context.
+     *
+     * @param length
+     *            The size, in bytes, of the page blob.
+     * @param options
+     *            A {@link BlobRequestOptions} object that specifies any additional options for the request. Specifying
+     *            <code>null</code> will use the default request options from the associated service client (
+     *            {@link CloudBlobClient}).
+     * @param opContext
+     *            An {@link OperationContext} object that represents the context for the current operation. This object
+     *            is used to track requests to the storage service, and to provide additional runtime information about
+     *            the operation.
+     *
+     * @throws IllegalArgumentException
+     *             If the length is not a multiple of 512.
+     *
+     * @throws StorageException
+     *             If a storage service error occurred.
+     */
+    void create(final long length, BlobRequestOptions options,
+            OperationContext opContext) throws StorageException;
+    
+
+    /**
+     * Uploads a range of contiguous pages, up to 4 MB in size, at the specified offset in the page blob, using the
+     * specified lease ID, request options, and operation context.
+     * 
+     * @param sourceStream
+     *            An <code>InputStream</code> object that represents the input stream to write to the page blob.
+     * @param offset
+     *            The offset, in number of bytes, at which to begin writing the data. This value must be a multiple of
+     *            512.
+     * @param length
+     *            The length, in bytes, of the data to write. This value must be a multiple of 512.
+     * @param options
+     *            A {@link BlobRequestOptions} object that specifies any additional options for the request. Specifying
+     *            <code>null</code> will use the default request options from the associated service client (
+     *            {@link CloudBlobClient}).
+     * @param opContext
+     *            An {@link OperationContext} object that represents the context for the current operation. This object
+     *            is used to track requests to the storage service, and to provide additional runtime information about
+     *            the operation.
+     * 
+     * @throws IllegalArgumentException
+     *             If the offset or length are not multiples of 512, or if the length is greater than 4 MB.
+     * @throws IOException
+     *             If an I/O exception occurred.
+     * @throws StorageException
+     *             If a storage service error occurred.
+     */
+    void uploadPages(final InputStream sourceStream, final long offset,
+        final long length, BlobRequestOptions options,
+        OperationContext opContext) throws StorageException, IOException;
+
+    /**
+     * Returns a collection of page ranges and their starting and ending byte offsets using the specified request
+     * options and operation context.
+     *
+     * @param options
+     *            A {@link BlobRequestOptions} object that specifies any additional options for the request. Specifying
+     *            <code>null</code> will use the default request options from the associated service client (
+     *            {@link CloudBlobClient}).
+     * @param opContext
+     *            An {@link OperationContext} object that represents the context for the current operation. This object
+     *            is used to track requests to the storage service, and to provide additional runtime information about
+     *            the operation.
+     *
+     * @return An <code>ArrayList</code> object that represents the set of page ranges and their starting and ending
+     *         byte offsets.
+     *
+     * @throws StorageException
+     *             If a storage service error occurred.
+     */
 
+    ArrayList<PageRange> downloadPageRanges(BlobRequestOptions options,
+            OperationContext opContext) throws StorageException;
+    
+    void uploadMetadata(OperationContext opContext)
+        throws StorageException; 
   }
-}
+}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java
index 935bf71..e44823c 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java
@@ -23,6 +23,7 @@
 import java.io.OutputStream;
 import java.net.URI;
 import java.net.URISyntaxException;
+import java.util.ArrayList;
 import java.util.EnumSet;
 import java.util.HashMap;
 import java.util.Iterator;
@@ -39,13 +40,16 @@
 import com.microsoft.windowsazure.storage.blob.BlobListingDetails;
 import com.microsoft.windowsazure.storage.blob.BlobProperties;
 import com.microsoft.windowsazure.storage.blob.BlobRequestOptions;
+import com.microsoft.windowsazure.storage.blob.CloudBlob;
 import com.microsoft.windowsazure.storage.blob.CloudBlobClient;
 import com.microsoft.windowsazure.storage.blob.CloudBlobContainer;
 import com.microsoft.windowsazure.storage.blob.CloudBlobDirectory;
 import com.microsoft.windowsazure.storage.blob.CloudBlockBlob;
+import com.microsoft.windowsazure.storage.blob.CloudPageBlob;
 import com.microsoft.windowsazure.storage.blob.CopyState;
 import com.microsoft.windowsazure.storage.blob.DeleteSnapshotsOption;
 import com.microsoft.windowsazure.storage.blob.ListBlobItem;
+import com.microsoft.windowsazure.storage.blob.PageRange;
 
 /**
  * A real implementation of the Azure interaction layer that just redirects
@@ -129,6 +133,8 @@ public ListBlobItem next() {
         return new CloudBlobDirectoryWrapperImpl((CloudBlobDirectory) unwrapped);
       } else if (unwrapped instanceof CloudBlockBlob) {
         return new CloudBlockBlobWrapperImpl((CloudBlockBlob) unwrapped);
+      } else if (unwrapped instanceof CloudPageBlob) {
+        return new CloudPageBlobWrapperImpl((CloudPageBlob) unwrapped);
       } else {
         return unwrapped;
       }
@@ -244,129 +250,217 @@ public CloudBlobDirectoryWrapper getDirectoryReference(String relativePath)
     }
 
     @Override
-    public CloudBlockBlobWrapper getBlockBlobReference(String relativePath)
+    public CloudBlobWrapper getBlockBlobReference(String relativePath)
         throws URISyntaxException, StorageException {
 
-      return new CloudBlockBlobWrapperImpl(
-          container.getBlockBlobReference(relativePath));
+      return new CloudBlockBlobWrapperImpl(container.getBlockBlobReference(relativePath));
     }
+    
+    @Override
+    public CloudBlobWrapper getPageBlobReference(String relativePath)
+        throws URISyntaxException, StorageException {
+      return new CloudPageBlobWrapperImpl(
+          container.getPageBlobReference(relativePath));
+    }
+
   }
+  
+  abstract static class CloudBlobWrapperImpl implements CloudBlobWrapper {
+    private final CloudBlob blob;
 
-  //
-  // CloudBlockBlobWrapperImpl
-  //
-  @InterfaceAudience.Private
-  static class CloudBlockBlobWrapperImpl extends CloudBlockBlobWrapper {
-    private final CloudBlockBlob blob;
+    @Override
+    public CloudBlob getBlob() {
+      return blob;
+    }
 
     public URI getUri() {
-      return blob.getUri();
+      return getBlob().getUri();
     }
 
-    public CloudBlockBlobWrapperImpl(CloudBlockBlob blob) {
+    protected CloudBlobWrapperImpl(CloudBlob blob) {
       this.blob = blob;
     }
 
     @Override
     public HashMap<String, String> getMetadata() {
-      return blob.getMetadata();
+      return getBlob().getMetadata();
     }
 
     @Override
-    public void startCopyFromBlob(CloudBlockBlobWrapper sourceBlob,
-        OperationContext opContext) throws StorageException, URISyntaxException {
-
-      blob.startCopyFromBlob(((CloudBlockBlobWrapperImpl) sourceBlob).blob,
-          null, null, null, opContext);
-
+    public void delete(OperationContext opContext, SelfRenewingLease lease)
+        throws StorageException {
+      getBlob().delete(DeleteSnapshotsOption.NONE, getLeaseCondition(lease),
+          null, opContext);
     }
 
-    @Override
-    public void delete(OperationContext opContext) throws StorageException {
-      blob.delete(DeleteSnapshotsOption.NONE, null, null, opContext);
+    /**
+     * Return and access condition for this lease, or else null if
+     * there's no lease.
+     */
+    private AccessCondition getLeaseCondition(SelfRenewingLease lease) {
+      AccessCondition leaseCondition = null;
+      if (lease != null) {
+        leaseCondition = AccessCondition.generateLeaseCondition(lease.getLeaseID());
+      }
+      return leaseCondition;
     }
 
     @Override
-    public boolean exists(OperationContext opContext) throws StorageException {
-      return blob.exists(null, null, opContext);
+    public boolean exists(OperationContext opContext)
+        throws StorageException {
+      return getBlob().exists(null, null, opContext);
     }
 
     @Override
-    public void downloadAttributes(OperationContext opContext)
-        throws StorageException {
-      blob.downloadAttributes(null, null, opContext);
+    public void downloadAttributes(
+        OperationContext opContext) throws StorageException {
+      getBlob().downloadAttributes(null, null, opContext);
     }
 
     @Override
     public BlobProperties getProperties() {
-      return blob.getProperties();
+      return getBlob().getProperties();
     }
 
     @Override
     public void setMetadata(HashMap<String, String> metadata) {
-      blob.setMetadata(metadata);
+      getBlob().setMetadata(metadata);
     }
 
     @Override
-    public InputStream openInputStream(BlobRequestOptions options,
+    public InputStream openInputStream(
+        BlobRequestOptions options,
         OperationContext opContext) throws StorageException {
-      return blob.openInputStream(null, options, opContext);
+      return getBlob().openInputStream(null, options, opContext);
     }
 
-    @Override
-    public OutputStream openOutputStream(BlobRequestOptions options,
+    public OutputStream openOutputStream(
+        BlobRequestOptions options,
         OperationContext opContext) throws StorageException {
-      return blob.openOutputStream(null, options, opContext);
+      return ((CloudBlockBlob) getBlob()).openOutputStream(null, options, opContext);
     }
 
-    @Override
     public void upload(InputStream sourceStream, OperationContext opContext)
         throws StorageException, IOException {
-      blob.upload(sourceStream, 0, null, null, opContext);
+      getBlob().upload(sourceStream, 0, null, null, opContext);
     }
 
     @Override
     public CloudBlobContainer getContainer() throws URISyntaxException,
         StorageException {
-      return blob.getContainer();
+      return getBlob().getContainer();
     }
 
     @Override
     public CloudBlobDirectory getParent() throws URISyntaxException,
         StorageException {
-      return blob.getParent();
+      return getBlob().getParent();
     }
 
     @Override
     public void uploadMetadata(OperationContext opContext)
         throws StorageException {
-      blob.uploadMetadata(null, null, opContext);
+      getBlob().uploadMetadata(null, null, opContext);
     }
 
-    @Override
-    public void uploadProperties(OperationContext opContext)
+    public void uploadProperties(OperationContext opContext, SelfRenewingLease lease)
         throws StorageException {
-      blob.uploadProperties(null, null, opContext);
+
+      // Include lease in request if lease not null.
+      getBlob().uploadProperties(getLeaseCondition(lease), null, opContext);
     }
 
     @Override
     public void setStreamMinimumReadSizeInBytes(int minimumReadSizeBytes) {
-      blob.setStreamMinimumReadSizeInBytes(minimumReadSizeBytes);
+      getBlob().setStreamMinimumReadSizeInBytes(minimumReadSizeBytes);
     }
 
     @Override
     public void setWriteBlockSizeInBytes(int writeBlockSizeBytes) {
-      blob.setStreamWriteSizeInBytes(writeBlockSizeBytes);
+      getBlob().setStreamWriteSizeInBytes(writeBlockSizeBytes);
     }
 
     @Override
     public StorageUri getStorageUri() {
-      return blob.getStorageUri();
+      return getBlob().getStorageUri();
     }
 
     @Override
     public CopyState getCopyState() {
-      return blob.getCopyState();
+      return getBlob().getCopyState();
+    }
+
+    @Override
+    public void startCopyFromBlob(URI source,
+        OperationContext opContext)
+            throws StorageException, URISyntaxException {
+      getBlob().startCopyFromBlob(source,
+          null, null, null, opContext);
+    }
+
+    @Override
+    public void downloadRange(long offset, long length, OutputStream outStream,
+        BlobRequestOptions options, OperationContext opContext)
+            throws StorageException, IOException {
+
+      getBlob().downloadRange(offset, length, outStream, null, options, opContext);
+    }
+
+    @Override
+    public SelfRenewingLease acquireLease() throws StorageException {
+      return new SelfRenewingLease(this);
+    }
+  }
+  
+
+  //
+  // CloudBlockBlobWrapperImpl
+  //
+
+  static class CloudBlockBlobWrapperImpl extends CloudBlobWrapperImpl implements CloudBlockBlobWrapper {
+    public CloudBlockBlobWrapperImpl(CloudBlockBlob blob) {
+      super(blob);
+    }
+
+    public OutputStream openOutputStream(
+        BlobRequestOptions options,
+        OperationContext opContext) throws StorageException {
+      return ((CloudBlockBlob) getBlob()).openOutputStream(null, options, opContext);
+    }
+
+    public void upload(InputStream sourceStream, OperationContext opContext)
+        throws StorageException, IOException {
+      getBlob().upload(sourceStream, 0, null, null, opContext);
+    }
+
+    public void uploadProperties(OperationContext opContext)
+        throws StorageException {
+      getBlob().uploadProperties(null, null, opContext);
+    }
+
+  }
+
+  static class CloudPageBlobWrapperImpl extends CloudBlobWrapperImpl implements CloudPageBlobWrapper {
+    public CloudPageBlobWrapperImpl(CloudPageBlob blob) {
+      super(blob);
+    }
+
+    public void create(final long length, BlobRequestOptions options,
+        OperationContext opContext) throws StorageException {
+      ((CloudPageBlob) getBlob()).create(length, null, options, opContext);
+    }
+
+    public void uploadPages(final InputStream sourceStream, final long offset,
+        final long length, BlobRequestOptions options, OperationContext opContext)
+        throws StorageException, IOException {
+      ((CloudPageBlob) getBlob()).uploadPages(sourceStream, offset, length, null,
+          options, opContext);
+    }
+
+    public ArrayList<PageRange> downloadPageRanges(BlobRequestOptions options,
+        OperationContext opContext) throws StorageException {
+      return ((CloudPageBlob) getBlob()).downloadPageRanges(
+          null, options, opContext);
     }
   }
 }
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SyncableDataOutputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SyncableDataOutputStream.java
new file mode 100644
index 0000000..58ac326
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SyncableDataOutputStream.java
@@ -0,0 +1,62 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs.azure;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+
+import org.apache.hadoop.fs.Syncable;
+
+/**
+ * Support the Syncable interface on top of a DataOutputStream.
+ * This allows passing the sync/hflush/hsync calls through to the
+ * wrapped stream passed in to the constructor. This is required
+ * for HBase when wrapping a PageBlobOutputStream used as a write-ahead log.
+ */
+public class SyncableDataOutputStream extends DataOutputStream implements Syncable {
+
+  public SyncableDataOutputStream(OutputStream out) {
+    super(out);
+  }
+
+  @Override
+  @Deprecated
+  public void sync() throws IOException {
+    hflush();
+  }
+
+  @Override
+  public void hflush() throws IOException {
+    if (out instanceof Syncable) {
+      ((Syncable) out).hflush();
+    } else {
+      out.flush();
+    }
+  }
+
+  @Override
+  public void hsync() throws IOException {
+    if (out instanceof Syncable) {
+      ((Syncable) out).hsync();
+    } else {
+      out.flush();
+    }
+  }
+}
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/Wasb.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/Wasb.java
index e098cef..dd354d7 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/Wasb.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/Wasb.java
@@ -27,7 +27,6 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.DelegateToFileSystem;
 
-
 /**
  * WASB implementation of AbstractFileSystem.
  * This impl delegates to the old FileSystem
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/AzureFileSystemInstrumentation.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/AzureFileSystemInstrumentation.java
index e389d7c..a08ad71 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/AzureFileSystemInstrumentation.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/AzureFileSystemInstrumentation.java
@@ -41,11 +41,11 @@
 @InterfaceAudience.Public
 @InterfaceStability.Evolving
 public final class AzureFileSystemInstrumentation implements MetricsSource {
-  
+
   public static final String METRIC_TAG_FILESYSTEM_ID = "wasbFileSystemId";
   public static final String METRIC_TAG_ACCOUNT_NAME = "accountName";
   public static final String METRIC_TAG_CONTAINTER_NAME = "containerName";
-  
+
   public static final String WASB_WEB_RESPONSES = "wasb_web_responses";
   public static final String WASB_BYTES_WRITTEN =
       "wasb_bytes_written_last_second";
@@ -381,7 +381,6 @@ public long getCurrentMaximumUploadBandwidth() {
    */
   public long getCurrentMaximumDownloadBandwidth() {
     return currentMaximumDownloadBytesPerSecond;
-
   }
 
   @Override
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/ResponseReceivedMetricUpdater.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/ResponseReceivedMetricUpdater.java
index e3f5d44..676adb9 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/ResponseReceivedMetricUpdater.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/metrics/ResponseReceivedMetricUpdater.java
@@ -33,8 +33,7 @@
 
 /**
  * An event listener to the ResponseReceived event from Azure Storage that will
- * update metrics appropriately.
- *
+ * update metrics appropriately when it gets that event.
  */
 @InterfaceAudience.Private
 public final class ResponseReceivedMetricUpdater extends StorageEvent<ResponseReceivedEvent> {
@@ -43,7 +42,7 @@
 
   private final AzureFileSystemInstrumentation instrumentation;
   private final BandwidthGaugeUpdater blockUploadGaugeUpdater;
-  
+
   private ResponseReceivedMetricUpdater(OperationContext operationContext,
       AzureFileSystemInstrumentation instrumentation,
       BandwidthGaugeUpdater blockUploadGaugeUpdater) {
@@ -142,6 +141,6 @@ public void eventOccurred(ResponseReceivedEvent eventArg) {
         instrumentation.rawBytesDownloaded(length);
         instrumentation.blockDownloaded(requestLatency);
       }
-    } 
+    }
   }
 }
diff --git a/hadoop-tools/hadoop-azure/src/main/resources/META-INF/services/org.apache.hadoop.fs.FileSystem b/hadoop-tools/hadoop-azure/src/main/resources/META-INF/services/org.apache.hadoop.fs.FileSystem
new file mode 100644
index 0000000..9f4922b
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/main/resources/META-INF/services/org.apache.hadoop.fs.FileSystem
@@ -0,0 +1,17 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+org.apache.hadoop.fs.azure.NativeAzureFileSystem
+org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/AzureBlobStorageTestAccount.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/AzureBlobStorageTestAccount.java
index 80e8e43..a323237 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/AzureBlobStorageTestAccount.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/AzureBlobStorageTestAccount.java
@@ -78,6 +78,8 @@
 
   private static final String KEY_DISABLE_THROTTLING = "fs.azure.disable.bandwidth.throttling";
   private static final String KEY_READ_TOLERATE_CONCURRENT_APPEND = "fs.azure.io.read.tolerate.concurrent.append";
+  public static final String DEFAULT_PAGE_BLOB_DIRECTORY = "pageBlobs";
+  public static final String DEFAULT_ATOMIC_RENAME_DIRECTORIES = "/atomicRenameDir1,/atomicRenameDir2";
 
   private CloudStorageAccount account;
   private CloudBlobContainer container;
@@ -85,12 +87,14 @@
   private NativeAzureFileSystem fs;
   private AzureNativeFileSystemStore storage;
   private MockStorageInterface mockStorage;
+  private String pageBlobDirectory;
   private static final ConcurrentLinkedQueue<MetricsRecord> allMetrics =
       new ConcurrentLinkedQueue<MetricsRecord>();
-  
+  private static boolean metricsConfigSaved = false;
   
   private AzureBlobStorageTestAccount(NativeAzureFileSystem fs,
-      CloudStorageAccount account, CloudBlobContainer container) {
+      CloudStorageAccount account,
+      CloudBlobContainer container) {
     this.account = account;
     this.container = container;
     this.fs = fs;
@@ -158,6 +162,14 @@ public static String toMockUri(Path path) {
     return toMockUri(path.toUri().getRawPath().substring(1)); 
   }
   
+  public static Path pageBlobPath() {
+    return new Path("/" + DEFAULT_PAGE_BLOB_DIRECTORY);
+  }
+
+  public static Path pageBlobPath(String fileName) {
+    return new Path(pageBlobPath(), fileName);
+  }
+
   public Number getLatestMetricValue(String metricName, Number defaultValue)
       throws IndexOutOfBoundsException{
     boolean found = false;
@@ -206,8 +218,10 @@ private boolean wasGeneratedByMe(MetricsRecord currentRecord) {
    *          The blob key (no initial slash).
    * @return The blob reference.
    */
-  public CloudBlockBlob getBlobReference(String blobKey) throws Exception {
-    return container.getBlockBlobReference(String.format(blobKey));
+  public CloudBlockBlob getBlobReference(String blobKey)
+      throws Exception {
+    return container.getBlockBlobReference(
+        String.format(blobKey));
   }
 
   /**
@@ -233,45 +247,79 @@ public void releaseLease(String leaseID, String blobKey) throws Exception {
     getBlobReference(blobKey).releaseLease(accessCondition);
   }
 
+  private static void saveMetricsConfigFile() {
+    if (!metricsConfigSaved) {
+      new org.apache.hadoop.metrics2.impl.ConfigBuilder()
+      .add("azure-file-system.sink.azuretestcollector.class",
+          StandardCollector.class.getName())
+      .save("hadoop-metrics2-azure-file-system.properties");
+      metricsConfigSaved = true;
+    }
+  }
+
   public static AzureBlobStorageTestAccount createMock() throws Exception {
     return createMock(new Configuration());
   }
 
-  public static AzureBlobStorageTestAccount createMock(Configuration conf)
-      throws Exception {
+  public static AzureBlobStorageTestAccount createMock(Configuration conf) throws Exception {
+    saveMetricsConfigFile();
+    configurePageBlobDir(conf);
+    configureAtomicRenameDir(conf);
     AzureNativeFileSystemStore store = new AzureNativeFileSystemStore();
     MockStorageInterface mockStorage = new MockStorageInterface();
     store.setAzureStorageInteractionLayer(mockStorage);
     NativeAzureFileSystem fs = new NativeAzureFileSystem(store);
-    addWasbToConfiguration(conf);
     setMockAccountKey(conf);
     // register the fs provider.
 
     fs.initialize(new URI(MOCK_WASB_URI), conf);
-    AzureBlobStorageTestAccount testAcct = new AzureBlobStorageTestAccount(fs,
-        mockStorage);
+    AzureBlobStorageTestAccount testAcct =
+        new AzureBlobStorageTestAccount(fs, mockStorage);
     return testAcct;
   }
 
   /**
+   * Set the page blob directories configuration to the default if it is not
+   * already set. Some tests may set it differently (e.g. the page blob
+   * tests in TestNativeAzureFSPageBlobLive).
+   * @param conf The configuration to conditionally update.
+   */
+  private static void configurePageBlobDir(Configuration conf) {
+    if (conf.get(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES) == null) {
+      conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES,
+          "/" + DEFAULT_PAGE_BLOB_DIRECTORY);
+    }
+  }
+
+  /** Do the same for the atomic rename directories configuration */
+  private static void configureAtomicRenameDir(Configuration conf) {
+    if (conf.get(AzureNativeFileSystemStore.KEY_ATOMIC_RENAME_DIRECTORIES) == null) {
+      conf.set(AzureNativeFileSystemStore.KEY_ATOMIC_RENAME_DIRECTORIES,
+          DEFAULT_ATOMIC_RENAME_DIRECTORIES);
+    }
+  }
+
+  /**
    * Creates a test account that goes against the storage emulator.
    * 
    * @return The test account, or null if the emulator isn't setup.
    */
   public static AzureBlobStorageTestAccount createForEmulator()
       throws Exception {
+    saveMetricsConfigFile();
     NativeAzureFileSystem fs = null;
     CloudBlobContainer container = null;
     Configuration conf = createTestConfiguration();
     if (!conf.getBoolean(USE_EMULATOR_PROPERTY_NAME, false)) {
       // Not configured to test against the storage emulator.
-      System.out.println("Skipping emulator Azure test because configuration "
-          + "doesn't indicate that it's running."
-          + " Please see README.txt for guidance.");
+      System.out
+        .println("Skipping emulator Azure test because configuration " +
+            "doesn't indicate that it's running." +
+            " Please see RunningLiveWasbTests.txt for guidance.");
       return null;
     }
-    CloudStorageAccount account = CloudStorageAccount
-        .getDevelopmentStorageAccount();
+    CloudStorageAccount account =
+        CloudStorageAccount.getDevelopmentStorageAccount();
     fs = new NativeAzureFileSystem();
     String containerName = String.format("wasbtests-%s-%tQ",
         System.getProperty("user.name"), new Date());
@@ -285,14 +333,18 @@ public static AzureBlobStorageTestAccount createForEmulator()
     fs.initialize(accountUri, conf);
 
     // Create test account initializing the appropriate member variables.
-    AzureBlobStorageTestAccount testAcct = new AzureBlobStorageTestAccount(fs,
-        account, container);
+    //
+    AzureBlobStorageTestAccount testAcct =
+        new AzureBlobStorageTestAccount(fs, account, container);
 
     return testAcct;
   }
 
   public static AzureBlobStorageTestAccount createOutOfBandStore(
       int uploadBlockSize, int downloadBlockSize) throws Exception {
+
+    saveMetricsConfigFile();
+
     CloudBlobContainer container = null;
     Configuration conf = createTestConfiguration();
     CloudStorageAccount account = createTestAccount(conf);
@@ -337,8 +389,9 @@ public static AzureBlobStorageTestAccount createOutOfBandStore(
     testStorage.initialize(accountUri, conf, instrumentation);
 
     // Create test account initializing the appropriate member variables.
-    AzureBlobStorageTestAccount testAcct = new AzureBlobStorageTestAccount(
-        testStorage, account, container);
+    //
+    AzureBlobStorageTestAccount testAcct =
+        new AzureBlobStorageTestAccount(testStorage, account, container);
 
     return testAcct;
   }
@@ -416,11 +469,11 @@ static CloudStorageAccount createStorageAccount(String accountName,
     }
   }
 
-  private static Configuration createTestConfiguration() {
+  public static Configuration createTestConfiguration() {
     return createTestConfiguration(null);
   }
 
-  protected static Configuration createTestConfiguration(Configuration conf) {
+  private static Configuration createTestConfiguration(Configuration conf) {
     if (conf == null) {
       conf = new Configuration();
     }
@@ -429,16 +482,9 @@ protected static Configuration createTestConfiguration(Configuration conf) {
     return conf;
   }
 
-  // for programmatic setting of the wasb configuration.
-  // note that tests can also get the
-  public static void addWasbToConfiguration(Configuration conf) {
-    conf.set("fs.wasb.impl", "org.apache.hadoop.fs.azure.NativeAzureFileSystem");
-    conf.set("fs.wasbs.impl",
-        "org.apache.hadoop.fs.azure.NativeAzureFileSystem");
-  }
-
-  static CloudStorageAccount createTestAccount() throws URISyntaxException,
-      KeyProviderException {
+  static CloudStorageAccount createTestAccount()
+      throws URISyntaxException, KeyProviderException
+  {
     return createTestAccount(createTestConfiguration());
   }
 
@@ -447,8 +493,8 @@ static CloudStorageAccount createTestAccount(Configuration conf)
     String testAccountName = conf.get(TEST_ACCOUNT_NAME_PROPERTY_NAME);
     if (testAccountName == null) {
       System.out
-          .println("Skipping live Azure test because of missing test account."
-              + " Please see README.txt for guidance.");
+        .println("Skipping live Azure test because of missing test account." +
+                 " Please see RunningLiveWasbTests.txt for guidance.");
       return null;
     }
     return createStorageAccount(testAccountName, conf, false);
@@ -466,9 +512,12 @@ public static AzureBlobStorageTestAccount create(String containerNameSuffix,
   public static AzureBlobStorageTestAccount create(String containerNameSuffix,
       EnumSet<CreateOptions> createOptions, Configuration initialConfiguration)
       throws Exception {
+    saveMetricsConfigFile();
     NativeAzureFileSystem fs = null;
     CloudBlobContainer container = null;
     Configuration conf = createTestConfiguration(initialConfiguration);
+    configurePageBlobDir(conf);
+    configureAtomicRenameDir(conf);
     CloudStorageAccount account = createTestAccount(conf);
     if (account == null) {
       return null;
@@ -510,15 +559,18 @@ public static AzureBlobStorageTestAccount create(String containerNameSuffix,
     fs.initialize(accountUri, conf);
 
     // Create test account initializing the appropriate member variables.
-    AzureBlobStorageTestAccount testAcct = new AzureBlobStorageTestAccount(fs,
-        account, container);
+    //
+    AzureBlobStorageTestAccount testAcct =
+        new AzureBlobStorageTestAccount(fs, account, container);
 
     return testAcct;
   }
 
   private static String generateContainerName() throws Exception {
-    String containerName = String.format("wasbtests-%s-%tQ",
-        System.getProperty("user.name"), new Date());
+    String containerName =
+        String.format ("wasbtests-%s-%tQ",
+            System.getProperty("user.name"),
+            new Date());
     return containerName;
   }
 
@@ -548,12 +600,16 @@ private static String generateSAS(CloudBlobContainer container,
 
     if (readonly) {
       // Set READ permissions
-      sasPolicy.setPermissions(EnumSet.of(SharedAccessBlobPermissions.READ,
+      sasPolicy.setPermissions(EnumSet.of(
+          SharedAccessBlobPermissions.READ,
           SharedAccessBlobPermissions.LIST));
     } else {
       // Set READ and WRITE permissions.
-      sasPolicy.setPermissions(EnumSet.of(SharedAccessBlobPermissions.READ,
-          SharedAccessBlobPermissions.WRITE, SharedAccessBlobPermissions.LIST));
+      //
+      sasPolicy.setPermissions(EnumSet.of(
+          SharedAccessBlobPermissions.READ,
+          SharedAccessBlobPermissions.WRITE,
+          SharedAccessBlobPermissions.LIST));
     }
 
     // Create the container permissions.
@@ -590,8 +646,11 @@ public static void primePublicContainer(CloudBlobClient blobClient,
     SharedAccessBlobPolicy sasPolicy = new SharedAccessBlobPolicy();
 
     // Set READ and WRITE permissions.
-    sasPolicy.setPermissions(EnumSet.of(SharedAccessBlobPermissions.READ,
-        SharedAccessBlobPermissions.WRITE, SharedAccessBlobPermissions.LIST,
+    //
+    sasPolicy.setPermissions(EnumSet.of(
+        SharedAccessBlobPermissions.READ,
+        SharedAccessBlobPermissions.WRITE,
+        SharedAccessBlobPermissions.LIST,
         SharedAccessBlobPermissions.DELETE));
 
     // Create the container permissions.
@@ -725,8 +784,9 @@ public static AzureBlobStorageTestAccount createRoot(final String blobName,
 
     // Create test account initializing the appropriate member variables.
     // Set the container value to null for the default root container.
-    AzureBlobStorageTestAccount testAcct = new AzureBlobStorageTestAccount(fs,
-        account, blobRoot);
+    //
+    AzureBlobStorageTestAccount testAcct = new AzureBlobStorageTestAccount(
+        fs, account, blobRoot);
 
     // Return to caller with test account.
     return testAcct;
@@ -805,5 +865,12 @@ public void putMetrics(MetricsRecord record) {
     public void flush() {
     }
   }
- 
+
+  public void setPageBlobDirectory(String directory) {
+    this.pageBlobDirectory = directory;
+  }
+
+  public String getPageBlobDirectory() {
+    return pageBlobDirectory;
+  }
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/InMemoryBlockBlobStore.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/InMemoryBlockBlobStore.java
index ab35961..b8971c4 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/InMemoryBlockBlobStore.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/InMemoryBlockBlobStore.java
@@ -41,12 +41,15 @@
     private final String key;
     private final HashMap<String, String> metadata;
     private final int contentLength;
+    private final boolean isPageBlob;
 
+    
     ListBlobEntry(String key, HashMap<String, String> metadata,
-        int contentLength) {
+        int contentLength, boolean isPageBlob) {
       this.key = key;
       this.metadata = metadata;
       this.contentLength = contentLength;
+      this.isPageBlob = isPageBlob;
     }
 
     public String getKey() {
@@ -60,6 +63,10 @@ public String getKey() {
     public int getContentLength() {
       return contentLength;
     }
+
+    public boolean isPageBlob() {
+      return isPageBlob;
+    }
   }
 
   /**
@@ -77,10 +84,13 @@ public int getContentLength() {
     ArrayList<ListBlobEntry> list = new ArrayList<ListBlobEntry>();
     for (Map.Entry<String, Entry> entry : blobs.entrySet()) {
       if (entry.getKey().startsWith(prefix)) {
-        list.add(new ListBlobEntry(entry.getKey(),
-            includeMetadata ? new HashMap<String, String>(
-                entry.getValue().metadata) : null,
-            entry.getValue().content.length));
+        list.add(new ListBlobEntry(
+            entry.getKey(),
+            includeMetadata ?
+                new HashMap<String, String>(entry.getValue().metadata) :
+                  null,
+            entry.getValue().content.length,
+            entry.getValue().isPageBlob));
       }
     }
     return list;
@@ -92,19 +102,49 @@ public int getContentLength() {
 
   @SuppressWarnings("unchecked")
   public synchronized void setContent(String key, byte[] value,
+      HashMap<String, String> metadata, boolean isPageBlob,
+      long length) {
+    blobs.put(key, new Entry(value, (HashMap<String, String>)metadata.clone(),
+        isPageBlob, length));
+  }
+
+  @SuppressWarnings("unchecked")
+  public synchronized void setMetadata(String key,
       HashMap<String, String> metadata) {
-    blobs
-        .put(key, new Entry(value, (HashMap<String, String>) metadata.clone()));
+    blobs.get(key).metadata = (HashMap<String, String>) metadata.clone();
   }
 
-  public OutputStream upload(final String key,
+  public OutputStream uploadBlockBlob(final String key,
       final HashMap<String, String> metadata) {
-    setContent(key, new byte[0], metadata);
+    setContent(key, new byte[0], metadata, false, 0);
+    return new ByteArrayOutputStream() {
+      @Override
+      public void flush()
+          throws IOException {
+        super.flush();
+        byte[] tempBytes = toByteArray();
+        setContent(key, tempBytes, metadata, false, tempBytes.length);
+      }
+      @Override
+      public void close()
+          throws IOException {
+        super.close();
+        byte[] tempBytes = toByteArray();
+        setContent(key, tempBytes, metadata, false, tempBytes.length);
+      }
+    };
+  }
+
+  public OutputStream uploadPageBlob(final String key,
+      final HashMap<String, String> metadata,
+      final long length) {
+    setContent(key, new byte[0], metadata, true, length);
     return new ByteArrayOutputStream() {
       @Override
-      public void flush() throws IOException {
+      public void flush()
+          throws IOException {
         super.flush();
-        setContent(key, toByteArray(), metadata);
+        setContent(key, toByteArray(), metadata, true, length);
       }
     };
   }
@@ -137,10 +177,16 @@ public synchronized void setContainerMetadata(HashMap<String, String> metadata)
   private static class Entry {
     private byte[] content;
     private HashMap<String, String> metadata;
+    private boolean isPageBlob;
+    @SuppressWarnings("unused") // TODO: use it
+    private long length;
 
-    public Entry(byte[] content, HashMap<String, String> metadata) {
+    public Entry(byte[] content, HashMap<String, String> metadata,
+        boolean isPageBlob, long length) {
       this.content = content;
       this.metadata = metadata;
+      this.isPageBlob = isPageBlob;
+      this.length = length;
     }
   }
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/MockStorageInterface.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/MockStorageInterface.java
index b03997c..047ea1b 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/MockStorageInterface.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/MockStorageInterface.java
@@ -33,7 +33,7 @@
 
 import org.apache.commons.httpclient.URIException;
 import org.apache.commons.httpclient.util.URIUtil;
-import org.apache.commons.io.output.ByteArrayOutputStream;
+import org.apache.commons.lang.NotImplementedException;
 
 import com.microsoft.windowsazure.storage.CloudStorageAccount;
 import com.microsoft.windowsazure.storage.OperationContext;
@@ -44,10 +44,15 @@
 import com.microsoft.windowsazure.storage.blob.BlobListingDetails;
 import com.microsoft.windowsazure.storage.blob.BlobProperties;
 import com.microsoft.windowsazure.storage.blob.BlobRequestOptions;
+import com.microsoft.windowsazure.storage.blob.CloudBlob;
 import com.microsoft.windowsazure.storage.blob.CloudBlobContainer;
 import com.microsoft.windowsazure.storage.blob.CloudBlobDirectory;
 import com.microsoft.windowsazure.storage.blob.CopyState;
 import com.microsoft.windowsazure.storage.blob.ListBlobItem;
+import com.microsoft.windowsazure.storage.blob.PageRange;
+
+import javax.ws.rs.core.UriBuilder;
+import javax.ws.rs.core.UriBuilderException;
 
 /**
  * A mock implementation of the Azure Storage interaction layer for unit tests.
@@ -55,7 +60,8 @@
  */
 public class MockStorageInterface extends StorageInterface {
   private InMemoryBlockBlobStore backingStore;
-  private final ArrayList<PreExistingContainer> preExistingContainers = new ArrayList<MockStorageInterface.PreExistingContainer>();
+  private final ArrayList<PreExistingContainer> preExistingContainers =
+      new ArrayList<MockStorageInterface.PreExistingContainer>();
   private String baseUriString;
 
   public InMemoryBlockBlobStore getBackingStore() {
@@ -107,6 +113,33 @@ public StorageCredentials getCredentials() {
     return null;
   }
 
+  /**
+   * Utility function used to convert a given URI to a decoded string
+   * representation sent to the backing store. URIs coming as input
+   * to this class will be encoded by the URI class, and we want
+   * the underlying storage to store keys in their original UTF-8 form.
+   */
+  private static String convertUriToDecodedString(URI uri) {
+    try {
+      String result = URIUtil.decode(uri.toString());
+      return result;
+    } catch (URIException e) {
+      throw new AssertionError("Failed to decode URI: " + uri.toString());
+    }
+  }
+
+  private static URI convertKeyToEncodedUri(String key) {
+    try {
+      String encodedKey = URIUtil.encodePath(key);
+      URI uri = new URI(encodedKey);
+      return uri;
+    } catch (URISyntaxException e) {
+      throw new AssertionError("Failed to encode key: " + key);
+    } catch (URIException e) {
+      throw new AssertionError("Failed to encode key: " + key);
+    }
+  }
+
   @Override
   public CloudBlobContainerWrapper getContainerReference(String name)
       throws URISyntaxException, StorageException {
@@ -196,6 +229,12 @@ public CloudBlockBlobWrapper getBlockBlobReference(String relativePath)
           false)), null, 0);
     }
 
+    @Override
+    public CloudPageBlobWrapper getPageBlobReference(String blobAddressUri)
+        throws URISyntaxException, StorageException {
+      return new MockCloudPageBlobWrapper(new URI(blobAddressUri), null, 0);
+    }
+
     // helper to create full URIs for directory and blob.
     // use withTrailingSlash=true to get a good path for a directory.
     private String fullUriString(String relativePath, boolean withTrailingSlash) {
@@ -260,24 +299,41 @@ public URI getUri() {
         BlobRequestOptions options, OperationContext opContext)
         throws URISyntaxException, StorageException {
       ArrayList<ListBlobItem> ret = new ArrayList<ListBlobItem>();
-      String fullPrefix = prefix == null ? uri.toString() : new URI(
-          uri.getScheme(), uri.getAuthority(), uri.getPath() + prefix,
-          uri.getQuery(), uri.getFragment()).toString();
-      boolean includeMetadata = listingDetails
-          .contains(BlobListingDetails.METADATA);
+      URI searchUri = null;
+      if (prefix == null) {
+        searchUri = uri;
+      } else {
+        try {
+          searchUri = UriBuilder.fromUri(uri).path(prefix).build();
+        } catch (UriBuilderException e) {
+          throw new AssertionError("Failed to encode path: " + prefix);
+        }
+      }
+
+      String fullPrefix = convertUriToDecodedString(searchUri);
+      boolean includeMetadata = listingDetails.contains(BlobListingDetails.METADATA);
       HashSet<String> addedDirectories = new HashSet<String>();
-      for (InMemoryBlockBlobStore.ListBlobEntry current : backingStore
-          .listBlobs(fullPrefix, includeMetadata)) {
+      for (InMemoryBlockBlobStore.ListBlobEntry current : backingStore.listBlobs(
+          fullPrefix, includeMetadata)) {
         int indexOfSlash = current.getKey().indexOf('/', fullPrefix.length());
         if (useFlatBlobListing || indexOfSlash < 0) {
-          ret.add(new MockCloudBlockBlobWrapper(new URI(current.getKey()),
-              current.getMetadata(), current.getContentLength()));
+          if (current.isPageBlob()) {
+            ret.add(new MockCloudPageBlobWrapper(
+                convertKeyToEncodedUri(current.getKey()),
+                current.getMetadata(),
+                current.getContentLength()));
+          } else {
+          ret.add(new MockCloudBlockBlobWrapper(
+              convertKeyToEncodedUri(current.getKey()),
+              current.getMetadata(),
+              current.getContentLength()));
+          }
         } else {
           String directoryName = current.getKey().substring(0, indexOfSlash);
           if (!addedDirectories.contains(directoryName)) {
             addedDirectories.add(current.getKey());
-            ret.add(new MockCloudBlobDirectoryWrapper(new URI(directoryName
-                + "/")));
+            ret.add(new MockCloudBlobDirectoryWrapper(new URI(
+                directoryName + "/")));
           }
         }
       }
@@ -286,35 +342,35 @@ public URI getUri() {
 
     @Override
     public StorageUri getStorageUri() {
-      throw new UnsupportedOperationException();
+      throw new NotImplementedException();
     }
-
   }
 
-  class MockCloudBlockBlobWrapper extends CloudBlockBlobWrapper {
-    private URI uri;
-    private HashMap<String, String> metadata = new HashMap<String, String>();
-    private BlobProperties properties;
+  abstract class MockCloudBlobWrapper implements CloudBlobWrapper {
+    protected final URI uri;
+    protected HashMap<String, String> metadata =
+        new HashMap<String, String>();
+    protected BlobProperties properties;
 
-    public MockCloudBlockBlobWrapper(URI uri, HashMap<String, String> metadata,
+    protected MockCloudBlobWrapper(URI uri, HashMap<String, String> metadata,
         int length) {
       this.uri = uri;
       this.metadata = metadata;
       this.properties = new BlobProperties();
       this.properties.setLength(length);
-      this.properties.setLastModified(Calendar.getInstance(
-          TimeZone.getTimeZone("UTC")).getTime());
+      this.properties.setLastModified(
+          Calendar.getInstance(TimeZone.getTimeZone("UTC")).getTime());
     }
 
-    private void refreshProperties(boolean getMetadata) {
-      if (backingStore.exists(uri.toString())) {
-        byte[] content = backingStore.getContent(uri.toString());
+    protected void refreshProperties(boolean getMetadata) {
+      if (backingStore.exists(convertUriToDecodedString(uri))) {
+        byte[] content = backingStore.getContent(convertUriToDecodedString(uri));
         properties = new BlobProperties();
         properties.setLength(content.length);
-        properties.setLastModified(Calendar.getInstance(
-            TimeZone.getTimeZone("UTC")).getTime());
+        properties.setLastModified(
+            Calendar.getInstance(TimeZone.getTimeZone("UTC")).getTime());
         if (getMetadata) {
-          metadata = backingStore.getMetadata(uri.toString());
+          metadata = backingStore.getMetadata(convertUriToDecodedString(uri));
         }
       }
     }
@@ -347,26 +403,27 @@ public void setMetadata(HashMap<String, String> metadata) {
     }
 
     @Override
-    public void startCopyFromBlob(CloudBlockBlobWrapper sourceBlob,
+    public void startCopyFromBlob(URI source,
         OperationContext opContext) throws StorageException, URISyntaxException {
-      backingStore.copy(sourceBlob.getUri().toString(), uri.toString());
-      // it would be best if backingStore.properties.CopyState were tracked
-      // If implemented, update azureNativeFileSystemStore.waitForCopyToComplete
+      backingStore.copy(convertUriToDecodedString(source), convertUriToDecodedString(uri));
+      //TODO: set the backingStore.properties.CopyState and
+      //      update azureNativeFileSystemStore.waitForCopyToComplete
     }
 
     @Override
     public CopyState getCopyState() {
-      return this.properties.getCopyState();
+       return this.properties.getCopyState();
     }
 
     @Override
-    public void delete(OperationContext opContext) throws StorageException {
-      backingStore.delete(uri.toString());
+    public void delete(OperationContext opContext, SelfRenewingLease lease)
+        throws StorageException {
+      backingStore.delete(convertUriToDecodedString(uri));
     }
 
     @Override
     public boolean exists(OperationContext opContext) throws StorageException {
-      return backingStore.exists(uri.toString());
+      return backingStore.exists(convertUriToDecodedString(uri));
     }
 
     @Override
@@ -383,37 +440,90 @@ public BlobProperties getProperties() {
     @Override
     public InputStream openInputStream(BlobRequestOptions options,
         OperationContext opContext) throws StorageException {
-      return new ByteArrayInputStream(backingStore.getContent(uri.toString()));
+      return new ByteArrayInputStream(
+          backingStore.getContent(convertUriToDecodedString(uri)));
+    }
+
+    @Override
+    public void uploadMetadata(OperationContext opContext)
+        throws StorageException {
+      backingStore.setMetadata(convertUriToDecodedString(uri), metadata);
+    }
+
+    @Override
+    public void downloadRange(long offset, long length, OutputStream os,
+        BlobRequestOptions options, OperationContext opContext)
+        throws StorageException {
+      throw new NotImplementedException();
+    }
+  }
+
+  class MockCloudBlockBlobWrapper extends MockCloudBlobWrapper
+    implements CloudBlockBlobWrapper {
+    public MockCloudBlockBlobWrapper(URI uri, HashMap<String, String> metadata,
+        int length) {
+      super(uri, metadata, length);
     }
 
     @Override
     public OutputStream openOutputStream(BlobRequestOptions options,
         OperationContext opContext) throws StorageException {
-      return backingStore.upload(uri.toString(), metadata);
+      return backingStore.uploadBlockBlob(convertUriToDecodedString(uri),
+          metadata);
     }
 
     @Override
-    public void upload(InputStream sourceStream, OperationContext opContext)
-        throws StorageException, IOException {
-      ByteArrayOutputStream allContent = new ByteArrayOutputStream();
-      allContent.write(sourceStream);
-      backingStore.setContent(uri.toString(), allContent.toByteArray(),
-          metadata);
-      refreshProperties(false);
-      allContent.close();
+    public void setStreamMinimumReadSizeInBytes(int minimumReadSizeBytes) {
     }
 
     @Override
-    public void uploadMetadata(OperationContext opContext)
-        throws StorageException {
-      backingStore.setContent(uri.toString(),
-          backingStore.getContent(uri.toString()), metadata);
+    public void setWriteBlockSizeInBytes(int writeBlockSizeBytes) {
     }
 
     @Override
-    public void uploadProperties(OperationContext opContext)
-        throws StorageException {
-      refreshProperties(false);
+    public StorageUri getStorageUri() {
+      return null;
+    }
+
+    @Override
+    public void uploadProperties(OperationContext context, SelfRenewingLease lease) {
+    }
+
+    @Override
+    public SelfRenewingLease acquireLease() {
+      return null;
+    }
+
+    @Override
+    public CloudBlob getBlob() {
+      return null;
+    }
+  }
+
+  class MockCloudPageBlobWrapper extends MockCloudBlobWrapper
+    implements CloudPageBlobWrapper {
+    public MockCloudPageBlobWrapper(URI uri, HashMap<String, String> metadata,
+        int length) {
+      super(uri, metadata, length);
+    }
+
+    @Override
+    public void create(long length, BlobRequestOptions options,
+        OperationContext opContext) throws StorageException {
+      throw new NotImplementedException();
+    }
+
+    @Override
+    public void uploadPages(InputStream sourceStream, long offset, long length,
+        BlobRequestOptions options, OperationContext opContext)
+        throws StorageException, IOException {
+      throw new NotImplementedException();
+    }
+
+    @Override
+    public ArrayList<PageRange> downloadPageRanges(BlobRequestOptions options,
+        OperationContext opContext) throws StorageException {
+      throw new NotImplementedException();
     }
 
     @Override
@@ -426,8 +536,23 @@ public void setWriteBlockSizeInBytes(int writeBlockSizeInBytes) {
 
     @Override
     public StorageUri getStorageUri() {
-      throw new UnsupportedOperationException();
+        throw new NotImplementedException();
     }
 
+    @Override
+    public void uploadProperties(OperationContext opContext,
+        SelfRenewingLease lease)
+        throws StorageException {
+    }
+
+    @Override
+    public SelfRenewingLease acquireLease() {
+      return null;
+    }
+
+    @Override
+    public CloudBlob getBlob() {
+      return null;
+    }
   }
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/NativeAzureFileSystemBaseTest.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/NativeAzureFileSystemBaseTest.java
index e731b21..01cf713 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/NativeAzureFileSystemBaseTest.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/NativeAzureFileSystemBaseTest.java
@@ -22,6 +22,7 @@
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 import static org.junit.Assume.assumeNotNull;
 
 import java.io.BufferedReader;
@@ -32,10 +33,15 @@
 import java.io.InputStreamReader;
 import java.io.OutputStream;
 import java.io.OutputStreamWriter;
+import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.Calendar;
 import java.util.Date;
 import java.util.TimeZone;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
@@ -49,6 +55,13 @@
 import org.junit.Before;
 import org.junit.Test;
 
+import org.apache.hadoop.fs.azure.AzureException;
+import org.apache.hadoop.fs.azure.NativeAzureFileSystem.FolderRenamePending;
+
+import com.microsoft.windowsazure.storage.AccessCondition;
+import com.microsoft.windowsazure.storage.StorageException;
+import com.microsoft.windowsazure.storage.blob.CloudBlob;
+
 /*
  * Tests the Native Azure file system (WASB) against an actual blob store if
  * provided in the environment.
@@ -59,12 +72,13 @@
  */
 public abstract class NativeAzureFileSystemBaseTest {
 
-  private FileSystem fs;
+  protected FileSystem fs;
   private AzureBlobStorageTestAccount testAccount;
   private final long modifiedTimeErrorMargin = 5 * 1000; // Give it +/-5 seconds
 
-  protected abstract AzureBlobStorageTestAccount createTestAccount()
-      throws Exception;
+  protected abstract AzureBlobStorageTestAccount createTestAccount() throws Exception;
+
+  public static final Log LOG = LogFactory.getLog(NativeAzureFileSystemBaseTest.class);
 
   @Before
   public void setUp() throws Exception {
@@ -140,7 +154,7 @@ public void testFolderOwnership() throws Exception {
   private void testOwnership(Path pathUnderTest) throws IOException {
     FileStatus ret = fs.getFileStatus(pathUnderTest);
     UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();
-    assertEquals(ret.getOwner(), currentUser.getShortUserName());
+    assertTrue(ret.getOwner().equals(currentUser.getShortUserName()));
     fs.delete(pathUnderTest, true);
   }
 
@@ -177,18 +191,29 @@ public void testFolderPermissions() throws Exception {
     fs.delete(testFolder, true);
   }
 
-  @Test
-  public void testDeepFileCreation() throws Exception {
-    Path testFile = new Path("deep/file/creation/test");
-    FsPermission permission = FsPermission.createImmutable((short) 644);
+  void testDeepFileCreationBase(String testFilePath, String firstDirPath, String middleDirPath,
+          short permissionShort, short umaskedPermissionShort) throws Exception  {
+    Path testFile = new Path(testFilePath);
+    Path firstDir = new Path(firstDirPath);
+    Path middleDir = new Path(middleDirPath);
+    FsPermission permission = FsPermission.createImmutable(permissionShort);
+    FsPermission umaskedPermission = FsPermission.createImmutable(umaskedPermissionShort);
+
     createEmptyFile(testFile, permission);
+    FsPermission rootPerm = fs.getFileStatus(firstDir.getParent()).getPermission();
+    FsPermission inheritPerm = FsPermission.createImmutable((short)(rootPerm.toShort() | 0300));
     assertTrue(fs.exists(testFile));
-    assertTrue(fs.exists(new Path("deep")));
-    assertTrue(fs.exists(new Path("deep/file/creation")));
-    FileStatus ret = fs.getFileStatus(new Path("deep/file"));
-    assertTrue(ret.isDirectory());
-    assertEqualsIgnoreStickyBit(permission, ret.getPermission());
-    assertTrue(fs.delete(new Path("deep"), true));
+    assertTrue(fs.exists(firstDir));
+    assertTrue(fs.exists(middleDir));
+    // verify that the indirectly created directory inherited its permissions from the root directory
+    FileStatus directoryStatus = fs.getFileStatus(middleDir);
+    assertTrue(directoryStatus.isDirectory());
+    assertEqualsIgnoreStickyBit(inheritPerm, directoryStatus.getPermission());
+    // verify that the file itself has the permissions as specified
+    FileStatus fileStatus = fs.getFileStatus(testFile);
+    assertFalse(fileStatus.isDirectory());
+    assertEqualsIgnoreStickyBit(umaskedPermission, fileStatus.getPermission());
+    assertTrue(fs.delete(firstDir, true));
     assertFalse(fs.exists(testFile));
 
     // An alternative test scenario would've been to delete the file first,
@@ -196,6 +221,22 @@ public void testDeepFileCreation() throws Exception {
     // doesn't actually work as expected right now.
   }
 
+  @Test
+  public void testDeepFileCreation() throws Exception {
+    // normal permissions in user home
+    testDeepFileCreationBase("deep/file/creation/test", "deep", "deep/file/creation", (short)0644, (short)0644);
+    // extra permissions in user home. umask will change the actual permissions.
+    testDeepFileCreationBase("deep/file/creation/test", "deep", "deep/file/creation", (short)0777, (short)0755);
+    // normal permissions in root
+    testDeepFileCreationBase("/deep/file/creation/test", "/deep", "/deep/file/creation", (short)0644, (short)0644);
+    // less permissions in root
+    testDeepFileCreationBase("/deep/file/creation/test", "/deep", "/deep/file/creation", (short)0700, (short)0700);
+    // one indirectly created directory in root
+    testDeepFileCreationBase("/deep/file", "/deep", "/deep", (short)0644, (short)0644);
+    // one indirectly created directory in user home
+    testDeepFileCreationBase("deep/file", "deep", "deep", (short)0644, (short)0644);
+  }
+
   private static enum RenameVariation {
     NormalFileName, SourceInAFolder, SourceWithSpace, SourceWithPlusAndPercent
   }
@@ -206,20 +247,20 @@ public void testRename() throws Exception {
       System.out.printf("Rename variation: %s\n", variation);
       Path originalFile;
       switch (variation) {
-      case NormalFileName:
-        originalFile = new Path("fileToRename");
-        break;
-      case SourceInAFolder:
-        originalFile = new Path("file/to/rename");
-        break;
-      case SourceWithSpace:
-        originalFile = new Path("file to rename");
-        break;
-      case SourceWithPlusAndPercent:
-        originalFile = new Path("file+to%rename");
-        break;
-      default:
-        throw new Exception("Unknown variation");
+        case NormalFileName:
+          originalFile = new Path("fileToRename");
+          break;
+        case SourceInAFolder:
+          originalFile = new Path("file/to/rename");
+          break;
+        case SourceWithSpace:
+          originalFile = new Path("file to rename");
+          break;
+        case SourceWithPlusAndPercent:
+          originalFile = new Path("file+to%rename");
+          break;
+        default:
+          throw new Exception("Unknown variation");
       }
       Path destinationFile = new Path("file/resting/destination");
       assertTrue(fs.createNewFile(originalFile));
@@ -227,7 +268,8 @@ public void testRename() throws Exception {
       assertFalse(fs.rename(originalFile, destinationFile)); // Parent directory
       // doesn't exist
       assertTrue(fs.mkdirs(destinationFile.getParent()));
-      assertTrue(fs.rename(originalFile, destinationFile));
+      boolean result = fs.rename(originalFile, destinationFile);
+      assertTrue(result);
       assertTrue(fs.exists(destinationFile));
       assertFalse(fs.exists(originalFile));
       fs.delete(destinationFile.getParent(), true);
@@ -239,10 +281,10 @@ public void testRenameImplicitFolder() throws Exception {
     Path testFile = new Path("deep/file/rename/test");
     FsPermission permission = FsPermission.createImmutable((short) 644);
     createEmptyFile(testFile, permission);
-    assertTrue(fs.rename(new Path("deep/file"), new Path("deep/renamed")));
+    boolean renameResult = fs.rename(new Path("deep/file"), new Path("deep/renamed"));
+    assertTrue(renameResult);
     assertFalse(fs.exists(testFile));
-    FileStatus newStatus = fs
-        .getFileStatus(new Path("deep/renamed/rename/test"));
+    FileStatus newStatus = fs.getFileStatus(new Path("deep/renamed/rename/test"));
     assertNotNull(newStatus);
     assertEqualsIgnoreStickyBit(permission, newStatus.getPermission());
     assertTrue(fs.delete(new Path("deep"), true));
@@ -256,21 +298,25 @@ public void testRenameImplicitFolder() throws Exception {
   public void testRenameFolder() throws Exception {
     for (RenameFolderVariation variation : RenameFolderVariation.values()) {
       Path originalFolder = new Path("folderToRename");
-      if (variation != RenameFolderVariation.CreateJustInnerFile){
+      if (variation != RenameFolderVariation.CreateJustInnerFile) {
         assertTrue(fs.mkdirs(originalFolder));
       }
       Path innerFile = new Path(originalFolder, "innerFile");
-      if (variation != RenameFolderVariation.CreateJustFolder){
+      Path innerFile2 = new Path(originalFolder, "innerFile2");
+      if (variation != RenameFolderVariation.CreateJustFolder) {
         assertTrue(fs.createNewFile(innerFile));
+        assertTrue(fs.createNewFile(innerFile2));
       }
       Path destination = new Path("renamedFolder");
       assertTrue(fs.rename(originalFolder, destination));
       assertTrue(fs.exists(destination));
-      if (variation != RenameFolderVariation.CreateJustFolder){
+      if (variation != RenameFolderVariation.CreateJustFolder) {
         assertTrue(fs.exists(new Path(destination, innerFile.getName())));
+        assertTrue(fs.exists(new Path(destination, innerFile2.getName())));
       }
       assertFalse(fs.exists(originalFolder));
       assertFalse(fs.exists(innerFile));
+      assertFalse(fs.exists(innerFile2));
       fs.delete(destination, true);
     }
   }
@@ -365,6 +411,43 @@ public void testUriEncodingMoreComplexCharacters() throws Exception {
   }
 
   @Test
+  public void testChineseCharacters() throws Exception {
+    // Create a file and a folder with Chinese (non-ASCI) characters
+    String chinese = "" + '\u963f' + '\u4db5';
+    String fileName = "filename" + chinese;
+    String directoryName = chinese;
+    fs.create(new Path(directoryName, fileName)).close();
+    FileStatus[] listing = fs.listStatus(new Path(directoryName));
+    assertEquals(1, listing.length);
+    assertEquals(fileName, listing[0].getPath().getName());
+    FileStatus status = fs.getFileStatus(new Path(directoryName, fileName));
+    assertEquals(fileName, status.getPath().getName());
+    InputStream stream = fs.open(new Path(directoryName, fileName));
+    assertNotNull(stream);
+    stream.close();
+    assertTrue(fs.delete(new Path(directoryName, fileName), true));
+    assertTrue(fs.delete(new Path(directoryName), true));
+  }
+
+  @Test
+  public void testChineseCharactersFolderRename() throws Exception {
+    // Create a file and a folder with Chinese (non-ASCI) characters
+    String chinese = "" + '\u963f' + '\u4db5';
+    String fileName = "filename" + chinese;
+    String srcDirectoryName = chinese;
+    String targetDirectoryName = "target" + chinese;
+    fs.create(new Path(srcDirectoryName, fileName)).close();
+    fs.rename(new Path(srcDirectoryName), new Path(targetDirectoryName));
+    FileStatus[] listing = fs.listStatus(new Path(targetDirectoryName));
+    assertEquals(1, listing.length);
+    assertEquals(fileName, listing[0].getPath().getName());
+    FileStatus status = fs.getFileStatus(new Path(targetDirectoryName, fileName));
+    assertEquals(fileName, status.getPath().getName());
+    assertTrue(fs.delete(new Path(targetDirectoryName, fileName), true));
+    assertTrue(fs.delete(new Path(targetDirectoryName), true));
+  }
+
+  @Test
   public void testReadingDirectoryAsFile() throws Exception {
     Path dir = new Path("/x");
     assertTrue(fs.mkdirs(dir));
@@ -403,7 +486,12 @@ public void testSetPermissionOnFile() throws Exception {
     assertEquals("supergroup", newStatus.getGroup());
     assertEquals(UserGroupInformation.getCurrentUser().getShortUserName(),
         newStatus.getOwner());
-    assertEquals(1, newStatus.getLen());
+
+    // Don't check the file length for page blobs. Only block blobs
+    // provide the actual length of bytes written.
+    if (!(this instanceof TestNativeAzureFSPageBlobLive)) {
+      assertEquals(1, newStatus.getLen());
+    }
   }
 
   @Test
@@ -429,7 +517,12 @@ public void testSetOwnerOnFile() throws Exception {
     assertNotNull(newStatus);
     assertEquals("newUser", newStatus.getOwner());
     assertEquals("supergroup", newStatus.getGroup());
-    assertEquals(1, newStatus.getLen());
+
+    // File length is only reported to be the size of bytes written to the file for block blobs.
+    // So only check it for block blobs, not page blobs.
+    if (!(this instanceof TestNativeAzureFSPageBlobLive)) {
+      assertEquals(1, newStatus.getLen());
+    }
     fs.setOwner(newFile, null, "newGroup");
     newStatus = fs.getFileStatus(newFile);
     assertNotNull(newStatus);
@@ -506,14 +599,570 @@ public void testFolderLastModifiedTime() throws Exception {
     testModifiedTime(destFolder);
   }
 
+  /**
+   * Verify we can get file status of a directory with various forms of
+   * the directory file name, including the nonstandard but legal form
+   * ending in "/.". Check that we're getting status for a directory.
+   */
   @Test
   public void testListSlash() throws Exception {
     Path testFolder = new Path("/testFolder");
     Path testFile = new Path(testFolder, "testFile");
     assertTrue(fs.mkdirs(testFolder));
     assertTrue(fs.createNewFile(testFile));
-    FileStatus status = fs.getFileStatus(new Path("/testFolder/."));
-    assertNotNull(status);
+    FileStatus status;
+    status = fs.getFileStatus(new Path("/testFolder"));
+    assertTrue(status.isDirectory());
+    status = fs.getFileStatus(new Path("/testFolder/"));
+    assertTrue(status.isDirectory());
+    status = fs.getFileStatus(new Path("/testFolder/."));
+    assertTrue(status.isDirectory());
+  }
+
+  @Test
+  public void testCannotCreatePageBlobByDefault() throws Exception {
+
+    // Verify that the page blob directory list configuration setting
+    // is not set in the default configuration.
+    Configuration conf = new Configuration();
+    String[] rawPageBlobDirs =
+        conf.getStrings(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES);
+    assertTrue(rawPageBlobDirs == null);
+  }
+
+  /*
+   * Set up a situation where a folder rename is partway finished.
+   * Then apply redo to finish the rename.
+   *
+   * The original source folder *would* have had contents
+   * folderToRename  (0 byte dummy file for directory)
+   * folderToRename/innerFile
+   * folderToRename/innerFile2
+   *
+   * The actual source folder (after partial rename and failure)
+   *
+   * folderToRename
+   * folderToRename/innerFile2
+   *
+   * The actual target folder (after partial rename and failure)
+   *
+   * renamedFolder
+   * renamedFolder/innerFile
+   */
+  @Test
+  public void testRedoRenameFolder() throws IOException {
+    // create original folder
+    String srcKey = "folderToRename";
+    Path originalFolder = new Path(srcKey);
+    assertTrue(fs.mkdirs(originalFolder));
+    Path innerFile = new Path(originalFolder, "innerFile");
+    assertTrue(fs.createNewFile(innerFile));
+    Path innerFile2 = new Path(originalFolder, "innerFile2");
+    assertTrue(fs.createNewFile(innerFile2));
+
+    String dstKey = "renamedFolder";
+
+    // propose (but don't do) the rename
+    Path home = fs.getHomeDirectory();
+    String relativeHomeDir = getRelativePath(home.toString());
+    NativeAzureFileSystem.FolderRenamePending pending =
+        new NativeAzureFileSystem.FolderRenamePending(
+            relativeHomeDir + "/" + srcKey,
+            relativeHomeDir + "/" + dstKey, null,
+            (NativeAzureFileSystem) fs);
+
+    // get the rename pending file contents
+    String renameDescription = pending.makeRenamePendingFileContents();
+
+    // Remove one file from source folder to simulate a partially done
+    // rename operation.
+    assertTrue(fs.delete(innerFile, false));
+
+    // Create the destination folder with just one file in it, again
+    // to simulate a partially done rename.
+    Path destination = new Path(dstKey);
+    Path innerDest = new Path(destination, "innerFile");
+    assertTrue(fs.createNewFile(innerDest));
+
+    // Create a rename-pending file and write rename information to it.
+    final String renamePendingStr = "folderToRename-RenamePending.json";
+    Path renamePendingFile = new Path(renamePendingStr);
+    FSDataOutputStream out = fs.create(renamePendingFile, true);
+    assertTrue(out != null);
+    writeString(out, renameDescription);
+
+    // Redo the rename operation based on the contents of the -RenamePending.json file.
+    // Trigger the redo by checking for existence of the original folder. It must appear
+    // to not exist.
+    assertFalse(fs.exists(originalFolder));
+
+    // Verify that the target is there, and the source is gone.
+    assertTrue(fs.exists(destination));
+    assertTrue(fs.exists(new Path(destination, innerFile.getName())));
+    assertTrue(fs.exists(new Path(destination, innerFile2.getName())));
+    assertFalse(fs.exists(originalFolder));
+    assertFalse(fs.exists(innerFile));
+    assertFalse(fs.exists(innerFile2));
+
+    // Verify that there's no RenamePending file left.
+    assertFalse(fs.exists(renamePendingFile));
+
+    // Verify that we can list the target directory.
+    FileStatus[] listed = fs.listStatus(destination);
+    assertEquals(2, listed.length);
+
+    // List the home directory and show the contents is a directory.
+    Path root = fs.getHomeDirectory();
+    listed = fs.listStatus(root);
+    assertEquals(1, listed.length);
+    assertTrue(listed[0].isDirectory());
+  }
+
+  /**
+   * If there is a folder to be renamed inside a parent folder,
+   * then when you list the parent folder, you should only see
+   * the final result, after the rename.
+   */
+  @Test
+  public void testRedoRenameFolderInFolderListing() throws IOException {
+
+    // create original folder
+    String parent = "parent";
+    Path parentFolder = new Path(parent);
+    assertTrue(fs.mkdirs(parentFolder));
+    Path inner = new Path(parentFolder, "innerFolder");
+    assertTrue(fs.mkdirs(inner));
+    Path inner2 = new Path(parentFolder, "innerFolder2");
+    assertTrue(fs.mkdirs(inner2));
+    Path innerFile = new Path(inner2, "file");
+    assertTrue(fs.createNewFile(innerFile));
+
+    Path inner2renamed = new Path(parentFolder, "innerFolder2Renamed");
+
+    // propose (but don't do) the rename of innerFolder2
+    Path home = fs.getHomeDirectory();
+    String relativeHomeDir = getRelativePath(home.toString());
+    NativeAzureFileSystem.FolderRenamePending pending =
+        new NativeAzureFileSystem.FolderRenamePending(
+            relativeHomeDir + "/" + inner2,
+            relativeHomeDir + "/" + inner2renamed, null,
+            (NativeAzureFileSystem) fs);
+
+    // Create a rename-pending file and write rename information to it.
+    final String renamePendingStr = inner2 + FolderRenamePending.SUFFIX;
+    Path renamePendingFile = new Path(renamePendingStr);
+    FSDataOutputStream out = fs.create(renamePendingFile, true);
+    assertTrue(out != null);
+    writeString(out, pending.makeRenamePendingFileContents());
+
+    // Redo the rename operation based on the contents of the
+    // -RenamePending.json file. Trigger the redo by checking for existence of
+    // the original folder. It must appear to not exist.
+    FileStatus[] listed = fs.listStatus(parentFolder);
+    assertEquals(2, listed.length);
+    assertTrue(listed[0].isDirectory());
+    assertTrue(listed[1].isDirectory());
+
+    // The rename pending file is not a directory, so at this point we know the
+    // redo has been done.
+    assertFalse(fs.exists(inner2)); // verify original folder is gone
+    assertTrue(fs.exists(inner2renamed)); // verify the target is there
+    assertTrue(fs.exists(new Path(inner2renamed, "file")));
+  }
+
+  /**
+   * Test the situation where a rename pending file exists but the rename
+   * is really done. This could happen if the rename process died just
+   * before deleting the rename pending file. It exercises a non-standard
+   * code path in redo().
+   */
+  @Test
+  public void testRenameRedoFolderAlreadyDone() throws IOException {
+    // create only destination folder
+    String orig = "originalFolder";
+    String dest = "renamedFolder";
+    Path destPath = new Path(dest);
+    assertTrue(fs.mkdirs(destPath));
+
+    // propose (but don't do) the rename of innerFolder2
+    Path home = fs.getHomeDirectory();
+    String relativeHomeDir = getRelativePath(home.toString());
+    NativeAzureFileSystem.FolderRenamePending pending =
+        new NativeAzureFileSystem.FolderRenamePending(
+            relativeHomeDir + "/" + orig,
+            relativeHomeDir + "/" + dest, null,
+            (NativeAzureFileSystem) fs);
+
+    // Create a rename-pending file and write rename information to it.
+    final String renamePendingStr = orig + FolderRenamePending.SUFFIX;
+    Path renamePendingFile = new Path(renamePendingStr);
+    FSDataOutputStream out = fs.create(renamePendingFile, true);
+    assertTrue(out != null);
+    writeString(out, pending.makeRenamePendingFileContents());
+
+    try {
+      pending.redo();
+    } catch (Exception e) {
+      fail();
+    }
+
+    // Make sure rename pending file is gone.
+    FileStatus[] listed = fs.listStatus(new Path("/"));
+    assertEquals(1, listed.length);
+    assertTrue(listed[0].isDirectory());
+  }
+
+  @Test
+  public void testRedoFolderRenameAll() throws IllegalArgumentException, IOException {
+    {
+      FileFolder original = new FileFolder("folderToRename");
+      original.add("innerFile").add("innerFile2");
+      FileFolder partialSrc = original.copy();
+      FileFolder partialDst = original.copy();
+      partialDst.setName("renamedFolder");
+      partialSrc.setPresent(0, false);
+      partialDst.setPresent(1, false);
+
+      testRenameRedoFolderSituation(original, partialSrc, partialDst);
+    }
+    {
+      FileFolder original = new FileFolder("folderToRename");
+      original.add("file1").add("file2").add("file3");
+      FileFolder partialSrc = original.copy();
+      FileFolder partialDst = original.copy();
+      partialDst.setName("renamedFolder");
+
+      // Set up this state before the redo:
+      // folderToRename: file1       file3
+      // renamedFolder:  file1 file2
+      // This gives code coverage for all 3 expected cases for individual file
+      // redo.
+      partialSrc.setPresent(1, false);
+      partialDst.setPresent(2, false);
+
+      testRenameRedoFolderSituation(original, partialSrc, partialDst);
+    }
+    {
+      // Simulate a situation with folder with a large number of files in it.
+      // For the first half of the files, they will be in the destination
+      // but not the source. For the second half, they will be in the source
+      // but not the destination. There will be one file in the middle that is
+      // in both source and destination. Then trigger redo and verify.
+      // For testing larger folder sizes, manually change this, temporarily, and
+      // edit the SIZE value.
+      final int SIZE = 5;
+      assertTrue(SIZE >= 3);
+      // Try a lot of files in the folder.
+      FileFolder original = new FileFolder("folderToRename");
+      for (int i = 0; i < SIZE; i++) {
+        original.add("file" + Integer.toString(i));
+      }
+      FileFolder partialSrc = original.copy();
+      FileFolder partialDst = original.copy();
+      partialDst.setName("renamedFolder");
+      for (int i = 0; i < SIZE; i++) {
+        partialSrc.setPresent(i, i >= SIZE / 2);
+        partialDst.setPresent(i, i <= SIZE / 2);
+      }
+
+      testRenameRedoFolderSituation(original, partialSrc, partialDst);
+    }
+    {
+      // Do a nested folder, like so:
+      // folderToRename:
+      //   nestedFolder: a, b, c
+      //   p
+      //   q
+      //
+      // Then delete file 'a' from the source and add it to destination.
+      // Then trigger redo.
+
+      FileFolder original = new FileFolder("folderToRename");
+      FileFolder nested = new FileFolder("nestedFolder");
+      nested.add("a").add("b").add("c");
+      original.add(nested).add("p").add("q");
+
+      FileFolder partialSrc = original.copy();
+      FileFolder partialDst = original.copy();
+      partialDst.setName("renamedFolder");
+
+      // logically remove 'a' from source
+      partialSrc.getMember(0).setPresent(0, false);
+
+      // logically eliminate b, c from destination
+      partialDst.getMember(0).setPresent(1, false);
+      partialDst.getMember(0).setPresent(2, false);
+
+      testRenameRedoFolderSituation(original, partialSrc, partialDst);
+    }
+  }
+
+  private void testRenameRedoFolderSituation(
+      FileFolder fullSrc,
+      FileFolder partialSrc,
+      FileFolder partialDst) throws IllegalArgumentException, IOException {
+
+    // make file folder tree for source
+    fullSrc.create();
+
+    // set up rename pending file
+    fullSrc.makeRenamePending(partialDst);
+
+    // prune away some files (as marked) from source to simulate partial rename
+    partialSrc.prune();
+
+    // Create only the files indicated for the destination to indicate a partial rename.
+    partialDst.create();
+
+    // trigger redo
+    assertFalse(fullSrc.exists());
+
+    // verify correct results
+    partialDst.verifyExists();
+    fullSrc.verifyGone();
+
+    // delete the new folder to leave no garbage behind
+    fs.delete(new Path(partialDst.getName()), true);
+  }
+
+  // Mock up of a generalized folder (which can also be a leaf-level file)
+  // for rename redo testing.
+  private class FileFolder {
+    private String name;
+
+    // For rename testing, indicates whether an expected
+    // file is present in the source or target folder.
+    private boolean present;
+    ArrayList<FileFolder> members; // Null if a leaf file, otherwise not null.
+
+    // Make a new, empty folder (not a regular leaf file).
+    public FileFolder(String name) {
+      this.name = name;
+      this.present = true;
+      members = new ArrayList<FileFolder>();
+    }
+
+    public FileFolder getMember(int i) {
+      return members.get(i);
+    }
+
+    // Verify a folder and all its contents are gone. This is only to
+    // be called on the root of a FileFolder.
+    public void verifyGone() throws IllegalArgumentException, IOException {
+      assertFalse(fs.exists(new Path(name)));
+      assertTrue(isFolder());
+      verifyGone(new Path(name), members);
+    }
+
+    private void verifyGone(Path prefix, ArrayList<FileFolder> members2) throws IOException {
+      for (FileFolder f : members2) {
+        f.verifyGone(prefix);
+      }
+    }
+
+    private void verifyGone(Path prefix) throws IOException {
+      assertFalse(fs.exists(new Path(prefix, name)));
+      if (isLeaf()) {
+        return;
+      }
+      for (FileFolder f : members) {
+        f.verifyGone(new Path(prefix, name));
+      }
+    }
+
+    public void verifyExists() throws IllegalArgumentException, IOException {
+
+      // verify the root is present
+      assertTrue(fs.exists(new Path(name)));
+      assertTrue(isFolder());
+
+      // check the members
+      verifyExists(new Path(name), members);
+    }
+
+    private void verifyExists(Path prefix, ArrayList<FileFolder> members2) throws IOException {
+      for (FileFolder f : members2) {
+        f.verifyExists(prefix);
+      }
+    }
+
+    private void verifyExists(Path prefix) throws IOException {
+
+      // verify this file/folder is present
+      assertTrue(fs.exists(new Path(prefix, name)));
+
+      // verify members are present
+      if (isLeaf()) {
+        return;
+      }
+
+      for (FileFolder f : members) {
+        f.verifyExists(new Path(prefix, name));
+      }
+    }
+
+    public boolean exists() throws IOException {
+      return fs.exists(new Path(name));
+    }
+
+    // Make a rename pending file for the situation where we rename
+    // this object (the source) to the specified destination.
+    public void makeRenamePending(FileFolder dst) throws IOException {
+
+      // Propose (but don't do) the rename.
+      Path home = fs.getHomeDirectory();
+      String relativeHomeDir = getRelativePath(home.toString());
+      NativeAzureFileSystem.FolderRenamePending pending =
+          new NativeAzureFileSystem.FolderRenamePending(
+              relativeHomeDir + "/" + this.getName(),
+              relativeHomeDir + "/" + dst.getName(), null,
+              (NativeAzureFileSystem) fs);
+
+      // Get the rename pending file contents.
+      String renameDescription = pending.makeRenamePendingFileContents();
+
+      // Create a rename-pending file and write rename information to it.
+      final String renamePendingStr = this.getName() + "-RenamePending.json";
+      Path renamePendingFile = new Path(renamePendingStr);
+      FSDataOutputStream out = fs.create(renamePendingFile, true);
+      assertTrue(out != null);
+      writeString(out, renameDescription);
+    }
+
+    // set whether a child is present or not
+    public void setPresent(int i, boolean b) {
+      members.get(i).setPresent(b);
+    }
+
+    // Make an uninitialized folder
+    private FileFolder() {
+      this.present = true;
+    }
+
+    public void setPresent(boolean value) {
+      present = value;
+    }
+
+    public FileFolder makeLeaf(String name) {
+      FileFolder f = new FileFolder();
+      f.setName(name);
+      return f;
+    }
+
+    void setName(String name) {
+      this.name = name;
+    }
+
+    public String getName() {
+      return name;
+    }
+
+    public boolean isLeaf() {
+      return members == null;
+    }
+
+    public boolean isFolder() {
+      return members != null;
+    }
+
+    FileFolder add(FileFolder folder) {
+      members.add(folder);
+      return this;
+    }
+
+    // Add a leaf file (by convention, if you pass a string argument, you get a leaf).
+    FileFolder add(String file) {
+      FileFolder leaf = makeLeaf(file);
+      members.add(leaf);
+      return this;
+    }
+
+    public FileFolder copy() {
+      if (isLeaf()) {
+        return makeLeaf(name);
+      } else {
+        FileFolder f = new FileFolder(name);
+        for (FileFolder member : members) {
+          f.add(member.copy());
+        }
+        return f;
+      }
+    }
+
+    // Create the folder structure. Return true on success, or else false.
+    public void create() throws IllegalArgumentException, IOException {
+      create(null);
+    }
+
+    private void create(Path prefix) throws IllegalArgumentException, IOException {
+      if (isFolder()) {
+        if (present) {
+          assertTrue(fs.mkdirs(makePath(prefix, name)));
+        }
+        create(makePath(prefix, name), members);
+      } else if (isLeaf()) {
+        if (present) {
+          assertTrue(fs.createNewFile(makePath(prefix, name)));
+        }
+      } else {
+        assertTrue("The object must be a (leaf) file or a folder.", false);
+      }
+    }
+
+    private void create(Path prefix, ArrayList<FileFolder> members2) throws IllegalArgumentException, IOException {
+      for (FileFolder f : members2) {
+        f.create(prefix);
+      }
+    }
+
+    private Path makePath(Path prefix, String name) {
+      if (prefix == null) {
+        return new Path(name);
+      } else {
+        return new Path(prefix, name);
+      }
+    }
+
+    // Remove the files marked as not present.
+    public void prune() throws IOException {
+      prune(null);
+    }
+
+    private void prune(Path prefix) throws IOException {
+      Path path = null;
+      if (prefix == null) {
+        path = new Path(name);
+      } else {
+        path = new Path(prefix, name);
+      }
+      if (isLeaf() && !present) {
+        assertTrue(fs.delete(path, false));
+      } else if (isFolder() && !present) {
+        assertTrue(fs.delete(path, true));
+      } else if (isFolder()) {
+        for (FileFolder f : members) {
+          f.prune(path);
+        }
+      }
+    }
+  }
+
+  private String getRelativePath(String path) {
+    // example input: wasb://wasbtests-ehans-1404322046279@ehans9.blob.core.windows.net/user/ehans/folderToRename
+    // example result: user/ehans/folderToRename
+
+    // Find the third / position and return input substring after that.
+    int slashCount = 0; // number of slashes so far
+    int i;
+    for (i = 0; i < path.length(); i++) {
+      if (path.charAt(i) == '/') {
+        slashCount++;
+        if (slashCount == 3) {
+          return path.substring(i + 1, path.length());
+        }
+      }
+    }
+    throw new RuntimeException("Incorrect path prefix -- expected wasb://.../...");
   }
 
   @Test
@@ -523,6 +1172,84 @@ public void testCloseFileSystemTwice() throws Exception {
     fs.close();
   }
 
+  // Test the available() method for the input stream returned by fs.open().
+  // This works for both page and block blobs.
+  int FILE_SIZE = 4 * 1024 * 1024 + 1; // Make this 1 bigger than internal
+                                       // buffer used in BlobInputStream
+                                       // to exercise that case.
+  int MAX_STRIDE = FILE_SIZE + 1;
+  Path PATH = new Path("/available.dat");
+  @Test
+  public void testAvailable() throws IOException {
+
+    // write FILE_SIZE bytes to page blob
+    FSDataOutputStream out = fs.create(PATH);
+    byte[] data = new byte[FILE_SIZE];
+    Arrays.fill(data, (byte) 5);
+    out.write(data, 0, FILE_SIZE);
+    out.close();
+
+    // Test available() for different read sizes
+    verifyAvailable(1);
+    verifyAvailable(100);
+    verifyAvailable(5000);
+    verifyAvailable(FILE_SIZE);
+    verifyAvailable(MAX_STRIDE);
+
+    fs.delete(PATH, false);
+  }
+
+  // Verify that available() for the input stream is always >= 1 unless we've
+  // consumed all the input, and then it is 0. This is to match expectations by
+  // HBase which were set based on behavior of DFSInputStream.available().
+  private void verifyAvailable(int readStride) throws IOException {
+    FSDataInputStream in = fs.open(PATH);
+    try {
+      byte[] inputBuffer = new byte[MAX_STRIDE];
+      int position = 0;
+      int bytesRead = 0;
+      while(bytesRead != FILE_SIZE) {
+        bytesRead += in.read(inputBuffer, position, readStride);
+        int available = in.available();
+        if (bytesRead < FILE_SIZE) {
+          if (available < 1) {
+            fail(String.format(
+                  "expected available > 0 but got: "
+                      + "position = %d, bytesRead = %d, in.available() = %d",
+                  position, bytesRead, available));
+          }
+        }
+      }
+      int available = in.available();
+      assertTrue(available == 0);
+    } finally {
+      in.close();
+    }
+  }
+
+  @Test
+  public void testGetFileSizeFromListing() throws IOException {
+    Path path = new Path("file.dat");
+    final int PAGE_SIZE = 512;
+    final int FILE_SIZE = PAGE_SIZE + 1;
+
+    // write FILE_SIZE bytes to page blob
+    FSDataOutputStream out = fs.create(path);
+    byte[] data = new byte[FILE_SIZE];
+    Arrays.fill(data, (byte) 5);
+    out.write(data, 0, FILE_SIZE);
+    out.close();
+
+    // list the file to get its properties
+    FileStatus[] status = fs.listStatus(path);
+    assertEquals(1, status.length);
+
+    // The file length should report the number of bytes
+    // written for either page or block blobs (subclasses
+    // of this test class will exercise both).
+    assertEquals(FILE_SIZE, status[0].getLen());
+  }
+
   private boolean testModifiedTime(Path testPath, long time) throws Exception {
     FileStatus fileStatus = fs.getFileStatus(testPath);
     final long errorMargin = modifiedTimeErrorMargin;
@@ -530,16 +1257,45 @@ private boolean testModifiedTime(Path testPath, long time) throws Exception {
     return (lastModified > (time - errorMargin) && lastModified < (time + errorMargin));
   }
 
+  @SuppressWarnings("deprecation")
+  @Test
+  public void testCreateNonRecursive() throws Exception {
+    Path testFolder = new Path("/testFolder");
+    Path testFile = new Path(testFolder, "testFile");
+    try {
+      fs.createNonRecursive(testFile, true, 1024, (short)1, 1024, null);
+      assertTrue("Should've thrown", false);
+    } catch (FileNotFoundException e) {
+    }
+    fs.mkdirs(testFolder);
+    fs.createNonRecursive(testFile, true, 1024, (short)1, 1024, null)
+      .close();
+    assertTrue(fs.exists(testFile));
+  }
+
+  public void testFileEndingInDot() throws Exception {
+    Path testFolder = new Path("/testFolder.");
+    Path testFile = new Path(testFolder, "testFile.");
+    assertTrue(fs.mkdirs(testFolder));
+    assertTrue(fs.createNewFile(testFile));
+    assertTrue(fs.exists(testFile));
+    FileStatus[] listed = fs.listStatus(testFolder);
+    assertEquals(1, listed.length);
+    assertEquals("testFile.", listed[0].getPath().getName());
+  }
   private void testModifiedTime(Path testPath) throws Exception {
     Calendar utc = Calendar.getInstance(TimeZone.getTimeZone("UTC"));
     long currentUtcTime = utc.getTime().getTime();
     FileStatus fileStatus = fs.getFileStatus(testPath);
-    assertTrue("Modification time "
-        + new Date(fileStatus.getModificationTime()) + " is not close to now: "
-        + utc.getTime(), testModifiedTime(testPath, currentUtcTime));
+    final long errorMargin = 10 * 1000; // Give it +/-10 seconds
+    assertTrue("Modification time " +
+        new Date(fileStatus.getModificationTime()) + " is not close to now: " +
+        utc.getTime(),
+        fileStatus.getModificationTime() > (currentUtcTime - errorMargin) &&
+        fileStatus.getModificationTime() < (currentUtcTime + errorMargin));
   }
 
-   private void createEmptyFile(Path testFile, FsPermission permission)
+  private void createEmptyFile(Path testFile, FsPermission permission)
       throws IOException {
     FSDataOutputStream outputStream = fs.create(testFile, permission, true,
         4096, (short) 1, 1024, null);
@@ -563,7 +1319,7 @@ private String readString(FSDataInputStream inputStream) throws IOException {
     final int BUFFER_SIZE = 1024;
     char[] buffer = new char[BUFFER_SIZE];
     int count = reader.read(buffer, 0, BUFFER_SIZE);
-    if (count >= BUFFER_SIZE) {
+    if (count > BUFFER_SIZE) {
       throw new IOException("Exceeded buffer size");
     }
     inputStream.close();
@@ -578,7 +1334,6 @@ private void writeString(FileSystem fs, Path path, String value)
       throws IOException {
     FSDataOutputStream outputStream = fs.create(path, true);
     writeString(outputStream, value);
-    outputStream.close();
   }
 
   private void writeString(FSDataOutputStream outputStream, String value)
@@ -588,4 +1343,175 @@ private void writeString(FSDataOutputStream outputStream, String value)
     writer.write(value);
     writer.close();
   }
+
+  @Test
+  // Acquire and free a Lease object. Wait for more than the lease
+  // timeout, to make sure the lease renews itself.
+  public void testSelfRenewingLease() throws IllegalArgumentException, IOException,
+    InterruptedException, StorageException {
+
+    SelfRenewingLease lease;
+    final String FILE_KEY = "file";
+    fs.create(new Path(FILE_KEY));
+    NativeAzureFileSystem nfs = (NativeAzureFileSystem) fs;
+    String fullKey = nfs.pathToKey(nfs.makeAbsolute(new Path(FILE_KEY)));
+    AzureNativeFileSystemStore store = nfs.getStore();
+    lease = store.acquireLease(fullKey);
+    assertTrue(lease.getLeaseID() != null);
+
+    // The sleep time for the keep-alive thread is 40 seconds, so sleep just
+    // a little beyond that, to make sure the keep-alive thread wakes up
+    // and renews the lease.
+    Thread.sleep(42000);
+    lease.free();
+
+    // Check that the lease is really freed.
+    CloudBlob blob = lease.getCloudBlob();
+
+    // Try to acquire it again, using direct Azure blob access.
+    // If that succeeds, then the lease was already freed.
+    String differentLeaseID = null;
+    try {
+      differentLeaseID = blob.acquireLease(15, null);
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail("Caught exception trying to directly re-acquire lease from Azure");
+    } finally {
+      assertTrue(differentLeaseID != null);
+      AccessCondition accessCondition = AccessCondition.generateEmptyCondition();
+      accessCondition.setLeaseID(differentLeaseID);
+      blob.releaseLease(accessCondition);
+    }
+  }
+
+  @Test
+  // Acquire a SelfRenewingLease object. Wait for more than the lease
+  // timeout, to make sure the lease renews itself. Delete the file.
+  // That will automatically free the lease.
+  // (that should work without any failures).
+  public void testSelfRenewingLeaseFileDelete()
+      throws IllegalArgumentException, IOException,
+        InterruptedException, StorageException {
+
+    SelfRenewingLease lease;
+    final String FILE_KEY = "file";
+    final Path path = new Path(FILE_KEY);
+    fs.create(path);
+    NativeAzureFileSystem nfs = (NativeAzureFileSystem) fs;
+    String fullKey = nfs.pathToKey(nfs.makeAbsolute(path));
+    lease = nfs.getStore().acquireLease(fullKey);
+    assertTrue(lease.getLeaseID() != null);
+
+    // The sleep time for the keep-alive thread is 40 seconds, so sleep just
+    // a little beyond that, to make sure the keep-alive thread wakes up
+    // and renews the lease.
+    Thread.sleep(42000);
+
+    nfs.getStore().delete(fullKey, lease);
+
+    // Check that the file is really gone and the lease is freed.
+    assertTrue(!fs.exists(path));
+    assertTrue(lease.isFreed());
+  }
+
+  // Variables to check assertions in next test.
+  private long firstEndTime;
+  private long secondStartTime;
+
+  // Create two threads. One will get a lease on a file.
+  // The second one will try to get the lease and thus block.
+  // Then the first one will free the lease and the second
+  // one will get it and proceed.
+  @Test
+  public void testLeaseAsDistributedLock() throws IllegalArgumentException,
+      IOException {
+    final String LEASE_LOCK_FILE_KEY = "file";
+    fs.create(new Path(LEASE_LOCK_FILE_KEY));
+    NativeAzureFileSystem nfs = (NativeAzureFileSystem) fs;
+    String fullKey = nfs.pathToKey(nfs.makeAbsolute(new Path(LEASE_LOCK_FILE_KEY)));
+
+    Thread first = new Thread(new LeaseLockAction("first-thread", fullKey));
+    first.start();
+    Thread second = new Thread(new LeaseLockAction("second-thread", fullKey));
+    second.start();
+    try {
+
+      // Wait for the two  threads to finish.
+      first.join();
+      second.join();
+      assertTrue(firstEndTime < secondStartTime);
+    } catch (InterruptedException e) {
+      fail("Unable to wait for threads to finish");
+      Thread.currentThread().interrupt();
+    }
+  }
+
+  private class LeaseLockAction implements Runnable {
+    private String name;
+    private String key;
+
+    LeaseLockAction(String name, String key) {
+      this.name = name;
+      this.key = key;
+    }
+
+    @Override
+    public void run() {
+      LOG.info("starting thread " + name);
+      SelfRenewingLease lease = null;
+      NativeAzureFileSystem nfs = (NativeAzureFileSystem) fs;
+
+      if (name.equals("first-thread")) {
+        try {
+          lease = nfs.getStore().acquireLease(key);
+          LOG.info(name + " acquired lease " + lease.getLeaseID());
+        } catch (AzureException e) {
+          assertTrue("Unanticipated exception", false);
+        }
+        assertTrue(lease != null);
+        try {
+
+          // Sleep long enough for the lease to renew once.
+          Thread.sleep(SelfRenewingLease.LEASE_RENEWAL_PERIOD + 2000);
+        } catch (InterruptedException e) {
+          Thread.currentThread().interrupt();
+        }
+        try {
+          firstEndTime = System.currentTimeMillis();
+          lease.free();
+          LOG.info(name + " freed lease " + lease.getLeaseID());
+        } catch (StorageException e) {
+          fail("Unanticipated exception");
+        }
+      } else if (name.equals("second-thread")) {
+        try {
+
+          // sleep 2 sec to let first thread get ahead of this one
+          Thread.sleep(2000);
+        } catch (InterruptedException e) {
+          Thread.currentThread().interrupt();
+        }
+        try {
+          LOG.info(name + " before getting lease");
+          lease = nfs.getStore().acquireLease(key);
+          secondStartTime = System.currentTimeMillis();
+          LOG.info(name + " acquired lease " + lease.getLeaseID());
+        } catch (AzureException e) {
+          assertTrue("Unanticipated exception", false);
+        }
+        assertTrue(lease != null);
+        try {
+          lease.free();
+          LOG.info(name + " freed lease " + lease.getLeaseID());
+        } catch (StorageException e) {
+          assertTrue("Unanticipated exception", false);
+        }
+      } else {
+        assertTrue("Unknown thread name", false);
+      }
+
+      LOG.info(name + " is exiting.");
+    }
+
+  }
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/RunningLiveWasbTests.txt b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/RunningLiveWasbTests.txt
new file mode 100644
index 0000000..54ba4d8
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/RunningLiveWasbTests.txt
@@ -0,0 +1,22 @@
+========================================================================
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+=========================================================================
+
+In order to run Windows Azure Storage Blob (WASB) unit tests against a live 
+Azure Storage account, you need to provide test account details in a configuration
+file called azure-test.xml. See hadoop-tools/hadoop-azure/README.txt for details
+on configuration, and how to run the tests.
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestAzureConcurrentOutOfBandIo.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestAzureConcurrentOutOfBandIo.java
index c10ac0f..0894cf5 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestAzureConcurrentOutOfBandIo.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestAzureConcurrentOutOfBandIo.java
@@ -22,11 +22,10 @@
 import static org.junit.Assert.fail;
 import static org.junit.Assume.assumeNotNull;
 
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
+import java.io.*;
 import java.util.Arrays;
 
+import org.apache.hadoop.fs.azure.AzureException;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.fs.permission.PermissionStatus;
 import org.junit.After;
@@ -100,13 +99,14 @@ public void stopWriting() {
     public void run() {
       byte[] dataBlockWrite = new byte[UPLOAD_BLOCK_SIZE];
 
-      DataOutputStream outputStream = null;
+      OutputStream outputStream = null;
 
       try {
         for (int i = 0; !done; i++) {
           // Write two 4 MB blocks to the blob.
           //
-          outputStream = writerStorageAccount.getStore().storefile(key,
+          outputStream = writerStorageAccount.getStore().storefile(
+              key,
               new PermissionStatus("", "", FsPermission.getDefault()));
 
           Arrays.fill(dataBlockWrite, (byte) (i % 256));
@@ -124,7 +124,7 @@ public void run() {
       } catch (IOException e) {
         System.out
             .println("DatablockWriter thread encountered an I/O exception."
-                + e.getMessage());
+            + e.getMessage());
       }
     }
   }
@@ -137,30 +137,29 @@ public void testReadOOBWrites() throws Exception {
 
     // Write to blob to make sure it exists.
     //
-    // Write five 4 MB blocks to the blob. To ensure there is data in the blob
-    // before reading. This eliminates the race between the reader and writer
-    // threads.
-    DataOutputStream outputStream = testAccount.getStore().storefile(
-        "WASB_String.txt",
-        new PermissionStatus("", "", FsPermission.getDefault()));
-    Arrays.fill(dataBlockWrite, (byte) 255);
-    for (int i = 0; i < NUMBER_OF_BLOCKS; i++) {
-      outputStream.write(dataBlockWrite);
-    }
-
-    outputStream.flush();
-    outputStream.close();
-
-    // Start writing blocks to Azure store using the DataBlockWriter thread.
+   // Write five 4 MB blocks to the blob. To ensure there is data in the blob before
+   // reading.  This eliminates the race between the reader and writer threads.
+   OutputStream outputStream = testAccount.getStore().storefile(
+       "WASB_String.txt",
+       new PermissionStatus("", "", FsPermission.getDefault()));
+   Arrays.fill(dataBlockWrite, (byte) 255);
+   for (int i = 0; i < NUMBER_OF_BLOCKS; i++) {
+     outputStream.write(dataBlockWrite);
+   }
+
+   outputStream.flush();
+   outputStream.close();
+
+   // Start writing blocks to Azure store using the DataBlockWriter thread.
     DataBlockWriter writeBlockTask = new DataBlockWriter(testAccount,
         "WASB_String.txt");
-    writeBlockTask.startWriting();
-    int count = 0;
-    DataInputStream inputStream = null;
+   writeBlockTask.startWriting();
+   int count = 0;
+   DataInputStream inputStream = null;
 
-    for (int i = 0; i < 5; i++) {
-      try {
-        inputStream = testAccount.getStore().retrieve("WASB_String.txt", 0);
+   for (int i = 0; i < 5; i++) {
+     try {
+        inputStream = testAccount.getStore().retrieve("WASB_String.txt");
         count = 0;
         int c = 0;
 
@@ -173,17 +172,17 @@ public void testReadOOBWrites() throws Exception {
           // Counting the number of bytes.
           count += c;
         }
-      } catch (IOException e) {
-        System.out.println(e.getCause().toString());
-        e.printStackTrace();
-        fail();
-      }
-
-      // Close the stream.
-      if (null != inputStream) {
-        inputStream.close();
-      }
-    }
+     } catch (IOException e) {
+       System.out.println(e.getCause().toString());
+       e.printStackTrace();
+       fail();
+     }
+
+     // Close the stream.
+     if (null != inputStream){
+       inputStream.close();
+     }
+   }
 
     // Stop writing blocks.
     writeBlockTask.stopWriting();
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestAzureFileSystemErrorConditions.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestAzureFileSystemErrorConditions.java
index 6e89822..febb605 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestAzureFileSystemErrorConditions.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestAzureFileSystemErrorConditions.java
@@ -32,7 +32,6 @@
 import java.util.HashMap;
 
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.TestHookOperationContext;
@@ -65,19 +64,18 @@ public void testNoInitialize() throws Exception {
    */
   @Test
   public void testAccessUnauthorizedPublicContainer() throws Exception {
-    Configuration conf = new Configuration();
-    AzureBlobStorageTestAccount.addWasbToConfiguration(conf);
     Path noAccessPath = new Path(
         "wasb://nonExistentContainer@hopefullyNonExistentAccount/someFile");
     NativeAzureFileSystem.suppressRetryPolicy();
     try {
-      FileSystem.get(noAccessPath.toUri(), conf).open(noAccessPath);
+      FileSystem.get(noAccessPath.toUri(), new Configuration())
+        .open(noAccessPath);
       assertTrue("Should've thrown.", false);
     } catch (AzureException ex) {
-      assertTrue("Unexpected message in exception " + ex, ex.getMessage()
-          .contains(
-              "Unable to access container nonExistentContainer in account"
-                  + " hopefullyNonExistentAccount"));
+      assertTrue("Unexpected message in exception " + ex,
+          ex.getMessage().contains(
+          "Unable to access container nonExistentContainer in account" +
+          " hopefullyNonExistentAccount"));
     } finally {
       NativeAzureFileSystem.resumeRetryPolicy();
     }
@@ -104,11 +102,11 @@ public void testAccessContainerWithWrongVersion() throws Exception {
         fs.listStatus(new Path("/"));
         passed = true;
       } catch (AzureException ex) {
-        assertTrue("Unexpected exception message: " + ex, ex.getMessage()
-            .contains("unsupported version: 2090-04-05."));
+        assertTrue("Unexpected exception message: " + ex,
+            ex.getMessage().contains("unsupported version: 2090-04-05."));
       }
-      assertFalse(
-          "Should've thrown an exception because of the wrong version.", passed);
+      assertFalse("Should've thrown an exception because of the wrong version.",
+          passed);
     } finally {
       fs.close();
     }
@@ -118,8 +116,7 @@ public void testAccessContainerWithWrongVersion() throws Exception {
     boolean isTargetConnection(HttpURLConnection connection);
   }
 
-  private class TransientErrorInjector extends
-      StorageEvent<SendingRequestEvent> {
+  private class TransientErrorInjector extends StorageEvent<SendingRequestEvent> {
     final ConnectionRecognizer connectionRecognizer;
     private boolean injectedErrorOnce = false;
 
@@ -129,8 +126,7 @@ public TransientErrorInjector(ConnectionRecognizer connectionRecognizer) {
 
     @Override
     public void eventOccurred(SendingRequestEvent eventArg) {
-      HttpURLConnection connection = (HttpURLConnection) eventArg
-          .getConnectionObject();
+      HttpURLConnection connection = (HttpURLConnection)eventArg.getConnectionObject();
       if (!connectionRecognizer.isTargetConnection(connection)) {
         return;
       }
@@ -157,8 +153,8 @@ public OperationContext modifyOperationContext(OperationContext original) {
   @Test
   public void testTransientErrorOnDelete() throws Exception {
     // Need to do this test against a live storage account
-    AzureBlobStorageTestAccount testAccount = AzureBlobStorageTestAccount
-        .create();
+    AzureBlobStorageTestAccount testAccount =
+        AzureBlobStorageTestAccount.create();
     assumeNotNull(testAccount);
     try {
       NativeAzureFileSystem fs = testAccount.getFileSystem();
@@ -179,7 +175,7 @@ public boolean isTargetConnection(HttpURLConnection connection) {
   private void writeAllThreeFile(NativeAzureFileSystem fs, Path testFile)
       throws IOException {
     byte[] buffer = new byte[ALL_THREE_FILE_SIZE];
-    Arrays.fill(buffer, (byte) 3);
+    Arrays.fill(buffer, (byte)3);
     OutputStream stream = fs.create(testFile);
     stream.write(buffer);
     stream.close();
@@ -189,7 +185,8 @@ private void readAllThreeFile(NativeAzureFileSystem fs, Path testFile)
       throws IOException {
     byte[] buffer = new byte[ALL_THREE_FILE_SIZE];
     InputStream inStream = fs.open(testFile);
-    assertEquals(buffer.length, inStream.read(buffer, 0, buffer.length));
+    assertEquals(buffer.length,
+        inStream.read(buffer, 0, buffer.length));
     inStream.close();
     for (int i = 0; i < buffer.length; i++) {
       assertEquals(3, buffer[i]);
@@ -199,8 +196,8 @@ private void readAllThreeFile(NativeAzureFileSystem fs, Path testFile)
   @Test
   public void testTransientErrorOnCommitBlockList() throws Exception {
     // Need to do this test against a live storage account
-    AzureBlobStorageTestAccount testAccount = AzureBlobStorageTestAccount
-        .create();
+    AzureBlobStorageTestAccount testAccount =
+        AzureBlobStorageTestAccount.create();
     assumeNotNull(testAccount);
     try {
       NativeAzureFileSystem fs = testAccount.getFileSystem();
@@ -222,8 +219,8 @@ public boolean isTargetConnection(HttpURLConnection connection) {
   @Test
   public void testTransientErrorOnRead() throws Exception {
     // Need to do this test against a live storage account
-    AzureBlobStorageTestAccount testAccount = AzureBlobStorageTestAccount
-        .create();
+    AzureBlobStorageTestAccount testAccount =
+        AzureBlobStorageTestAccount.create();
     assumeNotNull(testAccount);
     try {
       NativeAzureFileSystem fs = testAccount.getFileSystem();
@@ -240,16 +237,4 @@ public boolean isTargetConnection(HttpURLConnection connection) {
       testAccount.cleanup();
     }
   }
-  
-  // Tests an error during stream creation (in this case in the seek() implementation
-  // to verify the close-stream-on-error logic.
-  @Test (expected=AzureException.class)
-  public void testErrorDuringRetrieve() throws Exception {
-    NativeAzureFileSystem fs = AzureBlobStorageTestAccount.createMock().getFileSystem();
-    Path testFile = new Path("/testErrorDuringRetrieve");
-    writeAllThreeFile(fs, testFile);
-
-    FSDataInputStream stream = fs.open(testFile);
-    stream.seek(Integer.MAX_VALUE);
-  }
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobDataValidation.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobDataValidation.java
index b585c56..25bd338 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobDataValidation.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobDataValidation.java
@@ -128,7 +128,7 @@ private void testStoreBlobMd5(boolean expectMd5Stored) throws Exception {
       if (!expectMd5Stored) {
         throw ex;
       }
-      StorageException cause = (StorageException) ex.getCause();
+      StorageException cause = (StorageException)ex.getCause();
       assertNotNull(cause);
       assertTrue("Unexpected cause: " + cause,
           cause.getErrorCode().equals(StorageErrorCodeStrings.INVALID_MD5));
@@ -212,13 +212,13 @@ private void testCheckBlockMd5(final boolean expectMd5Checked)
     // validate the data as expected, but the HttpURLConnection wasn't
     // pluggable enough for me to do that.
     testAccount.getFileSystem().getStore()
-        .addTestHookToOperationContext(new TestHookOperationContext() {
-          @Override
+    .addTestHookToOperationContext(new TestHookOperationContext() {
+    @Override
           public OperationContext modifyOperationContext(
               OperationContext original) {
-            original.getResponseReceivedEventHandler().addListener(
-                new ContentMD5Checker(expectMd5Checked));
-            return original;
+      original.getResponseReceivedEventHandler().addListener(
+          new ContentMD5Checker(expectMd5Checked));
+      return original;
           }
         });
 
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobMetadata.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobMetadata.java
index b75fc38..6c49926 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobMetadata.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobMetadata.java
@@ -69,7 +69,8 @@ private static String getExpectedPermissionString(String permissionString)
       throws Exception {
     return String.format(
         "{\"owner\":\"%s\",\"group\":\"%s\",\"permissions\":\"%s\"}",
-        getExpectedOwner(), NativeAzureFileSystem.AZURE_DEFAULT_GROUP_DEFAULT,
+        getExpectedOwner(),
+        NativeAzureFileSystem.AZURE_DEFAULT_GROUP_DEFAULT,
         permissionString);
   }
 
@@ -80,8 +81,8 @@ private static String getExpectedPermissionString(String permissionString)
   public void testContainerVersionMetadata() throws Exception {
     // Do a write operation to trigger version stamp
     fs.createNewFile(new Path("/foo"));
-    HashMap<String, String> containerMetadata = backingStore
-        .getContainerMetadata();
+    HashMap<String, String> containerMetadata =
+        backingStore.getContainerMetadata();
     assertNotNull(containerMetadata);
     assertEquals(AzureNativeFileSystemStore.CURRENT_WASB_VERSION,
         containerMetadata.get(AzureNativeFileSystemStore.VERSION_METADATA_KEY));
@@ -226,26 +227,32 @@ public void testPermissionMetadata() throws Exception {
   @Test
   public void testOldPermissionMetadata() throws Exception {
     Path selfishFile = new Path("/noOneElse");
-    HashMap<String, String> metadata = new HashMap<String, String>();
-    metadata.put("asv_permission", getExpectedPermissionString("rw-------"));
-    backingStore.setContent(AzureBlobStorageTestAccount.toMockUri(selfishFile),
-        new byte[] {}, metadata);
-    FsPermission justMe = new FsPermission(FsAction.READ_WRITE, FsAction.NONE,
-        FsAction.NONE);
+    HashMap<String, String> metadata =
+        new HashMap<String, String>();
+    metadata.put("asv_permission",
+        getExpectedPermissionString("rw-------"));
+    backingStore.setContent(
+        AzureBlobStorageTestAccount.toMockUri(selfishFile),
+        new byte[] { },
+        metadata, false, 0);
+    FsPermission justMe = new FsPermission(
+        FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);
     FileStatus retrievedStatus = fs.getFileStatus(selfishFile);
     assertNotNull(retrievedStatus);
     assertEquals(justMe, retrievedStatus.getPermission());
     assertEquals(getExpectedOwner(), retrievedStatus.getOwner());
     assertEquals(NativeAzureFileSystem.AZURE_DEFAULT_GROUP_DEFAULT,
         retrievedStatus.getGroup());
-    FsPermission meAndYou = new FsPermission(FsAction.READ_WRITE,
-        FsAction.READ_WRITE, FsAction.NONE);
+    FsPermission meAndYou = new FsPermission(
+        FsAction.READ_WRITE, FsAction.READ_WRITE, FsAction.NONE);
     fs.setPermission(selfishFile, meAndYou);
-    metadata = backingStore.getMetadata(AzureBlobStorageTestAccount
-        .toMockUri(selfishFile));
+    metadata =
+        backingStore.getMetadata(
+            AzureBlobStorageTestAccount.toMockUri(selfishFile));
     assertNotNull(metadata);
     String storedPermission = metadata.get("hdi_permission");
-    assertEquals(getExpectedPermissionString("rw-rw----"), storedPermission);
+    assertEquals(getExpectedPermissionString("rw-rw----"),
+        storedPermission);
     assertNull(metadata.get("asv_permission"));
   }
 
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobTypeSpeedDifference.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobTypeSpeedDifference.java
new file mode 100644
index 0000000..afb16ef
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobTypeSpeedDifference.java
@@ -0,0 +1,160 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs.azure;
+
+import java.io.*;
+import java.util.*;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation;
+
+import junit.framework.*;
+
+import org.junit.Test;
+
+
+/**
+ * A simple benchmark to find out the difference in speed between block
+ * and page blobs.
+ */
+public class TestBlobTypeSpeedDifference extends TestCase {
+  /**
+   * Writes data to the given stream of the given size, flushing every
+   * x bytes.
+   */
+  private static void writeTestFile(OutputStream writeStream,
+      long size, long flushInterval) throws IOException {
+    int bufferSize = (int) Math.min(1000, flushInterval);
+    byte[] buffer = new byte[bufferSize];
+    Arrays.fill(buffer, (byte) 7);
+    int bytesWritten = 0;
+    int bytesUnflushed = 0;
+    while (bytesWritten < size) {
+      int numberToWrite = (int) Math.min(bufferSize, size - bytesWritten);
+      writeStream.write(buffer, 0, numberToWrite);
+      bytesWritten += numberToWrite;
+      bytesUnflushed += numberToWrite;
+      if (bytesUnflushed >= flushInterval) {
+        writeStream.flush();
+        bytesUnflushed = 0;
+      }
+    }
+  }
+
+  private static class TestResult {
+    final long timeTakenInMs;
+    final long totalNumberOfRequests;
+
+    TestResult(long timeTakenInMs, long totalNumberOfRequests) {
+      this.timeTakenInMs = timeTakenInMs;
+      this.totalNumberOfRequests = totalNumberOfRequests;
+    }
+  }
+
+  /**
+   * Writes data to the given file of the given size, flushing every
+   * x bytes. Measure performance of that and return it.
+   */
+  private static TestResult writeTestFile(NativeAzureFileSystem fs, Path path,
+      long size, long flushInterval) throws IOException {
+    AzureFileSystemInstrumentation instrumentation =
+        fs.getInstrumentation();
+    long initialRequests = instrumentation.getCurrentWebResponses();
+    Date start = new Date();
+    OutputStream output = fs.create(path);
+    writeTestFile(output, size, flushInterval);
+    output.close();
+    long finalRequests = instrumentation.getCurrentWebResponses();
+    return new TestResult(new Date().getTime() - start.getTime(),
+        finalRequests - initialRequests);
+  }
+
+  /**
+   * Writes data to a block blob of the given size, flushing every
+   * x bytes. Measure performance of that and return it.
+   */
+  private static TestResult writeBlockBlobTestFile(NativeAzureFileSystem fs,
+      long size, long flushInterval) throws IOException {
+    return writeTestFile(fs, new Path("/blockBlob"), size, flushInterval);
+  }
+
+  /**
+   * Writes data to a page blob of the given size, flushing every
+   * x bytes. Measure performance of that and return it.
+   */
+  private static TestResult writePageBlobTestFile(NativeAzureFileSystem fs,
+      long size, long flushInterval) throws IOException {
+    return writeTestFile(fs,
+        AzureBlobStorageTestAccount.pageBlobPath("pageBlob"),
+        size, flushInterval);
+  }
+
+  /**
+   * Runs the benchmark over a small 10 KB file, flushing every 500 bytes.
+   */
+  @Test
+  public void testTenKbFileFrequentFlush() throws Exception {
+    AzureBlobStorageTestAccount testAccount =
+        AzureBlobStorageTestAccount.create();
+    if (testAccount == null) {
+      return;
+    }
+    try {
+      testForSizeAndFlushInterval(testAccount.getFileSystem(), 10 * 1000, 500);
+    } finally {
+      testAccount.cleanup();
+    }
+  }
+
+  /**
+   * Runs the benchmark for the given file size and flush frequency.
+   */
+  private static void testForSizeAndFlushInterval(NativeAzureFileSystem fs,
+      final long size, final long flushInterval) throws IOException {
+    for (int i = 0; i < 5; i++) {
+      TestResult pageBlobResults = writePageBlobTestFile(fs, size, flushInterval);
+      System.out.printf(
+          "Page blob upload took %d ms. Total number of requests: %d.\n",
+          pageBlobResults.timeTakenInMs, pageBlobResults.totalNumberOfRequests);
+      TestResult blockBlobResults = writeBlockBlobTestFile(fs, size, flushInterval);
+      System.out.printf(
+          "Block blob upload took %d ms. Total number of requests: %d.\n",
+          blockBlobResults.timeTakenInMs, blockBlobResults.totalNumberOfRequests);
+    }
+  }
+
+  /**
+   * Runs the benchmark for the given file size and flush frequency from the
+   * command line.
+   */
+  public static void main(String argv[]) throws Exception {
+    Configuration conf = new Configuration();
+    long size = 10 * 1000 * 1000;
+    long flushInterval = 2000;
+    if (argv.length > 0) {
+      size = Long.parseLong(argv[0]);
+    }
+    if (argv.length > 1) {
+      flushInterval = Long.parseLong(argv[1]);
+    }
+    testForSizeAndFlushInterval((NativeAzureFileSystem)FileSystem.get(conf),
+        size, flushInterval);
+  }
+}
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFSPageBlobLive.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFSPageBlobLive.java
new file mode 100644
index 0000000..208cff3
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFSPageBlobLive.java
@@ -0,0 +1,43 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.fs.azure;
+
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ * Run the base Azure file system tests strictly on page blobs to make sure fundamental
+ * operations on page blob files and folders work as expected.
+ * These operations include create, delete, rename, list, and so on.
+ */
+public class TestNativeAzureFSPageBlobLive extends
+    NativeAzureFileSystemBaseTest {
+
+  @Override
+  protected AzureBlobStorageTestAccount createTestAccount()
+      throws Exception {
+    Configuration conf = new Configuration();
+
+    // Configure the page blob directories key so every file created is a page blob.
+    conf.set(AzureNativeFileSystemStore.KEY_PAGE_BLOB_DIRECTORIES, "/");
+
+    // Configure the atomic rename directories key so every folder will have
+    // atomic rename applied.
+    conf.set(AzureNativeFileSystemStore.KEY_ATOMIC_RENAME_DIRECTORIES, "/");
+    return AzureBlobStorageTestAccount.create(conf);
+  }
+}
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemConcurrency.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemConcurrency.java
index 5583fec..cbfc563 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemConcurrency.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemConcurrency.java
@@ -26,6 +26,7 @@
 import java.io.OutputStream;
 import java.io.PrintWriter;
 import java.io.StringWriter;
+import java.net.URLDecoder;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.concurrent.ConcurrentLinkedQueue;
@@ -67,8 +68,8 @@ public void testLinkBlobs() throws Exception {
     HashMap<String, String> metadata = backingStore
         .getMetadata(AzureBlobStorageTestAccount.toMockUri(filePath));
     assertNotNull(metadata);
-    String linkValue = metadata
-        .get(AzureNativeFileSystemStore.LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY);
+    String linkValue = metadata.get(AzureNativeFileSystemStore.LINK_BACK_TO_UPLOAD_IN_PROGRESS_METADATA_KEY);
+    linkValue = URLDecoder.decode(linkValue, "UTF-8");
     assertNotNull(linkValue);
     assertTrue(backingStore.exists(AzureBlobStorageTestAccount
         .toMockUri(linkValue)));
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemContractLive.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemContractLive.java
index 03292f3..0d7b9ad 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemContractLive.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemContractLive.java
@@ -19,6 +19,7 @@
 package org.apache.hadoop.fs.azure;
 
 import org.apache.hadoop.fs.FileSystemContractBaseTest;
+import org.junit.Ignore;
 
 public class TestNativeAzureFileSystemContractLive extends
     FileSystemContractBaseTest {
@@ -47,4 +48,29 @@ protected void runTest() throws Throwable {
       super.runTest();
     }
   }
+  
+  /**
+   * The following tests are failing on Azure and the Azure 
+   * file system code needs to be modified to make them pass.
+   * A separate work item has been opened for this.
+   */
+  @Ignore
+  public void testMoveFileUnderParent() throws Throwable {
+  }
+
+  @Ignore
+  public void testRenameFileToSelf() throws Throwable {
+  }
+  
+  @Ignore
+  public void testRenameChildDirForbidden() throws Exception {
+  }
+  
+  @Ignore
+  public void testMoveDirUnderParent() throws Throwable {
+  }
+  
+  @Ignore
+  public void testRenameDirToSelf() throws Throwable {
+  }
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemContractMocked.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemContractMocked.java
index f25055b..6d3df25 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemContractMocked.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemContractMocked.java
@@ -19,6 +19,7 @@
 package org.apache.hadoop.fs.azure;
 
 import org.apache.hadoop.fs.FileSystemContractBaseTest;
+import org.junit.Ignore;
 
 public class TestNativeAzureFileSystemContractMocked extends
     FileSystemContractBaseTest {
@@ -27,5 +28,29 @@
   protected void setUp() throws Exception {
     fs = AzureBlobStorageTestAccount.createMock().getFileSystem();
   }
+  
+  /**
+   * The following tests are failing on Azure and the Azure 
+   * file system code needs to be modified to make them pass.
+   * A separate work item has been opened for this.
+   */
+  @Ignore
+  public void testMoveFileUnderParent() throws Throwable {
+  }
 
+  @Ignore
+  public void testRenameFileToSelf() throws Throwable {
+  }
+  
+  @Ignore
+  public void testRenameChildDirForbidden() throws Exception {
+  }
+  
+  @Ignore
+  public void testMoveDirUnderParent() throws Throwable {
+  }
+  
+  @Ignore
+  public void testRenameDirToSelf() throws Throwable {
+  }
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemFileNameCheck.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemFileNameCheck.java
index d7ff0c7..82eabaa 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemFileNameCheck.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemFileNameCheck.java
@@ -123,11 +123,12 @@ public void testWasbFsck() throws Exception {
     assertFalse(runWasbFsck(testFolder1));
 
     // negative test
-    InMemoryBlockBlobStore backingStore = testAccount.getMockStorage()
-        .getBackingStore();
-    backingStore.setContent(AzureBlobStorageTestAccount
-        .toMockUri("testFolder1/testFolder2/test2:2"), new byte[] { 1, 2 },
-        new HashMap<String, String>());
+    InMemoryBlockBlobStore backingStore
+        = testAccount.getMockStorage().getBackingStore();
+    backingStore.setContent(
+        AzureBlobStorageTestAccount.toMockUri("testFolder1/testFolder2/test2:2"),
+        new byte[] { 1, 2 },
+        new HashMap<String, String>(), false, 0);
     assertTrue(runWasbFsck(testFolder1));
   }
 
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemLive.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemLive.java
index c82cee3..985c0e9d 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemLive.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemLive.java
@@ -18,6 +18,12 @@
 
 package org.apache.hadoop.fs.azure;
 
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import org.junit.Test;
+
 /*
  * Tests the Native Azure file system (WASB) against an actual blob store if
  * provided in the environment.
@@ -29,4 +35,73 @@
   protected AzureBlobStorageTestAccount createTestAccount() throws Exception {
     return AzureBlobStorageTestAccount.create();
   }
+
+  /**
+   * Check that isPageBlobKey works as expected. This assumes that
+   * in the test configuration, the list of supported page blob directories
+   * only includes "pageBlobs". That's why this test is made specific
+   * to this subclass.
+   */
+  @Test
+  public void testIsPageBlobKey() {
+    AzureNativeFileSystemStore store = ((NativeAzureFileSystem) fs).getStore();
+
+    // Use literal strings so it's easier to understand the tests.
+    // In case the constant changes, we want to know about it so we can update this test.
+    assertEquals(AzureBlobStorageTestAccount.DEFAULT_PAGE_BLOB_DIRECTORY, "pageBlobs");
+
+    // URI prefix for test environment.
+    String uriPrefix = "file:///";
+
+    // negative tests
+    String[] negativeKeys = { "", "/", "bar", "bar/", "bar/pageBlobs", "bar/pageBlobs/foo",
+        "bar/pageBlobs/foo/", "/pageBlobs/", "/pageBlobs", "pageBlobs", "pageBlobsxyz/" };
+    for (String s : negativeKeys) {
+      assertFalse(store.isPageBlobKey(s));
+      assertFalse(store.isPageBlobKey(uriPrefix + s));
+    }
+
+    // positive tests
+    String[] positiveKeys = { "pageBlobs/", "pageBlobs/foo/", "pageBlobs/foo/bar/" };
+    for (String s : positiveKeys) {
+      assertTrue(store.isPageBlobKey(s));
+      assertTrue(store.isPageBlobKey(uriPrefix + s));
+    }
+  }
+
+  /**
+   * Test that isAtomicRenameKey() works as expected.
+   */
+  @Test
+  public void testIsAtomicRenameKey() {
+
+    AzureNativeFileSystemStore store = ((NativeAzureFileSystem) fs).getStore();
+
+    // We want to know if the default configuration changes so we can fix
+    // this test.
+    assertEquals(AzureBlobStorageTestAccount.DEFAULT_ATOMIC_RENAME_DIRECTORIES,
+        "/atomicRenameDir1,/atomicRenameDir2");
+
+    // URI prefix for test environment.
+    String uriPrefix = "file:///";
+
+    // negative tests
+    String[] negativeKeys = { "", "/", "bar", "bar/", "bar/hbase",
+        "bar/hbase/foo", "bar/hbase/foo/", "/hbase/", "/hbase", "hbase",
+        "hbasexyz/", "foo/atomicRenameDir1/"};
+    for (String s : negativeKeys) {
+      assertFalse(store.isAtomicRenameKey(s));
+      assertFalse(store.isAtomicRenameKey(uriPrefix + s));
+    }
+
+    // Positive tests. The directories for atomic rename are /hbase
+    // plus the ones in the configuration (DEFAULT_ATOMIC_RENAME_DIRECTORIES
+    // for this test).
+    String[] positiveKeys = { "hbase/", "hbase/foo/", "hbase/foo/bar/",
+        "atomicRenameDir1/foo/", "atomicRenameDir2/bar/"};
+    for (String s : positiveKeys) {
+      assertTrue(store.isAtomicRenameKey(s));
+      assertTrue(store.isAtomicRenameKey(uriPrefix + s));
+    }
+  }
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemMocked.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemMocked.java
index 9819c18..aa1e4f7 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemMocked.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemMocked.java
@@ -18,6 +18,9 @@
 
 package org.apache.hadoop.fs.azure;
 
+import java.io.IOException;
+import org.junit.Ignore;
+
 public class TestNativeAzureFileSystemMocked extends
     NativeAzureFileSystemBaseTest {
 
@@ -25,4 +28,36 @@
   protected AzureBlobStorageTestAccount createTestAccount() throws Exception {
     return AzureBlobStorageTestAccount.createMock();
   }
+
+  // Ignore the following tests because taking a lease requires a real
+  // (not mock) file system store. These tests don't work on the mock.
+  @Override
+  @Ignore
+  public void testLeaseAsDistributedLock() {
+  }
+
+  @Override
+  @Ignore
+  public void testSelfRenewingLease() {
+  }
+
+  @Override
+  @Ignore
+  public void testRedoFolderRenameAll() {
+  }
+
+  @Override
+  @Ignore
+  public void testCreateNonRecursive() {
+  }
+
+  @Override
+  @Ignore
+  public void testSelfRenewingLeaseFileDelete() {
+  }
+
+  @Override
+  @Ignore
+  public void testRenameRedoFolderAlreadyDone() throws IOException{
+  }
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemOperationsMocked.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemOperationsMocked.java
index a6e782a..f01829b 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemOperationsMocked.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemOperationsMocked.java
@@ -27,8 +27,16 @@
 public class TestNativeAzureFileSystemOperationsMocked extends
     FSMainOperationsBaseTest {
 
-  public TestNativeAzureFileSystemOperationsMocked() {
-    super("/tmp/TestNativeAzureFileSystemOperationsMocked");
+  private static final String TEST_ROOT_DIR =
+      "/tmp/TestNativeAzureFileSystemOperationsMocked";
+  
+  public TestNativeAzureFileSystemOperationsMocked (){
+    super(TEST_ROOT_DIR);
+  }
+
+  @Override
+  public void setUp() throws Exception {
+    fSys = AzureBlobStorageTestAccount.createMock().getFileSystem();
   }
 
   @Override
@@ -42,4 +50,29 @@ public void testListStatusThrowsExceptionForUnreadableDir() throws Exception {
             + " doesn't honor directory permissions.");
     assumeTrue(!Path.WINDOWS);
   }
+
+  @Override
+  public String getTestRootDir() {
+    return TEST_ROOT_DIR;
+  }
+
+  @Override
+  public Path getTestRootPath(FileSystem fSys) {
+    return fSys.makeQualified(new Path(TEST_ROOT_DIR));
+  }
+
+  @Override
+  public Path getTestRootPath(FileSystem fSys, String pathString) {
+    return fSys.makeQualified(new Path(TEST_ROOT_DIR, pathString));
+  }
+
+  @Override
+  public Path getAbsoluteTestRootPath(FileSystem fSys) {
+    Path testRootPath = new Path(TEST_ROOT_DIR);
+    if (testRootPath.isAbsolute()) {
+      return testRootPath;
+    } else {
+      return new Path(fSys.getWorkingDirectory(), TEST_ROOT_DIR);
+    }
+  }
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemUploadLogic.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemUploadLogic.java
new file mode 100644
index 0000000..4c2df8d
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemUploadLogic.java
@@ -0,0 +1,186 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs.azure;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
+
+import java.io.ByteArrayInputStream;
+import java.io.InputStream;
+import java.io.OutputStream;
+
+import org.apache.hadoop.fs.Path;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Ignore;
+import org.junit.Test;
+
+/**
+ * Tests for the upload, buffering and flush logic in WASB.
+ */
+public class TestNativeAzureFileSystemUploadLogic {
+  private AzureBlobStorageTestAccount testAccount;
+
+  // Just an arbitrary number so that the values I write have a predictable
+  // pattern: 0, 1, 2, .. , 45, 46, 0, 1, 2, ...
+  static final int byteValuePeriod = 47;
+
+  @Before
+  public void setUp() throws Exception {
+    testAccount = AzureBlobStorageTestAccount.createMock();
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    if (testAccount != null) {
+      testAccount.cleanup();
+      testAccount = null;
+    }
+  }
+
+  /**
+   * Various scenarios to test in how often we flush data while uploading.
+   */
+  private enum FlushFrequencyVariation {
+    /**
+     * Flush before even a single in-memory buffer is full.
+     */
+    BeforeSingleBufferFull,
+    /**
+     * Flush after a single in-memory buffer is full.
+     */
+    AfterSingleBufferFull,
+    /**
+     * Flush after all the in-memory buffers got full and were
+     * automatically flushed to the backing store.
+     */
+    AfterAllRingBufferFull,
+  }
+
+  /**
+   * Tests that we upload consistently if we flush after every little
+   * bit of data.
+   */
+  @Test
+  @Ignore /* flush() no longer does anything. @@TODO: implement a force-flush and reinstate this test */
+  public void testConsistencyAfterSmallFlushes() throws Exception {
+    testConsistencyAfterManyFlushes(FlushFrequencyVariation.BeforeSingleBufferFull);
+  }
+
+  /**
+   * Tests that we upload consistently if we flush after every medium-sized
+   * bit of data.
+   */
+  @Test
+  @Ignore /* flush() no longer does anything. @@TODO: implement a force-flush and reinstate this test */
+  public void testConsistencyAfterMediumFlushes() throws Exception {
+    testConsistencyAfterManyFlushes(FlushFrequencyVariation.AfterSingleBufferFull);
+  }
+
+  /**
+   * Tests that we upload consistently if we flush after every large chunk
+   * of data.
+   */
+  @Test
+  @Ignore /* flush() no longer does anything. @@TODO: implement a force-flush and reinstate this test */
+  public void testConsistencyAfterLargeFlushes() throws Exception {
+    testConsistencyAfterManyFlushes(FlushFrequencyVariation.AfterAllRingBufferFull);
+  }
+
+  /**
+   * Makes sure the data in the given input is what I'd expect.
+   * @param inStream The input stream.
+   * @param expectedSize The expected size of the data in there.
+   */
+  private void assertDataInStream(InputStream inStream, int expectedSize)
+      throws Exception {
+    int byteRead;
+    int countBytes = 0;
+    while ((byteRead = inStream.read()) != -1) {
+      assertEquals(countBytes % byteValuePeriod, byteRead);
+      countBytes++;
+    }
+    assertEquals(expectedSize, countBytes);
+  }
+
+  /**
+   * Checks that the data in the given file is what I'd expect.
+   * @param file The file to check.
+   * @param expectedSize The expected size of the data in there.
+   */
+  private void assertDataInFile(Path file, int expectedSize) throws Exception {
+    InputStream inStream = testAccount.getFileSystem().open(file);
+    assertDataInStream(inStream, expectedSize);
+    inStream.close();
+  }
+
+  /**
+   * Checks that the data in the current temporary upload blob
+   * is what I'd expect.
+   * @param expectedSize The expected size of the data in there.
+   */
+  private void assertDataInTempBlob(int expectedSize) throws Exception {
+    // Look for the temporary upload blob in the backing store.
+    InMemoryBlockBlobStore backingStore =
+        testAccount.getMockStorage().getBackingStore();
+    String tempKey = null;
+    for (String key : backingStore.getKeys()) {
+      if (key.contains(NativeAzureFileSystem.AZURE_TEMP_FOLDER)) {
+        // Assume this is the one we're looking for.
+        tempKey = key;
+        break;
+      }
+    }
+    assertNotNull(tempKey);
+    InputStream inStream = new ByteArrayInputStream(backingStore.getContent(tempKey));
+    assertDataInStream(inStream, expectedSize);
+    inStream.close();
+  }
+
+  /**
+   * Tests the given scenario for uploading a file while flushing
+   * periodically and making sure the data is always consistent
+   * with what I'd expect.
+   * @param variation The variation/scenario to test.
+   */
+  private void testConsistencyAfterManyFlushes(FlushFrequencyVariation variation)
+      throws Exception {
+    Path uploadedFile = new Path("/uploadedFile");
+    OutputStream outStream = testAccount.getFileSystem().create(uploadedFile);
+    final int totalSize = 9123;
+    int flushPeriod;
+    switch (variation) {
+      case BeforeSingleBufferFull: flushPeriod = 300; break;
+      case AfterSingleBufferFull: flushPeriod = 600; break;
+      case AfterAllRingBufferFull: flushPeriod = 1600; break;
+      default:
+        throw new IllegalArgumentException("Unknown variation: " + variation);
+    }
+    for (int i = 0; i < totalSize; i++) {
+      outStream.write(i % byteValuePeriod);
+      if ((i + 1) % flushPeriod == 0) {
+        outStream.flush();
+        assertDataInTempBlob(i + 1);
+      }
+    }
+    outStream.close();
+    assertDataInFile(uploadedFile, totalSize);
+  }
+}
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestOutOfBandAzureBlobOperations.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestOutOfBandAzureBlobOperations.java
index b49f6ee..9d5d6a2 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestOutOfBandAzureBlobOperations.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestOutOfBandAzureBlobOperations.java
@@ -57,10 +57,14 @@ public void tearDown() throws Exception {
   }
 
   private void createEmptyBlobOutOfBand(String path) {
-    backingStore.setContent(AzureBlobStorageTestAccount.toMockUri(path),
-        new byte[] { 1, 2 }, new HashMap<String, String>());
+    backingStore.setContent(
+        AzureBlobStorageTestAccount.toMockUri(path),
+        new byte[] { 1, 2 },
+        new HashMap<String, String>(),
+        false, 0);
   }
 
+  @SuppressWarnings("deprecation")
   @Test
   public void testImplicitFolderListed() throws Exception {
     createEmptyBlobOutOfBand("root/b");
@@ -69,20 +73,20 @@ public void testImplicitFolderListed() throws Exception {
     FileStatus[] obtained = fs.listStatus(new Path("/root/b"));
     assertNotNull(obtained);
     assertEquals(1, obtained.length);
-    assertFalse(obtained[0].isDirectory());
+    assertFalse(obtained[0].isDir());
     assertEquals("/root/b", obtained[0].getPath().toUri().getPath());
 
     // List the directory
     obtained = fs.listStatus(new Path("/root"));
     assertNotNull(obtained);
     assertEquals(1, obtained.length);
-    assertFalse(obtained[0].isDirectory());
+    assertFalse(obtained[0].isDir());
     assertEquals("/root/b", obtained[0].getPath().toUri().getPath());
 
     // Get the directory's file status
     FileStatus dirStatus = fs.getFileStatus(new Path("/root"));
     assertNotNull(dirStatus);
-    assertTrue(dirStatus.isDirectory());
+    assertTrue(dirStatus.isDir());
     assertEquals("/root", dirStatus.getPath().toUri().getPath());
   }
 
@@ -102,6 +106,7 @@ public void testFileInImplicitFolderDeleted() throws Exception {
     assertTrue(fs.exists(new Path("/root")));
   }
 
+  @SuppressWarnings("deprecation")
   @Test
   public void testFileAndImplicitFolderSameName() throws Exception {
     createEmptyBlobOutOfBand("root/b");
@@ -109,7 +114,7 @@ public void testFileAndImplicitFolderSameName() throws Exception {
     FileStatus[] listResult = fs.listStatus(new Path("/root/b"));
     // File should win.
     assertEquals(1, listResult.length);
-    assertFalse(listResult[0].isDirectory());
+    assertFalse(listResult[0].isDir());
     try {
       // Trying to delete root/b/c would cause a dilemma for WASB, so
       // it should throw.
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestOutOfBandAzureBlobOperationsLive.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestOutOfBandAzureBlobOperationsLive.java
index 1855c3b..9ac67e7 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestOutOfBandAzureBlobOperationsLive.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestOutOfBandAzureBlobOperationsLive.java
@@ -163,6 +163,27 @@ public void outOfBandFolder_rename() throws Exception {
     fs.rename(srcFilePath, destFilePath);
   }
 
+  // Verify that you can rename a file which is the only file in an implicit folder in the
+  // WASB file system.
+  // scenario for this particular test described at MONARCH-HADOOP-892
+  @Test
+  public void outOfBandSingleFile_rename() throws Exception {
+
+    //NOTE: manual use of CloubBlockBlob targets working directory explicitly.
+    //       WASB driver methods prepend working directory implicitly.
+    String workingDir = "user/" + UserGroupInformation.getCurrentUser().getShortUserName() + "/";
+    CloudBlockBlob blob = testAccount.getBlobReference(workingDir + "testFolder5/a/input/file");
+    BlobOutputStream s = blob.openOutputStream();
+    s.close();
+
+    Path srcFilePath = new Path("testFolder5/a/input/file");
+    assertTrue(fs.exists(srcFilePath));
+
+    Path destFilePath = new Path("testFolder5/file2");
+    fs.rename(srcFilePath, destFilePath);
+  }
+
+  // WASB must force explicit parent directories in create, delete, mkdirs, rename.
   // scenario for this particular test described at MONARCH-HADOOP-764
   @Test
   public void outOfBandFolder_rename_rootLevelFiles() throws Exception {
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestReadAndSeekPageBlobAfterWrite.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestReadAndSeekPageBlobAfterWrite.java
new file mode 100644
index 0000000..7c60373
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestReadAndSeekPageBlobAfterWrite.java
@@ -0,0 +1,333 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs.azure;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assume.assumeNotNull;
+
+import java.io.IOException;
+import java.io.OutputStream;
+import java.util.Arrays;
+import java.util.Random;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.azure.AzureException;
+import org.apache.hadoop.util.Time;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+/**
+ * Write data into a page blob and verify you can read back all of it
+ * or just a part of it.
+ */
+public class TestReadAndSeekPageBlobAfterWrite {
+  private static final Log LOG = LogFactory.getLog(TestReadAndSeekPageBlobAfterWrite.class);
+
+  private FileSystem fs;
+  private AzureBlobStorageTestAccount testAccount;
+  private byte[] randomData;
+
+  // Page blob physical page size
+  private static final int PAGE_SIZE = PageBlobFormatHelpers.PAGE_SIZE;
+
+  // Size of data on page (excluding header)
+  private static final int PAGE_DATA_SIZE = PAGE_SIZE - PageBlobFormatHelpers.PAGE_HEADER_SIZE;
+  private static final int MAX_BYTES = 33554432; // maximum bytes in a file that we'll test
+  private static final int MAX_PAGES = MAX_BYTES / PAGE_SIZE; // maximum number of pages we'll test
+  private Random rand = new Random();
+
+  // A key with a prefix under /pageBlobs, which for the test file system will
+  // force use of a page blob.
+  private static final String KEY = "/pageBlobs/file.dat";
+  private static final Path PATH = new Path(KEY); // path of page blob file to read and write
+
+  protected AzureBlobStorageTestAccount createTestAccount() throws Exception {
+    return AzureBlobStorageTestAccount.create();
+  }
+
+  @Before
+  public void setUp() throws Exception {
+    testAccount = createTestAccount();
+    if (testAccount != null) {
+      fs = testAccount.getFileSystem();
+    }
+    assumeNotNull(testAccount);
+
+    // Make sure we are using an integral number of pages.
+    assertEquals(0, MAX_BYTES % PAGE_SIZE);
+
+    // load an in-memory array of random data
+    randomData = new byte[PAGE_SIZE * MAX_PAGES];
+    rand.nextBytes(randomData);
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    if (testAccount != null) {
+      testAccount.cleanup();
+      testAccount = null;
+      fs = null;
+    }
+  }
+
+  /**
+   * Make sure the file name (key) is a page blob file name. If anybody changes that,
+   * we need to come back and update this test class.
+   */
+  @Test
+  public void testIsPageBlobFileName() {
+    AzureNativeFileSystemStore store = ((NativeAzureFileSystem) fs).getStore();
+    String[] a = KEY.split("/");
+    String key2 = a[1] + "/";
+    assertTrue(store.isPageBlobKey(key2));
+  }
+
+  /**
+   * For a set of different file sizes, write some random data to a page blob,
+   * read it back, and compare that what was read is the same as what was written.
+   */
+  @Test
+  public void testReadAfterWriteRandomData() throws IOException {
+
+    // local shorthand
+    final int PDS = PAGE_DATA_SIZE;
+
+    // Test for sizes at and near page boundaries
+    int[] dataSizes = {
+
+        // on first page
+        0, 1, 2, 3,
+
+        // Near first physical page boundary (because the implementation
+        // stores PDS + the page header size bytes on each page).
+        PDS - 1, PDS, PDS + 1, PDS + 2, PDS + 3,
+
+        // near second physical page boundary
+        (2 * PDS) - 1, (2 * PDS), (2 * PDS) + 1, (2 * PDS) + 2, (2 * PDS) + 3,
+
+        // near tenth physical page boundary
+        (10 * PDS) - 1, (10 * PDS), (10 * PDS) + 1, (10 * PDS) + 2, (10 * PDS) + 3,
+
+        // test one big size, >> 4MB (an internal buffer size in the code)
+        MAX_BYTES
+    };
+
+    for (int i : dataSizes) {
+      testReadAfterWriteRandomData(i);
+    }
+  }
+
+  private void testReadAfterWriteRandomData(int size) throws IOException {
+    writeRandomData(size);
+    readRandomDataAndVerify(size);
+  }
+
+  /**
+   * Read "size" bytes of data and verify that what was read and what was written
+   * are the same.
+   */
+  private void readRandomDataAndVerify(int size) throws AzureException, IOException {
+    byte[] b = new byte[size];
+    FSDataInputStream stream = fs.open(PATH);
+    int bytesRead = stream.read(b);
+    stream.close();
+    assertEquals(bytesRead, size);
+
+    // compare the data read to the data written
+    assertTrue(comparePrefix(randomData, b, size));
+  }
+
+  // return true if the beginning "size" values of the arrays are the same
+  private boolean comparePrefix(byte[] a, byte[] b, int size) {
+    if (a.length < size || b.length < size) {
+      return false;
+    }
+    for (int i = 0; i < size; i++) {
+      if (a[i] != b[i]) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  // Write a specified amount of random data to the file path for this test class.
+  private void writeRandomData(int size) throws IOException {
+    OutputStream output = fs.create(PATH);
+    output.write(randomData, 0, size);
+    output.close();
+  }
+
+  /**
+   * Write data to a page blob, open it, seek, and then read a range of data.
+   * Then compare that the data read from that range is the same as the data originally written.
+   */
+  @Test
+  public void testPageBlobSeekAndReadAfterWrite() throws IOException {
+    writeRandomData(PAGE_SIZE * MAX_PAGES);
+    int recordSize = 100;
+    byte[] b = new byte[recordSize];
+    FSDataInputStream stream = fs.open(PATH);
+
+    // Seek to a boundary around the middle of the 6th page
+    int seekPosition = 5 * PAGE_SIZE + 250;
+    stream.seek(seekPosition);
+
+    // Read a record's worth of bytes and verify results
+    int bytesRead = stream.read(b);
+    verifyReadRandomData(b, bytesRead, seekPosition, recordSize);
+
+    // Seek to another spot and read a record greater than a page
+    seekPosition = 10 * PAGE_SIZE + 250;
+    stream.seek(seekPosition);
+    recordSize = 1000;
+    b = new byte[recordSize];
+    bytesRead = stream.read(b);
+    verifyReadRandomData(b, bytesRead, seekPosition, recordSize);
+
+    // Read the last 100 bytes of the file
+    recordSize = 100;
+    seekPosition = PAGE_SIZE * MAX_PAGES - recordSize;
+    stream.seek(seekPosition);
+    b = new byte[recordSize];
+    bytesRead = stream.read(b);
+    verifyReadRandomData(b, bytesRead, seekPosition, recordSize);
+
+    // Read past the end of the file and we should get only partial data.
+    recordSize = 100;
+    seekPosition = PAGE_SIZE * MAX_PAGES - recordSize + 50;
+    stream.seek(seekPosition);
+    b = new byte[recordSize];
+    bytesRead = stream.read(b);
+    assertEquals(50, bytesRead);
+
+    // compare last 50 bytes written with those read
+    byte[] tail = Arrays.copyOfRange(randomData, seekPosition, randomData.length);
+    assertTrue(comparePrefix(tail, b, 50));
+  }
+
+  // Verify that reading a record of data after seeking gives the expected data.
+  private void verifyReadRandomData(byte[] b, int bytesRead, int seekPosition, int recordSize) {
+    byte[] originalRecordData =
+        Arrays.copyOfRange(randomData, seekPosition, seekPosition + recordSize + 1);
+    assertEquals(recordSize, bytesRead);
+    assertTrue(comparePrefix(originalRecordData, b, recordSize));
+  }
+
+  // Test many small flushed writes interspersed with periodic hflush calls.
+  // For manual testing, increase NUM_WRITES to a large number.
+  // The goal for a long-running manual test is to make sure that it finishes
+  // and the close() call does not time out. It also facilitates debugging into
+  // hflush/hsync.
+  @Test
+  public void testManySmallWritesWithHFlush() throws IOException {
+    writeAndReadOneFile(50, 100, 20);
+  }
+
+  /**
+   * Write a total of numWrites * recordLength data to a file, read it back,
+   * and check to make sure what was read is the same as what was written.
+   * The syncInterval is the number of writes after which to call hflush to
+   * force the data to storage.
+   */
+  private void writeAndReadOneFile(int numWrites, int recordLength, int syncInterval) throws IOException {
+    final int NUM_WRITES = numWrites;
+    final int RECORD_LENGTH = recordLength;
+    final int SYNC_INTERVAL = syncInterval;
+
+    // A lower bound on the minimum time we think it will take to do
+    // a write to Azure storage.
+    final long MINIMUM_EXPECTED_TIME = 20;
+    LOG.info("Writing " + NUM_WRITES * RECORD_LENGTH + " bytes to " + PATH.getName());
+    FSDataOutputStream output = fs.create(PATH);
+    int writesSinceHFlush = 0;
+    try {
+
+      // Do a flush and hflush to exercise case for empty write queue in PageBlobOutputStream,
+      // to test concurrent execution gates.
+      output.flush();
+      output.hflush();
+      for (int i = 0; i < NUM_WRITES; i++) {
+        output.write(randomData, i * RECORD_LENGTH, RECORD_LENGTH);
+        writesSinceHFlush++;
+        output.flush();
+        if ((i % SYNC_INTERVAL) == 0) {
+          long start = Time.monotonicNow();
+          output.hflush();
+          writesSinceHFlush = 0;
+          long end = Time.monotonicNow();
+
+          // A true, round-trip synchronous flush to Azure must take
+          // a significant amount of time or we are not syncing to storage correctly.
+          LOG.debug("hflush duration = " + (end - start) + " msec.");
+          assertTrue(String.format(
+            "hflush duration of %d, less than minimum expected of %d",
+            end - start, MINIMUM_EXPECTED_TIME),
+            end - start >= MINIMUM_EXPECTED_TIME);
+        }
+      }
+    } finally {
+      long start = Time.monotonicNow();
+      output.close();
+      long end = Time.monotonicNow();
+      LOG.debug("close duration = " + (end - start) + " msec.");
+      if (writesSinceHFlush > 0) {
+        assertTrue(String.format(
+            "close duration with >= 1 pending write is %d, less than minimum expected of %d",
+            end - start, MINIMUM_EXPECTED_TIME),
+            end - start >= MINIMUM_EXPECTED_TIME);
+        }
+    }
+
+    // Read the data back and check it.
+    FSDataInputStream stream = fs.open(PATH);
+    int SIZE = NUM_WRITES * RECORD_LENGTH;
+    byte[] b = new byte[SIZE];
+    try {
+      stream.seek(0);
+      stream.read(b, 0, SIZE);
+      verifyReadRandomData(b, SIZE, 0, SIZE);
+    } finally {
+      stream.close();
+    }
+
+    // delete the file
+    fs.delete(PATH, false);
+  }
+
+  // Test writing to a large file repeatedly as a stress test.
+  // Set the repetitions to a larger number for manual testing
+  // for a longer stress run.
+  @Test
+  public void testLargeFileStress() throws IOException {
+    int numWrites = 32;
+    int recordSize = 1024 * 1024;
+    int syncInterval = 10;
+    int repetitions = 1;
+    for (int i = 0; i < repetitions; i++) {
+      writeAndReadOneFile(numWrites, recordSize, syncInterval);
+    }
+  }
+}
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestWasbFsck.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestWasbFsck.java
index a6c3f39..467424b 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestWasbFsck.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestWasbFsck.java
@@ -29,6 +29,7 @@
 import org.apache.hadoop.fs.Path;
 import org.junit.After;
 import org.junit.Before;
+import org.junit.Ignore;
 import org.junit.Test;
 
 public class TestWasbFsck {
@@ -63,6 +64,38 @@ private int getNumTempBlobs() {
     return count;
   }
 
+  /**
+   * Tests that we recover files properly
+   */
+  @Test
+  @Ignore  /* flush() no longer does anything  @@TODO: reinstate an appropriate test of fsck recovery*/
+  public void testRecover() throws Exception {
+    Path danglingFile = new Path("/crashedInTheMiddle");
+
+    // Create a file and leave it dangling and try to recover it.
+    FSDataOutputStream stream = fs.create(danglingFile);
+    stream.write(new byte[] { 1, 2, 3 });
+    stream.flush();
+
+    // Now we should still only see a zero-byte file in this place
+    FileStatus fileStatus = fs.getFileStatus(danglingFile);
+    assertNotNull(fileStatus);
+    assertEquals(0, fileStatus.getLen());
+    assertEquals(1, getNumTempBlobs());
+
+    // Run WasbFsck -move to recover the file.
+    runFsck("-move");
+
+    // Now we should the see the file in lost+found with the data there.
+    fileStatus = fs.getFileStatus(new Path("/lost+found",
+        danglingFile.getName()));
+    assertNotNull(fileStatus);
+    assertEquals(3, fileStatus.getLen());
+    assertEquals(0, getNumTempBlobs());
+    // But not in its original location
+    assertFalse(fs.exists(danglingFile));
+  }
+
   private void runFsck(String command) throws Exception {
     Configuration conf = fs.getConf();
     // Set the dangling cutoff to zero, so every temp blob is considered
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestWasbUriAndConfiguration.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestWasbUriAndConfiguration.java
index ea24c59..0360e32 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestWasbUriAndConfiguration.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestWasbUriAndConfiguration.java
@@ -274,8 +274,8 @@ public void testMultipleContainers() throws Exception {
     assumeNotNull(firstAccount);
     assumeNotNull(secondAccount);
     try {
-      FileSystem firstFs = firstAccount.getFileSystem(), secondFs = secondAccount
-          .getFileSystem();
+      FileSystem firstFs = firstAccount.getFileSystem(),
+          secondFs = secondAccount.getFileSystem();
       Path testFile = new Path("/testWasb");
       assertTrue(validateIOStreams(firstFs, testFile));
       assertTrue(validateIOStreams(secondFs, testFile));
@@ -356,13 +356,16 @@ public void testNoUriAuthority() throws Exception {
     // the actual URI being asv(s)/wasb(s):///, it should work.
 
     String[] wasbAliases = new String[] { "wasb", "wasbs" };
-    for (String defaultScheme : wasbAliases){
+    for (String defaultScheme : wasbAliases) {
       for (String wantedScheme : wasbAliases) {
         testAccount = AzureBlobStorageTestAccount.createMock();
         Configuration conf = testAccount.getFileSystem().getConf();
         String authority = testAccount.getFileSystem().getUri().getAuthority();
         URI defaultUri = new URI(defaultScheme, authority, null, null, null);
         conf.set("fs.default.name", defaultUri.toString());
+        
+        // Add references to file system implementations for wasb and wasbs.
+        conf.addResource("azure-test.xml");
         URI wantedUri = new URI(wantedScheme + ":///random/path");
         NativeAzureFileSystem obtained = (NativeAzureFileSystem) FileSystem
             .get(wantedUri, conf);
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/AzureMetricsTestUtil.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/AzureMetricsTestUtil.java
index 1269417..4c706ce 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/AzureMetricsTestUtil.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/AzureMetricsTestUtil.java
@@ -42,6 +42,7 @@ public static long getLongCounterValue(AzureFileSystemInstrumentation instrument
   }
 
 
+
   /**
    * Gets the current value of the wasb_bytes_written_last_second counter.
    */
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/TestAzureFileSystemInstrumentation.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/TestAzureFileSystemInstrumentation.java
index 35004d6..896ec1b 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/TestAzureFileSystemInstrumentation.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/TestAzureFileSystemInstrumentation.java
@@ -104,7 +104,7 @@ public void testMetricTags() throws Exception {
   @Test
   public void testMetricsOnMkdirList() throws Exception {
     long base = getBaseWebResponses();
-    
+
     // Create a directory
     assertTrue(fs.mkdirs(new Path("a")));
     // At the time of writing, it takes 1 request to create the actual directory,
@@ -121,7 +121,7 @@ public void testMetricsOnMkdirList() throws Exception {
         AzureMetricsTestUtil.getLongCounterValue(getInstrumentation(), WASB_DIRECTORIES_CREATED));
 
     // List the root contents
-    assertEquals(1, fs.listStatus(new Path("/")).length);    
+    assertEquals(1, fs.listStatus(new Path("/")).length);
     base = assertWebResponsesEquals(base, 1);
 
     assertNoErrors();
@@ -142,7 +142,7 @@ private BandwidthGaugeUpdater getBandwidthGaugeUpdater() {
   @Test
   public void testMetricsOnFileCreateRead() throws Exception {
     long base = getBaseWebResponses();
-    
+
     assertEquals(0, AzureMetricsTestUtil.getCurrentBytesWritten(getInstrumentation()));
 
     Path filePath = new Path("/metricsTest_webResponses");
@@ -158,7 +158,7 @@ public void testMetricsOnFileCreateRead() throws Exception {
     outputStream.write(nonZeroByteArray(FILE_SIZE));
     outputStream.close();
     long uploadDurationMs = new Date().getTime() - start.getTime();
-    
+
     // The exact number of requests/responses that happen to create a file
     // can vary  - at the time of writing this code it takes 10
     // requests/responses for the 1000 byte file (33 for 100 MB),
@@ -200,7 +200,7 @@ public void testMetricsOnFileCreateRead() throws Exception {
         " the case since the test overestimates the latency by looking at " +
         " end-to-end time instead of just block upload time.",
         uploadLatency <= expectedLatency);
-    
+
     // Read the file
     start = new Date();
     InputStream inputStream = fs.open(filePath);
@@ -380,19 +380,19 @@ public void testMetricsOnFileExistsDelete() throws Exception {
   @Test
   public void testMetricsOnDirRename() throws Exception {
     long base = getBaseWebResponses();
-    
+
     Path originalDirName = new Path("/metricsTestDirectory_RenameStart");
     Path innerFileName = new Path(originalDirName, "innerFile");
     Path destDirName = new Path("/metricsTestDirectory_RenameFinal");
-    
+
     // Create an empty directory
     assertTrue(fs.mkdirs(originalDirName));
     base = getCurrentWebResponses();
-    
+
     // Create an inner file
     assertTrue(fs.createNewFile(innerFileName));
     base = getCurrentWebResponses();
-    
+
     // Rename the directory
     assertTrue(fs.rename(originalDirName, destDirName));
     // At the time of writing this code it takes 11 requests/responses
@@ -499,7 +499,7 @@ private AzureFileSystemInstrumentation getInstrumentation() {
    */
   private static class TagMatcher extends TagExistsMatcher {
     private final String tagValue;
-    
+
     public TagMatcher(String tagName, String tagValue) {
       super(tagName);
       this.tagValue = tagValue;
@@ -522,7 +522,7 @@ public void describeTo(Description desc) {
    */
   private static class TagExistsMatcher extends BaseMatcher<MetricsTag> {
     private final String tagName;
-    
+
     public TagExistsMatcher(String tagName) {
       this.tagName = tagName;
     }
@@ -532,7 +532,7 @@ public boolean matches(Object toMatch) {
       MetricsTag asTag = (MetricsTag)toMatch;
       return asTag.name().equals(tagName) && matches(asTag);
     }
-    
+
     protected boolean matches(MetricsTag toMatch) {
       return true;
     }
@@ -542,5 +542,32 @@ public void describeTo(Description desc) {
       desc.appendText("Has tag " + tagName);
     }
   }
-  
+
+  /**
+   * A matcher class for asserting that a long value is in a
+   * given range.
+   */
+  private static class InRange extends BaseMatcher<Long> {
+    private final long inclusiveLowerLimit;
+    private final long inclusiveUpperLimit;
+    private long obtained;
+
+    public InRange(long inclusiveLowerLimit, long inclusiveUpperLimit) {
+      this.inclusiveLowerLimit = inclusiveLowerLimit;
+      this.inclusiveUpperLimit = inclusiveUpperLimit;
+    }
+
+    @Override
+    public boolean matches(Object number) {
+      obtained = (Long)number;
+      return obtained >= inclusiveLowerLimit &&
+          obtained <= inclusiveUpperLimit;
+    }
+
+    @Override
+    public void describeTo(Description description) {
+      description.appendText("Between " + inclusiveLowerLimit +
+          " and " + inclusiveUpperLimit + " inclusively");
+    }
+  }
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/TestBandwidthGaugeUpdater.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/TestBandwidthGaugeUpdater.java
index ef34422..9fed21b 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/TestBandwidthGaugeUpdater.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/metrics/TestBandwidthGaugeUpdater.java
@@ -20,14 +20,10 @@
 
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
-import static org.junit.Assume.assumeNotNull;
 
 import java.util.Date;
-import java.util.Map;
 
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount;
-import org.junit.Assume;
 import org.junit.Test;
 
 public class TestBandwidthGaugeUpdater {
@@ -79,47 +75,4 @@ public void run() {
     assertEquals(10 * threads.length, AzureMetricsTestUtil.getCurrentBytesRead(instrumentation));
     updater.close();
   }
-  
-  @Test
-  public void testFinalizerThreadShutdown() throws Exception {
-    
-    // force cleanup of any existing wasb filesystems
-    System.gc();
-    System.runFinalization();
-    
-    int nUpdaterThreadsStart = getWasbThreadCount();
-    assertTrue("Existing WASB threads have not been cleared", nUpdaterThreadsStart == 0);
-    
-    final int nFilesystemsToSpawn = 10;
-    AzureBlobStorageTestAccount testAccount = null;
-    
-    for(int i = 0; i < nFilesystemsToSpawn; i++){
-      testAccount = AzureBlobStorageTestAccount.createMock();
-      testAccount.getFileSystem();
-    }
-
-    int nUpdaterThreadsAfterSpawn = getWasbThreadCount();
-    Assume.assumeTrue("Background threads should have spawned.", nUpdaterThreadsAfterSpawn == 10);
-   
-    testAccount = null;  //clear the last reachable reference
-    
-    // force cleanup
-    System.gc();
-    System.runFinalization();
-    
-    int nUpdaterThreadsAfterCleanup = getWasbThreadCount();
-    assertTrue("Finalizers should have reduced the thread count.  ", nUpdaterThreadsAfterCleanup == 0 );
-  }
-
-  private int getWasbThreadCount() {
-    int c = 0;
-    Map<Thread, StackTraceElement[]> stacksStart = Thread.getAllStackTraces();
-    for (Thread t : stacksStart.keySet()){
-      if(t.getName().equals(BandwidthGaugeUpdater.THREAD_NAME))
-      {
-        c++;
-      }
-    }
-    return c;
-  }
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/resources/azure-test.xml b/hadoop-tools/hadoop-azure/src/test/resources/azure-test.xml
index 7eeff92..98e68c4 100644
--- a/hadoop-tools/hadoop-azure/src/test/resources/azure-test.xml
+++ b/hadoop-tools/hadoop-azure/src/test/resources/azure-test.xml
@@ -15,19 +15,7 @@
 
 <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
 <configuration xmlns:xi="http://www.w3.org/2001/XInclude">
-
-  <property> 
-    <name>fs.wasb.impl</name> 
-    <value>org.apache.hadoop.fs.azure.NativeAzureFileSystem</value> 
-  </property> 
-   
-  <property> 
-    <name>fs.wasbs.impl</name> 
-    <value>org.apache.hadoop.fs.azure.NativeAzureFileSystem</value> 
-  </property> 
- 
  
-  
   <!-- For tests against live azure, provide the following account information -->
   <!--
   <property>
-- 
1.7.9.5

