From 2185e0a4d96a7129dfda4405182c090743e1a3db Mon Sep 17 00:00:00 2001
From: Arun Suresh <asuresh@cloudera.com>
Date: Fri, 5 Jun 2015 08:03:16 -0700
Subject: [PATCH 589/596] Revert "CLOUDERA-BUILD: CDH-27001. Fix security
 vulnerability in (en/de)crypting MR intermediate
 data spill files"

This reverts commit a411399d0adc82fae419688c418d6590606a9631.
---
 .../hadoop/mapred/LocalContainerLauncher.java      |   10 -------
 .../hadoop/mapred/TaskAttemptListenerImpl.java     |   17 ++---------
 .../java/org/apache/hadoop/mapred/YarnChild.java   |   18 ------------
 .../hadoop/mapreduce/v2/app/MRAppMaster.java       |   24 +---------------
 .../main/java/org/apache/hadoop/mapred/Task.java   |   25 ----------------
 .../org/apache/hadoop/mapreduce/CryptoUtils.java   |   17 +++++------
 .../org/apache/hadoop/mapreduce/JobSubmitter.java  |   13 ++++-----
 .../hadoop/mapreduce/security/TokenCache.java      |   10 -------
 .../hadoop/mapreduce/task/reduce/LocalFetcher.java |    6 ++--
 .../hadoop/mapreduce/task/reduce/TestMerger.java   |    2 +-
 .../mapred/TestMRIntermediateDataEncryption.java   |   30 ++++++--------------
 .../org/apache/hadoop/mapred/TestMapProgress.java  |   14 ++++-----
 12 files changed, 37 insertions(+), 149 deletions(-)

diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java
index b30a695..218ac83 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java
@@ -82,7 +82,6 @@
   private final TaskUmbilicalProtocol umbilical;
   private ExecutorService taskRunner;
   private Thread eventHandler;
-  private byte[] encryptedSpillKey = new byte[] {0};
   private BlockingQueue<ContainerLauncherEvent> eventQueue =
       new LinkedBlockingQueue<ContainerLauncherEvent>();
 
@@ -157,11 +156,6 @@ public void handle(ContainerLauncherEvent event) {
     }
   }
 
-  public void setEncryptedSpillKey(byte[] encryptedSpillKey) {
-    if (encryptedSpillKey != null) {
-      this.encryptedSpillKey = encryptedSpillKey;
-    }
-  }
 
   /*
    * Uber-AM lifecycle/ordering ("normal" case):
@@ -360,10 +354,6 @@ private void runSubtask(org.apache.hadoop.mapred.Task task,
         // map to handle)
         conf.setBoolean("mapreduce.task.uberized", true);
 
-        // Check and handle Encrypted spill key
-        task.setEncryptedSpillKey(encryptedSpillKey);
-        YarnChild.setEncryptedSpillKeyIfRequired(task);
-
         // META-FIXME: do we want the extra sanity-checking (doneWithMaps,
         // etc.), or just assume/hope the state machine(s) and uber-AM work
         // as expected?
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java
index 8712fa3..78f28be 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java
@@ -81,27 +81,17 @@
     jvmIDToActiveAttemptMap
       = new ConcurrentHashMap<WrappedJvmID, org.apache.hadoop.mapred.Task>();
   private Set<WrappedJvmID> launchedJVMs = Collections
-      .newSetFromMap(new ConcurrentHashMap<WrappedJvmID, Boolean>());
-
+      .newSetFromMap(new ConcurrentHashMap<WrappedJvmID, Boolean>()); 
+  
   private JobTokenSecretManager jobTokenSecretManager = null;
-  private byte[] encryptedSpillKey;
-
+  
   public TaskAttemptListenerImpl(AppContext context,
       JobTokenSecretManager jobTokenSecretManager,
       RMHeartbeatHandler rmHeartbeatHandler) {
-    this(context, jobTokenSecretManager, rmHeartbeatHandler,
-            null);
-  }
-
-  public TaskAttemptListenerImpl(AppContext context,
-      JobTokenSecretManager jobTokenSecretManager,
-      RMHeartbeatHandler rmHeartbeatHandler,
-      byte[] secretShuffleKey) {
     super(TaskAttemptListenerImpl.class.getName());
     this.context = context;
     this.jobTokenSecretManager = jobTokenSecretManager;
     this.rmHeartbeatHandler = rmHeartbeatHandler;
-    this.encryptedSpillKey = secretShuffleKey;
   }
 
   @Override
@@ -446,7 +436,6 @@ public JvmTask getTask(JvmContext context) throws IOException {
             jvmIDToActiveAttemptMap.remove(wJvmID);
         launchedJVMs.remove(wJvmID);
         LOG.info("JVM with ID: " + jvmId + " given task: " + task.getTaskID());
-        task.setEncryptedSpillKey(encryptedSpillKey);
         jvmTask = new JvmTask(task, false);
       }
     }
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/YarnChild.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/YarnChild.java
index 7f3111f..fec13a8 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/YarnChild.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/YarnChild.java
@@ -159,7 +159,6 @@ public TaskUmbilicalProtocol run() throws Exception {
         @Override
         public Object run() throws Exception {
           // use job-specified working directory
-          setEncryptedSpillKeyIfRequired(taskFinal);
           FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());
           taskFinal.run(job, umbilical); // run the task
           return null;
@@ -219,23 +218,6 @@ public Object run() throws Exception {
   }
 
   /**
-   * Utility method to check if the Encrypted Spill Key needs to be set into the
-   * user credentials of the user running the Map / Reduce Task
-   * @param task The Map / Reduce task to set the Encrypted Spill information in
-   * @throws Exception
-   */
-  public static void setEncryptedSpillKeyIfRequired(Task task) throws
-          Exception {
-    if ((task != null) && (task.getEncryptedSpillKey() != null) && (task
-            .getEncryptedSpillKey().length > 1)) {
-      Credentials creds =
-              UserGroupInformation.getCurrentUser().getCredentials();
-      TokenCache.setEncryptedSpillKey(task.getEncryptedSpillKey(), creds);
-      UserGroupInformation.getCurrentUser().addCredentials(creds);
-    }
-  }
-
-  /**
    * Configure mapred-local dirs. This config is used by the task for finding
    * out an output directory.
    * @throws IOException 
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
index cba3b00..55eb88b 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
@@ -21,7 +21,6 @@
 import java.io.IOException;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.InvocationTargetException;
-import java.security.NoSuchAlgorithmException;
 import java.security.PrivilegedExceptionAction;
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -48,7 +47,6 @@
 import org.apache.hadoop.mapred.TaskAttemptListenerImpl;
 import org.apache.hadoop.mapred.TaskLog;
 import org.apache.hadoop.mapred.TaskUmbilicalProtocol;
-import org.apache.hadoop.mapreduce.CryptoUtils;
 import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.MRJobConfig;
 import org.apache.hadoop.mapreduce.OutputCommitter;
@@ -147,8 +145,6 @@
 
 import com.google.common.annotations.VisibleForTesting;
 
-import javax.crypto.KeyGenerator;
-
 /**
  * The Map-Reduce Application Master.
  * The state machine is encapsulated in the implementation of Job interface.
@@ -176,7 +172,6 @@
    * Priority of the MRAppMaster shutdown hook.
    */
   public static final int SHUTDOWN_HOOK_PRIORITY = 30;
-  public static final String INTERMEDIATE_DATA_ENCRYPTION_ALGO = "HmacSHA1";
 
   private Clock clock;
   private final long startTime;
@@ -207,7 +202,6 @@
   private JobEventDispatcher jobEventDispatcher;
   private JobHistoryEventHandler jobHistoryEventHandler;
   private SpeculatorEventDispatcher speculatorEventDispatcher;
-  private byte[] encryptedSpillKey;
 
   private Job job;
   private Credentials jobCredentials = new Credentials(); // Filled during init
@@ -651,22 +645,8 @@ protected void initJobCredentialsAndUGI(Configuration conf) {
     try {
       this.currentUser = UserGroupInformation.getCurrentUser();
       this.jobCredentials = ((JobConf)conf).getCredentials();
-      if (CryptoUtils.isEncryptedSpillEnabled(conf)) {
-        int keyLen = conf.getInt(
-                MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS,
-                MRJobConfig
-                        .DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS);
-        KeyGenerator keyGen =
-                KeyGenerator.getInstance(INTERMEDIATE_DATA_ENCRYPTION_ALGO);
-        keyGen.init(keyLen);
-        encryptedSpillKey = keyGen.generateKey().getEncoded();
-      } else {
-        encryptedSpillKey = new byte[] {0};
-      }
     } catch (IOException e) {
       throw new YarnRuntimeException(e);
-    } catch (NoSuchAlgorithmException e) {
-      throw new YarnRuntimeException(e);
     }
   }
 
@@ -722,7 +702,7 @@ public Speculator call(Configuration conf) {
   protected TaskAttemptListener createTaskAttemptListener(AppContext context) {
     TaskAttemptListener lis =
         new TaskAttemptListenerImpl(context, jobTokenSecretManager,
-            getRMHeartbeatHandler(), encryptedSpillKey);
+            getRMHeartbeatHandler());
     return lis;
   }
 
@@ -889,8 +869,6 @@ protected void serviceStart() throws Exception {
       if (job.isUber()) {
         this.containerLauncher = new LocalContainerLauncher(context,
             (TaskUmbilicalProtocol) taskAttemptListener);
-        ((LocalContainerLauncher) this.containerLauncher)
-                .setEncryptedSpillKey(encryptedSpillKey);
       } else {
         this.containerLauncher = new ContainerLauncherImpl(context);
       }
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
index 1494a27..71be030 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java
@@ -149,8 +149,6 @@ static synchronized String getOutputName(int partition) {
   private String user;                            // user running the job
   private TaskAttemptID taskId;                   // unique, includes job id
   private int partition;                          // id within job
-  private byte[] encryptedSpillKey = new byte[] {0};  // Key Used to encrypt
-  // intermediate spills
   TaskStatus taskStatus;                          // current status of the task
   protected JobStatus.State jobRunStateForCleanup;
   protected boolean jobCleanup = false;
@@ -259,24 +257,6 @@ public void setJobTokenSecret(SecretKey tokenSecret) {
   }
 
   /**
-   * Get Encrypted spill key
-   * @return encrypted spill key
-   */
-  public byte[] getEncryptedSpillKey() {
-    return encryptedSpillKey;
-  }
-
-  /**
-   * Set Encrypted spill key
-   * @param encryptedSpillKey key
-   */
-  public void setEncryptedSpillKey(byte[] encryptedSpillKey) {
-    if (encryptedSpillKey != null) {
-      this.encryptedSpillKey = encryptedSpillKey;
-    }
-  }
-
-  /**
    * Get the job token secret
    * @return the token secret
    */
@@ -506,8 +486,6 @@ public void write(DataOutput out) throws IOException {
     out.writeBoolean(writeSkipRecs);
     out.writeBoolean(taskCleanup);
     Text.writeString(out, user);
-    out.writeInt(encryptedSpillKey.length);
-    out.write(encryptedSpillKey);
     extraData.write(out);
   }
   
@@ -533,9 +511,6 @@ public void readFields(DataInput in) throws IOException {
       setPhase(TaskStatus.Phase.CLEANUP);
     }
     user = StringInterner.weakIntern(Text.readString(in));
-    int len = in.readInt();
-    encryptedSpillKey = new byte[len];
-    in.readFully(encryptedSpillKey);
     extraData.readFields(in);
   }
 
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/CryptoUtils.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/CryptoUtils.java
index 744b9de..184cdf0 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/CryptoUtils.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/CryptoUtils.java
@@ -34,6 +34,7 @@
 import org.apache.hadoop.fs.crypto.CryptoFSDataInputStream;
 import org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream;
 import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.mapreduce.MRJobConfig;
 import org.apache.hadoop.mapreduce.security.TokenCache;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.util.LimitInputStream;
@@ -49,7 +50,7 @@
 
   private static final Log LOG = LogFactory.getLog(CryptoUtils.class);
 
-  public static boolean isEncryptedSpillEnabled(Configuration conf) {
+  public static boolean isShuffleEncrypted(Configuration conf) {
     return conf.getBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA,
         MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA);
   }
@@ -63,7 +64,7 @@ public static boolean isEncryptedSpillEnabled(Configuration conf) {
    */
   public static byte[] createIV(Configuration conf) throws IOException {
     CryptoCodec cryptoCodec = CryptoCodec.getInstance(conf);
-    if (isEncryptedSpillEnabled(conf)) {
+    if (isShuffleEncrypted(conf)) {
       byte[] iv = new byte[cryptoCodec.getCipherSuite().getAlgorithmBlockSize()];
       cryptoCodec.generateSecureRandom(iv);
       return iv;
@@ -74,13 +75,13 @@ public static boolean isEncryptedSpillEnabled(Configuration conf) {
 
   public static int cryptoPadding(Configuration conf) {
     // Sizeof(IV) + long(start-offset)
-    return isEncryptedSpillEnabled(conf) ? CryptoCodec.getInstance(conf)
+    return isShuffleEncrypted(conf) ? CryptoCodec.getInstance(conf)
         .getCipherSuite().getAlgorithmBlockSize() + 8 : 0;
   }
 
   private static byte[] getEncryptionKey() throws IOException {
-    return TokenCache.getEncryptedSpillKey(UserGroupInformation.getCurrentUser()
-            .getCredentials());
+    return TokenCache.getShuffleSecretKey(UserGroupInformation.getCurrentUser()
+        .getCredentials());
   }
 
   private static int getBufferSize(Configuration conf) {
@@ -101,7 +102,7 @@ private static int getBufferSize(Configuration conf) {
    */
   public static FSDataOutputStream wrapIfNecessary(Configuration conf,
       FSDataOutputStream out) throws IOException {
-    if (isEncryptedSpillEnabled(conf)) {
+    if (isShuffleEncrypted(conf)) {
       out.write(ByteBuffer.allocate(8).putLong(out.getPos()).array());
       byte[] iv = createIV(conf);
       out.write(iv);
@@ -136,7 +137,7 @@ public static FSDataOutputStream wrapIfNecessary(Configuration conf,
    */
   public static InputStream wrapIfNecessary(Configuration conf, InputStream in,
       long length) throws IOException {
-    if (isEncryptedSpillEnabled(conf)) {
+    if (isShuffleEncrypted(conf)) {
       int bufferSize = getBufferSize(conf);
       if (length > -1) {
         in = new LimitInputStream(in, length);
@@ -173,7 +174,7 @@ public static InputStream wrapIfNecessary(Configuration conf, InputStream in,
    */
   public static FSDataInputStream wrapIfNecessary(Configuration conf,
       FSDataInputStream in) throws IOException {
-    if (isEncryptedSpillEnabled(conf)) {
+    if (isShuffleEncrypted(conf)) {
       CryptoCodec cryptoCodec = CryptoCodec.getInstance(conf);
       int bufferSize = getBufferSize(conf);
       // Not going to be used... but still has to be read...
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java
index 21f867d..e810cdf 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java
@@ -52,7 +52,6 @@
 import static org.apache.hadoop.mapred.QueueManager.toFullPropertyName;
 
 import org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager;
-import org.apache.hadoop.mapreduce.counters.Limits;
 import org.apache.hadoop.mapreduce.filecache.DistributedCache;
 import org.apache.hadoop.mapreduce.protocol.ClientProtocol;
 import org.apache.hadoop.mapreduce.security.TokenCache;
@@ -466,8 +465,13 @@ JobStatus submitJobInternal(Job job, Cluster cluster)
       if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {
         KeyGenerator keyGen;
         try {
+         
+          int keyLen = CryptoUtils.isShuffleEncrypted(conf) 
+              ? conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS, 
+                  MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS)
+              : SHUFFLE_KEY_LENGTH;
           keyGen = KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);
-          keyGen.init(SHUFFLE_KEY_LENGTH);
+          keyGen.init(keyLen);
         } catch (NoSuchAlgorithmException e) {
           throw new IOException("Error generating shuffle secret key", e);
         }
@@ -475,11 +479,6 @@ JobStatus submitJobInternal(Job job, Cluster cluster)
         TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),
             job.getCredentials());
       }
-      if (CryptoUtils.isEncryptedSpillEnabled(conf)) {
-        conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 1);
-        LOG.warn("Max job attempts set to 1 since encrypted intermediate" +
-                "data spill is enabled");
-      }
 
       copyAndConfigureFiles(job, submitJobDir);
       
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/TokenCache.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/TokenCache.java
index c6555d9..5a572cf 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/TokenCache.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/TokenCache.java
@@ -177,7 +177,6 @@ private static void mergeBinaryTokens(Credentials creds, Configuration conf) {
   public static final String JOB_TOKENS_FILENAME = "mapreduce.job.jobTokenFile";
   private static final Text JOB_TOKEN = new Text("JobToken");
   private static final Text SHUFFLE_TOKEN = new Text("MapReduceShuffleToken");
-  private static final Text ENC_SPILL_KEY = new Text("MapReduceEncryptedSpillKey");
   
   /**
    * load job token from a file
@@ -246,15 +245,6 @@ public static void setShuffleSecretKey(byte[] key, Credentials credentials) {
     return getSecretKey(credentials, SHUFFLE_TOKEN);
   }
 
-  @InterfaceAudience.Private
-  public static void setEncryptedSpillKey(byte[] key, Credentials credentials) {
-    credentials.addSecretKey(ENC_SPILL_KEY, key);
-  }
-
-  @InterfaceAudience.Private
-  public static byte[] getEncryptedSpillKey(Credentials credentials) {
-    return getSecretKey(credentials, ENC_SPILL_KEY);
-  }
   /**
    * @deprecated Use {@link Credentials#getToken(org.apache.hadoop.io.Text)}
    * instead, this method is included for compatibility against Hadoop-1
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java
index de2382c..6794c99 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java
@@ -127,9 +127,6 @@ private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {
     long compressedLength = ir.partLength;
     long decompressedLength = ir.rawLength;
 
-    compressedLength -= CryptoUtils.cryptoPadding(job);
-    decompressedLength -= CryptoUtils.cryptoPadding(job);
-
     // Get the location for the map output - either in-memory or on-disk
     MapOutput<K, V> mapOutput = merger.reserve(mapTaskId, decompressedLength,
         id);
@@ -153,7 +150,8 @@ private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {
     inStream = CryptoUtils.wrapIfNecessary(job, inStream);
 
     try {
-      inStream.seek(ir.startOffset + CryptoUtils.cryptoPadding(job));
+      inStream.seek(ir.startOffset);
+
       mapOutput.shuffle(LOCALHOST, inStream, compressedLength, decompressedLength, metrics, reporter);
     } finally {
       try {
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMerger.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMerger.java
index a6b1964..6e3bedf 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMerger.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMerger.java
@@ -87,7 +87,7 @@ public void testEncryptedMerger() throws Throwable {
     jobConf.setBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA, true);
     conf.setBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA, true);
     Credentials credentials = UserGroupInformation.getCurrentUser().getCredentials();
-    TokenCache.setEncryptedSpillKey(new byte[16], credentials);
+    TokenCache.setShuffleSecretKey(new byte[16], credentials);
     UserGroupInformation.getCurrentUser().addCredentials(credentials);
     testInMemoryAndOnDiskMerger();
   }
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMRIntermediateDataEncryption.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMRIntermediateDataEncryption.java
index 28b2295..ebc32ad 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMRIntermediateDataEncryption.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMRIntermediateDataEncryption.java
@@ -52,31 +52,24 @@
 
   @Test
   public void testSingleReducer() throws Exception {
-    doEncryptionTest(3, 1, 2, false);
-  }
-
-  @Test
-  public void testUberMode() throws Exception {
-    doEncryptionTest(3, 1, 2, true);
+    doEncryptionTest(3, 1, 2);
   }
 
   @Test
   public void testMultipleMapsPerNode() throws Exception {
-    doEncryptionTest(8, 1, 2, false);
+    doEncryptionTest(8, 1, 2);
   }
 
   @Test
   public void testMultipleReducers() throws Exception {
-    doEncryptionTest(2, 4, 2, false);
+    doEncryptionTest(2, 4, 2);
   }
 
-  public void doEncryptionTest(int numMappers, int numReducers, int numNodes,
-                               boolean isUber) throws Exception {
-    doEncryptionTest(numMappers, numReducers, numNodes, 1000, isUber);
+  public void doEncryptionTest(int numMappers, int numReducers, int numNodes) throws Exception {
+    doEncryptionTest(numMappers, numReducers, numNodes, 1000);
   }
 
-  public void doEncryptionTest(int numMappers, int numReducers, int numNodes,
-                               int numLines, boolean isUber) throws Exception {
+  public void doEncryptionTest(int numMappers, int numReducers, int numNodes, int numLines) throws Exception {
     MiniDFSCluster dfsCluster = null;
     MiniMRClientCluster mrCluster = null;
     FileSystem fileSystem = null;
@@ -92,8 +85,7 @@ public void doEncryptionTest(int numMappers, int numReducers, int numNodes,
       // Generate input.
       createInput(fileSystem, numMappers, numLines);
       // Run the test.
-      runMergeTest(new JobConf(mrCluster.getConfig()), fileSystem,
-              numMappers, numReducers, numLines, isUber);
+      runMergeTest(new JobConf(mrCluster.getConfig()), fileSystem, numMappers, numReducers, numLines);
     } finally {
       if (dfsCluster != null) {
         dfsCluster.shutdown();
@@ -119,9 +111,8 @@ private void createInput(FileSystem fs, int numMappers, int numLines) throws Exc
     }
   }
 
-  private void runMergeTest(JobConf job, FileSystem fileSystem, int
-          numMappers, int numReducers, int numLines, boolean isUber)
-          throws Exception {
+  private void runMergeTest(JobConf job, FileSystem fileSystem, int numMappers, int numReducers, int numLines)
+    throws Exception {
     fileSystem.delete(OUTPUT, true);
     job.setJobName("Test");
     JobClient client = new JobClient(job);
@@ -142,9 +133,6 @@ private void runMergeTest(JobConf job, FileSystem fileSystem, int
     job.setInt("mapreduce.map.maxattempts", 1);
     job.setInt("mapreduce.reduce.maxattempts", 1);
     job.setInt("mapred.test.num_lines", numLines);
-    if (isUber) {
-      job.setBoolean("mapreduce.job.ubertask.enable", true);
-    }
     job.setBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA, true);
     try {
       submittedJob = client.submitJob(job);
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapProgress.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapProgress.java
index 1fe549b..bb4a2de 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapProgress.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapProgress.java
@@ -118,14 +118,12 @@ public boolean statusUpdate(TaskAttemptID taskId, TaskStatus taskStatus)
     throws IOException, InterruptedException {
       StringBuffer buf = new StringBuffer("Task ");
       buf.append(taskId);
-      if (taskStatus != null) {
-        buf.append(" making progress to ");
-        buf.append(taskStatus.getProgress());
-        String state = taskStatus.getStateString();
-        if (state != null) {
-          buf.append(" and state of ");
-          buf.append(state);
-        }
+      buf.append(" making progress to ");
+      buf.append(taskStatus.getProgress());
+      String state = taskStatus.getStateString();
+      if (state != null) {
+        buf.append(" and state of ");
+        buf.append(state);
       }
       LOG.info(buf.toString());
       // ignore phase
-- 
1.7.9.5

